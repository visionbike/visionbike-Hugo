<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Setting Up NVIDIA CUDA on Windows 11 | Visionbike - Personal Blog of CV | DSP | ML notes</title>
<meta name=keywords content><meta name=description content="In this post, I will cover how to setup NVIDIA CUDA on Windows 11. To ensure a smooth setup process, it is crucial to follow the following steps:
NVIDIA Drivers. Microsoft Visual Studio 2022 Community. NVIDIA CUDA Toolkit. NVIDIA cuDNN. TensorRT (optional). Miniforge (optional). 1. System Requirements To use CUDA, make sure your machine has a CUDA-capable GPU inside and the Microsoft Windows 11 should be updated 21H2 version.
You can verify if your machine has CUDA-supported GPU through Display Adapters section in the Windows Device Manager."><meta name=author content="Visionbike"><link rel=canonical href=http://visionbike.github.io/posts/setting-up-nvidia-cuda-on-windows-11/><link crossorigin=anonymous href=/assets/css/stylesheet.min.a7de6bd7965b438aef36ff32266198769e31d4184a0499a0e716b5ef890a1ece.css integrity="sha256-p95r15ZbQ4rvNv8yJmGYdp4x1BhKBJmg5xa174kKHs4=" rel="preload stylesheet" as=style><link rel=icon type=image/png href=/images/favicon.ico><link rel=apple-touch-icon href=/images/apple-touch-icon.png><link rel=icon type=image/png sizes=16x16 href=/images/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=/images/favicon-32x32.png><link rel=alternate hreflang=en href=http://visionbike.github.io/posts/setting-up-nvidia-cuda-on-windows-11/><meta name=twitter:title content="Setting Up NVIDIA CUDA on Windows 11 | Visionbike - Personal Blog of CV | DSP | ML notes"><meta name=twitter:description content="In this post, I will cover how to setup NVIDIA CUDA on Windows 11. To ensure a smooth setup process, it is crucial to follow the following steps:
NVIDIA Drivers. Microsoft Visual Studio 2022 Community. NVIDIA CUDA Toolkit. NVIDIA cuDNN. TensorRT (optional). Miniforge (optional). 1. System Requirements To use CUDA, make sure your machine has a CUDA-capable GPU inside and the Microsoft Windows 11 should be updated 21H2 version.
You can verify if your machine has CUDA-supported GPU through Display Adapters section in the Windows Device Manager."><meta property="og:title" content="Setting Up NVIDIA CUDA on Windows 11 | Visionbike - Personal Blog of CV | DSP | ML notes"><meta property="og:description" content="In this post, I will cover how to setup NVIDIA CUDA on Windows 11. To ensure a smooth setup process, it is crucial to follow the following steps:
NVIDIA Drivers. Microsoft Visual Studio 2022 Community. NVIDIA CUDA Toolkit. NVIDIA cuDNN. TensorRT (optional). Miniforge (optional). 1. System Requirements To use CUDA, make sure your machine has a CUDA-capable GPU inside and the Microsoft Windows 11 should be updated 21H2 version.
You can verify if your machine has CUDA-supported GPU through Display Adapters section in the Windows Device Manager."><meta property="og:type" content="article"><meta property="og:url" content="http://visionbike.github.io/posts/setting-up-nvidia-cuda-on-windows-11/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-07-06T12:41:36+08:00"><meta property="article:modified_time" content="2023-07-06T12:41:36+08:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://visionbike.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Setting Up NVIDIA CUDA on Windows 11","item":"http://visionbike.github.io/posts/setting-up-nvidia-cuda-on-windows-11/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Setting Up NVIDIA CUDA on Windows 11 | Visionbike - Personal Blog of CV | DSP | ML notes","name":"Setting Up NVIDIA CUDA on Windows 11","description":"In this post, I will cover how to setup NVIDIA CUDA on Windows 11. To ensure a smooth setup process, it is crucial to follow the following steps:\nNVIDIA Drivers. Microsoft Visual Studio 2022 Community. NVIDIA CUDA Toolkit. NVIDIA cuDNN. TensorRT (optional). Miniforge (optional). 1. System Requirements To use CUDA, make sure your machine has a CUDA-capable GPU inside and the Microsoft Windows 11 should be updated 21H2 version.\nYou can verify if your machine has CUDA-supported GPU through Display Adapters section in the Windows Device Manager.","keywords":[],"wordCount":"2509","inLanguage":"en","datePublished":"2023-07-06T12:41:36+08:00","dateModified":"2023-07-06T12:41:36+08:00","author":{"@type":"Person","name":"Visionbike"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://visionbike.github.io/posts/setting-up-nvidia-cuda-on-windows-11/"},"publisher":{"@type":"Organization","name":"Visionbike - Personal Blog of CV | DSP | ML notes","logo":{"@type":"ImageObject","url":"http://visionbike.github.io/favicon.ico"}}}</script><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=lib/fontawesome-free/all.min.css></head><body class="dark type-posts kind-page layout-" id=top><script data-no-instant>function switchTheme(e){switch(e){case"light":document.body.classList.remove("dark");break;case"dark":document.body.classList.add("dark");break;default:window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")}}function isDarkTheme(){return document.body.className.includes("dark")}function getPrefTheme(){return localStorage.getItem("pref-theme")}function setPrefTheme(e){switchTheme(e),localStorage.setItem("pref-theme",e)}const toggleThemeCallbacks={};toggleThemeCallbacks.main=e=>{setPrefTheme(e?"light":"dark")},window.addEventListener("toggle-theme",function(){const e=isDarkTheme();for(const t in toggleThemeCallbacks)toggleThemeCallbacks[t](e)});function toggleThemeListener(){window.dispatchEvent(new CustomEvent("toggle-theme"))}</script><script>(function(){const t="dark",e=getPrefTheme(),n=e||t;switchTheme(n)})()</script><script>document.addEventListener("DOMContentLoaded",function(){var e=document.querySelectorAll(".command code");e.forEach(function(e){var t,n,s=e.textContent.split(`
`);e.textContent="",n=e.dataset.lang||"bash",t=n==="cmd"?">":"$",t=n==="python"?">>>":t,s.forEach(function(n,o){n.trim()!==""&&(e.innerHTML+='<span class="prompt">'+t+"</span> "+n,o!==s.length-1&&(e.innerHTML+="<br>"))})})})</script><header class=header><nav class=nav><div class=logo><a href=http://visionbike.github.io/ accesskey=h title="Visionbike (Alt + H)"><img src=http://visionbike.github.io/images/apple-touch-icon.png alt=logo aria-label=logo height=35 width=35>Visionbike</a>
<span class=logo-switches></span></div><ul id=menu><li><a href=http://visionbike.github.io/posts/ title=Posts class=active>Posts</a></li><li><a href=http://visionbike.github.io/tags/ title=Tags>Tags</a></li><li><a href=http://visionbike.github.io/archives/ title=Archive>Archive</a></li><li><a href=http://visionbike.github.io/publish/ title=Publish>Publish</a></li><li><a href=http://visionbike.github.io/about/ title=About>About</a></li><li><a href=http://visionbike.github.io/search/ title="Search (Alt + /)" data-no-instant accesskey=/>Search</a></li></ul></nav></header><main class="main post"><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://visionbike.github.io/>Home</a>&nbsp;»&nbsp;<a href=http://visionbike.github.io/posts/>Posts</a></div><h1 class=post-title>Setting Up NVIDIA CUDA on Windows 11</h1><div class=post-meta><span class=meta-item><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar" style="user-select:text"><rect x="3" y="4" width="18" height="18" rx="2" ry="2" style="user-select:text"/><line x1="16" y1="2" x2="16" y2="6" style="user-select:text"/><line x1="8" y1="2" x2="8" y2="6" style="user-select:text"/><line x1="3" y1="10" x2="21" y2="10" style="user-select:text"/></svg>
<span>July 6, 2023</span></span><span class=meta-item>
<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text" style="user-select:text"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z" style="user-select:text"/><polyline points="14 2 14 8 20 8" style="user-select:text"/><line x1="16" y1="13" x2="8" y2="13" style="user-select:text"/><line x1="16" y1="17" x2="8" y2="17" style="user-select:text"/><polyline points="10 9 9 9 8 9" style="user-select:text"/></svg>
<span>2509 words</span></span><span class=meta-item>
<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke="currentcolor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<span>12 min</span></span></div></header><div class="toc side right"><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#1-system-requirements aria-label="1. System Requirements">1. System Requirements</a></li><li><a href=#2-installing-mamba aria-label="2. Installing Mamba">2. Installing Mamba</a></li><li><a href=#3-installing-nvidia-cuda-toolkit aria-label="3. Installing NVIDIA CUDA Toolkit">3. Installing NVIDIA CUDA Toolkit</a></li><li><a href=#4-installing-nvidia-cudnn aria-label="4. Installing NVIDIA cuDNN">4. Installing NVIDIA cuDNN</a></li><li><a href=#5-installing-nvidia-tensorrt-optional aria-label="5. Installing NVIDIA TensorRT (Optional)">5. Installing NVIDIA TensorRT (Optional)</a></li><li><a href=#conclusion aria-label=Conclusion>Conclusion</a></li><li><a href=#reference aria-label=Reference>Reference</a></li></ul></div></details></div><div class=post-content><p>In this post, I will cover how to setup <strong>NVIDIA CUDA</strong> on Windows 11. To ensure a smooth setup process, it is crucial to follow the following steps:</p><ul><li>NVIDIA Drivers.</li><li>Microsoft Visual Studio 2022 Community.</li><li>NVIDIA CUDA Toolkit.</li><li>NVIDIA cuDNN.</li><li>TensorRT (optional).</li><li>Miniforge (optional).</li></ul><h2 id=1-system-requirements>1. System Requirements<a hidden class=anchor aria-hidden=true href=#1-system-requirements>¶</a></h2><p>To use CUDA, make sure your machine has a CUDA-capable GPU inside and the <strong>Microsoft Windows 11</strong> should be updated <strong>21H2</strong> version.</p><p>You can verify if your machine has CUDA-supported GPU through <strong>Display Adapters</strong> section in the <strong>Windows Device Manager</strong>. Here you will find the vendor name and model of your GPU.</p><p><img loading=lazy src=/posts/setting-up-nvidia-cuda-on-windows-11/device-manager.png type alt="device manager"></p><p>You also need to install <strong>Microsft Visual Studio 2022 Community</strong> as the native compilter for x86_64 application. The download link os the latest version is <a href="https://visualstudio.microsoft.com/thank-you-downloading-visual-studio/?sku=Community"><strong>here</strong></a> and install the <strong>Desktop development with C++</strong> workload.</p><p><img loading=lazy src=/posts/setting-up-nvidia-cuda-on-windows-11/visual-studio-installation.png type alt="Visual Studio Installation"></p><p>If there is any NVIDIA CUDA Toolkit installed before, you need to uninstall before proceeding further, following these steps:</p><ol><li>Open the <strong>Settings > Apps > Installed Apps</strong>.</li><li>Scroll down and find NVIDIA CUDA applications.</li></ol><p><img loading=lazy src=/posts/setting-up-nvidia-cuda-on-windows-11/nvidia-apps-list.png type alt="NVIDIA apps list"></p><ol start=3><li>Click to &ldquo;&mldr;&rdquo; button on the right and uninstall all NVIDIA GPU drivers and any associated software.</li></ol><p><img loading=lazy src=/posts/setting-up-nvidia-cuda-on-windows-11/nvidia-app-uninstallation.png type alt="NVIDIA app uninstallation"></p><p>Then you need to install NVIDIA driver to communicate your computer with NVIDIA devices. You can find the suitable driver from this <a href="https://www.nvidia.com/download/index.aspx?lang=en-us"><strong>website</strong></a>.</p><p><img loading=lazy src=/posts/setting-up-nvidia-cuda-on-windows-11/nvidia-driver-installation.png type alt="NVIDIA Driver Installation"></p><p>After the installation is complete, reboot your system.</p><h2 id=2-installing-mamba>2. Installing Mamba<a hidden class=anchor aria-hidden=true href=#2-installing-mamba>¶</a></h2><p><a href=https://mamba.readthedocs.io/en/latest/user_guide/mamba.html><strong>Mamba</strong></a> is a command-line interfacer (CLI) to manage <code>conda</code>&rsquo;s environemts. For <code>mamba</code> configuration, please refer to <a href=https://conda.io/projects/conda/en/latest/user-guide/configuration/index.html><strong>conda documentation</strong></a>.</p><p>For the fresh installation, you can install <a href=https://github.com/conda-forge/miniforge><strong>Miniforge distribution</strong></a> >= <code>Miniforge3-22.3.1.0</code>. <strong>Miniforge</strong> comes with the popular <code>conda-forge</code> channel preconfigured, but you can modify the configuration to use any channel you like.</p><div class="admonition note"><p class=admonition-title>Installation</p><p>Follow the instaltion prompts, taking note of options to <strong>Create start menu shortcuts</strong> and <strong>Add Miniforge3 to my PATH environment variable</strong>.</p><p><img loading=lazy src=/posts/setting-up-nvidia-cuda-on-windows-11/miniforge-installation.png type alt="Miniforge Installation"></p></div><p>After successful installation, you can use use the mamba commands as described in this <a href=https://mamba.readthedocs.io/en/latest/user_guide/mamba.html#mamba><strong>user guide</strong></a>.</p><div class="admonition note"><p class=admonition-title>Post-installation</p><p>After installation, you make sure that the <strong>Anaconda</strong> is not the default configured channel, seeing <a href=https://mamba.readthedocs.io/en/latest/user_guide/troubleshooting.html#defaults-channels><strong>this</strong></a>.</p><p><strong>DO NOT</strong> install anything into the <code>base</code> environment as this might break your installation. See <a href=https://mamba.readthedocs.io/en/latest/user_guide/troubleshooting.html#base-packages><strong>here</strong></a> for details.</p></div><h2 id=3-installing-nvidia-cuda-toolkit>3. Installing NVIDIA CUDA Toolkit<a hidden class=anchor aria-hidden=true href=#3-installing-nvidia-cuda-toolkit>¶</a></h2><p>You can visit the <a href=https://developer.nvidia.com/cuda-downloads><strong>NVIDIA Developer website for CUDA Toolkit</strong></a> TO get the latest version of NVIDIA CUDA Toolkit. For previous versions, you can check from the <a href=https://developer.nvidia.com/cuda-toolkit-archive><strong>Archive of Previous CUDA Releases</strong></a> page.</p><p>On the dowload page, you choose the appropriate version based on your system.</p><p><img loading=lazy src=/posts/setting-up-nvidia-cuda-on-windows-11/nvidia-cuda-toolkit-download-page.png type alt="Download CUDA Toolkit"></p><p>Then, you locate the downloaded installer file and double-click on it to start the installation process. Follow the on-screen instructions provided by the installer.</p><p><img loading=lazy src=/posts/setting-up-nvidia-cuda-on-windows-11/nvidia-cuda-toolkit-installation.png type alt="NVIDIA CUDA Toolkit install"></p><p>Once the installation is completed, you check environment variables <code>CUDA_PATH</code> and <code>PATH</code> to ensure that your system recognizes <strong>NVIDIA CUDA Toolkit</strong>.</p><p><img loading=lazy src=/posts/setting-up-nvidia-cuda-on-windows-11/nvidia-cuda-path-1.png type alt="NVIDIA CUDA Path 1"></p><p><img loading=lazy src=/posts/setting-up-nvidia-cuda-on-windows-11/nvidia-cuda-path-2.png type alt="NVIDIA CUDA Path 2"></p><details class="admonition info"><summary class=admonition-title>Verify CUDA Toolkit Installation</summary><p>You can verify the installation by running the following command in command prompt.</p><div class="highlight command"><pre tabindex=0 class=chroma><code class=language-cmd data-lang=cmd><span class=line><span class=cl>nvcc --version
</span></span></code></pre></div><p>If the installation was successful, you should see the CUDA version information displayed.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cmd data-lang=cmd><span class=line><span class=cl>nvcc: NVIDIA (R) Cuda compiler driver
</span></span><span class=line><span class=cl>Copyright (c) 2005-2024 NVIDIA Corporation
</span></span><span class=line><span class=cl>Built on Tue_Feb_27_16:28:36_Pacific_Standard_Time_2024
</span></span><span class=line><span class=cl>Cuda compilation tools, release 12.4, V12.4.99
</span></span><span class=line><span class=cl>Build cuda_12.4.r12.4/compiler.33961263_0
</span></span></code></pre></div><p>It is important to verify that the <strong>NVIDIA CUDA Toolkit</strong> can find and communicate correctly with CUDA-compatible hardware. To do this, you need to compile and run some sample programs.</p><p>CUDA samples are located in <a href=https://github.com/nvidia/cuda-samples><strong>https://github.com/nvidia/cuda-samples</strong></a>. To use the samples, clone the project, build the samples in <code>cyda-samples</code> directory using <strong>MVSC 2022 compiler</strong> and run them following the instruction on the Github page.</p><p>To verify a correct configuration of the hardware and software, it is highly recommended that you build and run the <code>deviceQuery</code> sample program.</p><div class="highlight command"><pre tabindex=0 class=chroma><code class=language-cmd data-lang=cmd><span class=line><span class=cl>deviceQuery.exe
</span></span></code></pre></div><p>If CUDA is installed and configured correctly, the output should look similar as below:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cmd data-lang=cmd><span class=line><span class=cl>deviceQuery.exe Starting...
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl> CUDA Device Query (Runtime API) version (CUDART static linking)
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Detected 1 CUDA Capable device(s)
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Device 0: <span class=s2>&#34;NVIDIA GeForce RTX 3070 Ti Laptop GPU&#34;</span>
</span></span><span class=line><span class=cl>  CUDA Driver Version / Runtime Version          12.4 / 12.4
</span></span><span class=line><span class=cl>  CUDA Capability Major/Minor version number:    8.6
</span></span><span class=line><span class=cl>  Total amount of global memory:                 8192 MBytes (8589410304 bytes)
</span></span><span class=line><span class=cl>  <span class=p>(</span>046<span class=p>)</span> Multiprocessors, (128) CUDA Cores/MP:    5888 CUDA Cores
</span></span><span class=line><span class=cl>  GPU Max Clock rate:                            1410 MHz (1.41 GHz)
</span></span><span class=line><span class=cl>  Memory Clock rate:                             7001 Mhz
</span></span><span class=line><span class=cl>  Memory Bus Width:                              256-bit
</span></span><span class=line><span class=cl>  L2 Cache Size:                                 4194304 bytes
</span></span><span class=line><span class=cl>  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)
</span></span><span class=line><span class=cl>  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers
</span></span><span class=line><span class=cl>  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers
</span></span><span class=line><span class=cl>  Total amount of constant memory:               65536 bytes
</span></span><span class=line><span class=cl>  Total amount of shared memory per block:       49152 bytes
</span></span><span class=line><span class=cl>  Total shared memory per multiprocessor:        102400 bytes
</span></span><span class=line><span class=cl>  Total number of registers available per block: 65536
</span></span><span class=line><span class=cl>  Warp size:                                     32
</span></span><span class=line><span class=cl>  Maximum number of threads per multiprocessor:  1536
</span></span><span class=line><span class=cl>  Maximum number of threads per block:           1024
</span></span><span class=line><span class=cl>  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)
</span></span><span class=line><span class=cl>  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)
</span></span><span class=line><span class=cl>  Maximum memory pitch:                          2147483647 bytes
</span></span><span class=line><span class=cl>  Texture alignment:                             512 bytes
</span></span><span class=line><span class=cl>  Concurrent copy and kernel execution:          Yes with 1 copy engine(s)
</span></span><span class=line><span class=cl>  Run time limit on kernels:                     Yes
</span></span><span class=line><span class=cl>  Integrated GPU sharing Host Memory:            No
</span></span><span class=line><span class=cl>  Support host page-locked memory mapping:       Yes
</span></span><span class=line><span class=cl>  Alignment requirement for Surfaces:            Yes
</span></span><span class=line><span class=cl>  Device has ECC support:                        Disabled
</span></span><span class=line><span class=cl>  CUDA Device Driver Mode (TCC or WDDM):         WDDM (Windows Display Driver Model)
</span></span><span class=line><span class=cl>  Device supports Unified Addressing (UVA):      Yes
</span></span><span class=line><span class=cl>  Device supports Managed Memory:                Yes
</span></span><span class=line><span class=cl>  Device supports Compute Preemption:            Yes
</span></span><span class=line><span class=cl>  Supports Cooperative Kernel Launch:            Yes
</span></span><span class=line><span class=cl>  Supports MultiDevice Co-op Kernel Launch:      No
</span></span><span class=line><span class=cl>  Device PCI Domain ID / Bus ID / location ID:   0 / 1 / 0
</span></span><span class=line><span class=cl>  Compute Mode:
</span></span><span class=line><span class=cl>     <span class=p>&lt;</span> Default <span class=p>(</span>multiple host threads can use ::cudaSetDevice(<span class=p>)</span> with device simultaneously) &gt;
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 12.4, CUDA Runtime Version = 12.4, NumDevs = 1
</span></span><span class=line><span class=cl>Result = PASS
</span></span></code></pre></div><p>By running the <code>bandwidthTest</code> program, you can ensure that the system and CUDA-capable device are able to communicate correctly.</p><div class="highlight command"><pre tabindex=0 class=chroma><code class=language-cmd data-lang=cmd><span class=line><span class=cl>bandwidthText.exe
</span></span></code></pre></div><p>The output shoud be here.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cmd data-lang=cmd><span class=line><span class=cl>[CUDA Bandwidth Test] - Starting...
</span></span><span class=line><span class=cl>Running on...
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl> Device 0: NVIDIA GeForce RTX 3070 Ti Laptop GPU
</span></span><span class=line><span class=cl> Quick Mode
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl> Host to Device Bandwidth, 1 Device(s)
</span></span><span class=line><span class=cl> PINNED Memory Transfers
</span></span><span class=line><span class=cl>   Transfer Size (Bytes)        Bandwidth(GB/s)
</span></span><span class=line><span class=cl>   32000000                     11.3
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl> Device to Host Bandwidth, 1 Device(s)
</span></span><span class=line><span class=cl> PINNED Memory Transfers
</span></span><span class=line><span class=cl>   Transfer Size (Bytes)        Bandwidth(GB/s)
</span></span><span class=line><span class=cl>   32000000                     13.9
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl> Device to Device Bandwidth, 1 Device(s)
</span></span><span class=line><span class=cl> PINNED Memory Transfers
</span></span><span class=line><span class=cl>   Transfer Size (Bytes)        Bandwidth(GB/s)
</span></span><span class=line><span class=cl>   32000000                     361.1
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Result = PASS
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.
</span></span></code></pre></div><p>To see a graphical representation, you can run the <code>particles</code> sample program.</p><div class="highlight command"><pre tabindex=0 class=chroma><code class=language-cmd data-lang=cmd><span class=line><span class=cl>particles.exe
</span></span></code></pre></div><p><img loading=lazy src=/posts/setting-up-nvidia-cuda-on-windows-11/nvidia-cuda-particle-sample.png type alt="NVIDIA CUDA Particle Sample"></p></details><p>The installed <strong>NVIDIA CUDA Toolkit</strong> provides the necessary libraries, compilers, and tools for developing and running CUDA-accelerated applications and machine learning models.</p><h2 id=4-installing-nvidia-cudnn>4. Installing NVIDIA cuDNN<a hidden class=anchor aria-hidden=true href=#4-installing-nvidia-cudnn>¶</a></h2><p><strong>cuDNN (CUDA Deep Neural Network Library)</strong> is a GPU-accelerated library specifically designed and colaborated with <strong>NVIDA CUDA Toolkit</strong> to accelerate deep neural network computations. By utilizing <strong>cuDNN</strong>, deep learning frameworks can leverage the parallel processing capabilities of NVIDIA GPUs, leading to significant speed improvements in training and inference of deep neural networks.</p><p>You can visit <a href=https://developer.nvidia.com/rdp/cudnn-download><strong>NVIDIA Developer website for cuDNN</strong></a> for the latest version. You will need to register or log in to your NVIDIA Developer account in order to access the cuDNN download files. If you don&rsquo;t have an account, you can create one for free.</p><p>Once you are logged in, choose the appropriate <strong>cuDNN</strong> version based on the <strong>NVIDIA CUDA Toolkit</strong> version and operating system. There are two main installation options:</p><ol><li><strong>Graphical installation</strong> (executable): the graphical installer bundles the available per-CUDA cuDNN verions in one package.</li><li><strong>Tarball installation</strong> (zip): per-CUDA cuDNN versions are provided as saperate tarballs (zip). These <code>.zip</code> archives do not replace the graphical installer and are not meant for general consumption, as they are not installers. These <code>zip</code> archives can be found at <a href=https://developer.download.nvidia.com/compute/cudnn/redist/cudnn/windows-x86_64/><strong>this</strong></a>.</li></ol><p>Select one of two options for installing <strong>cuDNN</strong>. In this post, I will install <strong>cuDNN</strong> via the Tarball installation option.</p><p>Once download the <code>zip</code> archive, you unzip the <strong>cuDNN</strong> package.</p><p><img loading=lazy src=/posts/setting-up-nvidia-cuda-on-windows-11/nvidia-cudnn-unzip.png type alt="NVIDIA cuDNN Unzip"></p><p>Copy the following files from the unzipped package into the <strong>NVIDIA cuDNN</strong> directory created by yourself.</p><ul><li>Copy <code>bin\cudnn*.h</code> to <strong>C:Program Files\NVIDIA\CUDNN\vx.y\bin</strong>.</li><li>Copy <code>include\cudnn*.h</code> to <strong>C:\Program Files\NVIDIA\CUDNN\vx.y\include</strong>.</li><li>Copy <code>lib\x64\cudnn*.lib</code> to <strong>C:\Program Files\NVIDIA\CUDNN\vx.y\lib</strong>.</li></ul><p>You must replace <code>x.y</code> with your specific <strong>cuDNN</strong> version.</p><p>Set the environment variable to point to where <strong>cuDNN</strong> is located and add <code>bin</code> directory path to <code>PATH</code> environment variable.</p><p><img loading=lazy src=/posts/setting-up-nvidia-cuda-on-windows-11/nvidia-cudnn-path.png type alt="NVIDIA cuDNN Path"></p><p>For upgrading <strong>cuDNN</strong>, the remove the path to the directory containing <strong>cuDNN</strong> from <code>PATH</code> environment variable.</p><details class="admonition info"><summary class=admonition-title>Verify cuDNN Installation</summary><p>The <strong>cuDNN</strong> samples can be found <a href=https://developer.download.nvidia.com/compute/cudnn/redist/cudnn_samples/source/><strong>here</strong></a> and download and extract the <code>tar.xz</code> archive.</p><p><img loading=lazy src=/posts/setting-up-nvidia-cuda-on-windows-11/nvidia-cudnn-samples.png type alt="NVIDIA cuDNN Samples"></p><p>Since this is cross-platform LINUX samples, you need to install <a href=https://cmake.org/download/><strong>CMAKE</strong></a> and use it to compile the <strong>cuDNN</strong> samples.</p><p>Inside the <code>cuda_sample_vx</code> directory (<code>x</code> as the <strong>cuDNN</strong> version), make the comment to line 21 <code># add_subdirectory(mnistCUDNN)</code>.</p><p>Run the following command in the command prompt.</p><div class="highlight command"><pre tabindex=0 class=chroma><code class=language-cmd data-lang=cmd><span class=line><span class=cl><span class=k>mkdir</span> build <span class=p>&amp;&amp;</span> <span class=k>cd</span> build
</span></span><span class=line><span class=cl>cmake -G <span class=s2>&#34;Visual Studio 17 2022&#34;</span> -D cuDNN_INCLUDE_DIR=<span class=s2>&#34;C:\Program Files\NVIDIA\CUDNN\v9.0\include&#34;</span> -D cuDNN_LIBRARY_DIR=<span class=s2>&#34;C:\Program Files\NVIDIA\CUDNN\v9.0\lib&#34;</span> ..
</span></span><span class=line><span class=cl>cmake --build . --config Release
</span></span></code></pre></div><p>By running the <code>conv_sample</code> program, you can ensure the <strong>cuDNN</strong> work in your system.</p><div class="highlight command"><pre tabindex=0 class=chroma><code class=language-cmd data-lang=cmd><span class=line><span class=cl>.\conv_sample\Release\conv_sample.exe
</span></span></code></pre></div><p>The result should be as following:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cmd data-lang=cmd><span class=line><span class=cl>Executing: conv_sample.exe
</span></span><span class=line><span class=cl>Using format CUDNN_TENSOR_NCHW (for INT8x4 and INT8x32 tests use CUDNN_TENSOR_NCHW_VECT_C)
</span></span><span class=line><span class=cl>Testing single precision
</span></span><span class=line><span class=cl>====USER DIMENSIONS====
</span></span><span class=line><span class=cl>input dims are 1, 32, 4, 4
</span></span><span class=line><span class=cl>filter dims are 32, 32, 1, 1
</span></span><span class=line><span class=cl>output dims are 1, 32, 4, 4
</span></span><span class=line><span class=cl>====PADDING DIMENSIONS====
</span></span><span class=line><span class=cl>padded input dims are 1, 32, 4, 4
</span></span><span class=line><span class=cl>padded filter dims are 32, 32, 1, 1
</span></span><span class=line><span class=cl>padded output dims are 1, 32, 4, 4
</span></span><span class=line><span class=cl>Testing conv
</span></span><span class=line><span class=cl><span class=se>^^^^</span> CUDA : elapsed = 0.0127218 sec,
</span></span><span class=line><span class=cl>Test PASSED
</span></span><span class=line><span class=cl>Testing half precision (math in single precision)
</span></span><span class=line><span class=cl>====USER DIMENSIONS====
</span></span><span class=line><span class=cl>input dims are 1, 32, 4, 4
</span></span><span class=line><span class=cl>filter dims are 32, 32, 1, 1
</span></span><span class=line><span class=cl>output dims are 1, 32, 4, 4
</span></span><span class=line><span class=cl>====PADDING DIMENSIONS====
</span></span><span class=line><span class=cl>padded input dims are 1, 32, 4, 4
</span></span><span class=line><span class=cl>padded filter dims are 32, 32, 1, 1
</span></span><span class=line><span class=cl>padded output dims are 1, 32, 4, 4
</span></span><span class=line><span class=cl>Testing conv
</span></span><span class=line><span class=cl><span class=se>^^^^</span> CUDA : elapsed = 0.0029165 sec,
</span></span><span class=line><span class=cl>Test PASSED
</span></span></code></pre></div></details><p>For <strong>Visual Studio</strong> project, add <strong>cuDNN</strong> by following steps:</p><ul><li>Right-click on the project name in <strong>Solution Explorer</strong> and choose **Properties.</li><li>Click <strong>VC++ Directories</strong> and append <code>C:\Program Files\NVIDIA\CUDNN\v9.x\include</code> to the <strong>Include Direcotries</strong> field.</li><li>Click <strong>Linker > General</strong> and append <code>C:\Program Files\NVIDIA\CUDNN\v9.x\lib</code> to the <strong>Additional Library Directories</strong> field.</li><li>Click <strong>Linker > Input</strong> and append <code>cudnn.h</code> to the <strong>Additional Dependencies</strong> field and click <strong>OK</strong>.</li></ul><h2 id=5-installing-nvidia-tensorrt-optional>5. Installing NVIDIA TensorRT (Optional)<a hidden class=anchor aria-hidden=true href=#5-installing-nvidia-tensorrt-optional>¶</a></h2><p><strong>NVIDIA TensorRT</strong> is a C++ library that facilitates high-performance inference NVIDIA graphic processing units (GPUs). <strong>TensorRT</strong> takes a trained network, which consists of a network definition and a set of trained parameters, and produces a highly optimized runtime engine that performs inference for that network.</p><p><strong>TensorRT</strong> provides APIs via C++ and Python that help to express deep learning model via the Network Definition API or load a pre-defined model via the ONNX parser that allow <strong>TensorRT</strong> to optimize and run them on the NVIDIA GPU.</p><p><strong>TensorRT</strong> also include optional high speed mixed precision capabilities with difference NVIDIA architectures.</p><p>You can download the <strong>TensorRT</strong> at <a href=https://developer.nvidia.com/tensorrt/download/10x><strong>here</strong></a>. For Windows architecture, there is only <code>zip</code> archive installation.</p><p>Unzip the <code>zip</code> archive and copy files in <code>lib</code>, <code>include</code> direcotries to <strong>C:\Program Files\NVIDIA\TensorRT\v10.0</strong> directory created by yourself. Then, you add <code>lib</code> directory path to <code>PATH</code> environment variable.</p><p><img loading=lazy src=/posts/setting-up-nvidia-cuda-on-windows-11/nvidia-tensorrt-path.png type alt="NVIDIA TensorRT Path"></p><details class="admonition info"><summary class=admonition-title>Verify TensorRT Installation</summary><p>Inside the <code>zip</code> archive also include the sample programs. To verify the installation is working, you should open a Visual Studio file from one of the samples, such as <code>sampleOnnxMNIST</code>.In the project, ensure that following is presented in the Visual Studio Solution project properties:</p><ul><li>Add <code>C:\Program Files\NVIDIA\TensorRT\v10.0\lib</code> to <code>PATH</code> and <strong>VC++ Directories > Executable Directories</strong>.</li><li>Add <code>C:\Program Files\NVIDIA\TensorRT\v10.0\include</code> to <strong>C/C++ > General > Additional Directories</strong>.</li><li>Add <code>nvinfer.lib</code> and <code>.lib</code> files that that the projects requires to <strong>Linker > Input > Additional Dependencies</strong>.</li></ul><p>Compile the source code and run the example.</p><div class="highlight command"><pre tabindex=0 class=chroma><code class=language-cmd data-lang=cmd><span class=line><span class=cl>.\bin\sample_onnx_mnist.exe
</span></span></code></pre></div><p>The output should be as following:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cmd data-lang=cmd><span class=line><span class=cl><span class=p>&amp;&amp;&amp;&amp;</span> RUNNING TensorRT.sample_onnx_mnist [TensorRT v100000] # .\sample_onnx_mnist.exe
</span></span><span class=line><span class=cl>[04/08/2024-18:12:10] [I] Building and running a GPU inference engine for Onnx MNIST
</span></span><span class=line><span class=cl>[04/08/2024-18:12:10] [I] [TRT] [MemUsageChange] Init CUDA: CPU +109, GPU +0, now: CPU 18227, GPU 1091 (MiB)
</span></span><span class=line><span class=cl>[04/08/2024-18:12:18] [I] [TRT] [MemUsageChange] Init builder kernel library: CPU +2597, GPU +310, now: CPU 21109, GPU 1401 (MiB)
</span></span><span class=line><span class=cl>[04/08/2024-18:12:18] [I] [TRT] ----------------------------------------------------------------
</span></span><span class=line><span class=cl>[04/08/2024-18:12:18] [I] [TRT] Input filename:   ../data/mnist/mnist.onnx
</span></span><span class=line><span class=cl>[04/08/2024-18:12:18] [I] [TRT] ONNX IR version:  0.0.3
</span></span><span class=line><span class=cl>[04/08/2024-18:12:18] [I] [TRT] Opset version:    8
</span></span><span class=line><span class=cl>[04/08/2024-18:12:18] [I] [TRT] Producer name:    CNTK
</span></span><span class=line><span class=cl>[04/08/2024-18:12:18] [I] [TRT] Producer version: 2.5.1
</span></span><span class=line><span class=cl>[04/08/2024-18:12:18] [I] [TRT] Domain:           ai.cntk
</span></span><span class=line><span class=cl>[04/08/2024-18:12:18] [I] [TRT] Model version:    1
</span></span><span class=line><span class=cl>[04/08/2024-18:12:18] [I] [TRT] Doc string:
</span></span><span class=line><span class=cl>[04/08/2024-18:12:18] [I] [TRT] ----------------------------------------------------------------
</span></span><span class=line><span class=cl>[04/08/2024-18:12:18] [I] [TRT] Local timing cache in use. Profiling results in this builder pass will not be stored.
</span></span><span class=line><span class=cl>[04/08/2024-18:12:20] [I] [TRT] Detected 1 inputs and 1 output network tensors.
</span></span><span class=line><span class=cl>[04/08/2024-18:12:20] [I] [TRT] Total Host Persistent Memory: 26400
</span></span><span class=line><span class=cl>[04/08/2024-18:12:20] [I] [TRT] Total Device Persistent Memory: 0
</span></span><span class=line><span class=cl>[04/08/2024-18:12:20] [I] [TRT] Total Scratch Memory: 0
</span></span><span class=line><span class=cl>[04/08/2024-18:12:20] [I] [TRT] [BlockAssignment] Started assigning block shifts. This will take 6 steps to complete.
</span></span><span class=line><span class=cl>[04/08/2024-18:12:20] [I] [TRT] [BlockAssignment] Algorithm ShiftNTopDown took 0.8572ms to assign 3 blocks to 6 nodes requiring 32256 bytes.
</span></span><span class=line><span class=cl>[04/08/2024-18:12:20] [I] [TRT] Total Activation Memory: 31744
</span></span><span class=line><span class=cl>[04/08/2024-18:12:20] [I] [TRT] Total Weights Memory: 26152
</span></span><span class=line><span class=cl>[04/08/2024-18:12:20] [I] [TRT] Engine generation completed in 1.68333 seconds.
</span></span><span class=line><span class=cl>[04/08/2024-18:12:20] [I] [TRT] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 5 MiB
</span></span><span class=line><span class=cl>[04/08/2024-18:12:20] [I] [TRT] [MemUsageStats] Peak memory usage during Engine building and serialization: CPU: 3036 MiB
</span></span><span class=line><span class=cl>[04/08/2024-18:12:20] [I] [TRT] Loaded engine size: 0 MiB
</span></span><span class=line><span class=cl>[04/08/2024-18:12:21] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 0 (MiB)
</span></span><span class=line><span class=cl>[04/08/2024-18:12:21] [I] Input:
</span></span><span class=line><span class=cl>[04/08/2024-18:12:21] [I] @@@@@@@@@@@@@@@@@@@@@@@@@@@@
</span></span><span class=line><span class=cl><span class=p>@@@@@@@@@@@@@@@@@@@@@@@@@@@@</span>
</span></span><span class=line><span class=cl><span class=p>@@@@@@@@@@@@@@@@@@@@@@@@@@@@</span>
</span></span><span class=line><span class=cl><span class=p>@@@@@@@@@@@@@@@@@@@@@@@@@@@@</span>
</span></span><span class=line><span class=cl><span class=p>@@@@@@@@@@@@@@@@@@@@@@@@@@@@</span>
</span></span><span class=line><span class=cl><span class=p>@@@@@@@@@@</span>=   ++++#++=*@@@@@
</span></span><span class=line><span class=cl><span class=p>@@@@@@@@</span>#.            *@@@@@
</span></span><span class=line><span class=cl><span class=p>@@@@@@@@</span>=             *@@@@@
</span></span><span class=line><span class=cl><span class=p>@@@@@@@@</span>.   .. ...****%@@@@@
</span></span><span class=line><span class=cl><span class=p>@@@@@@@@</span>: .%@@#@@@@@@@@@@@@@
</span></span><span class=line><span class=cl><span class=p>@@@@@@@</span>%  -@@@@@@@@@@@@@@@@@
</span></span><span class=line><span class=cl><span class=p>@@@@@@@</span>%  -@@*@@@*@@@@@@@@@@
</span></span><span class=line><span class=cl><span class=p>@@@@@@@</span>#  :#- ::. ::=@@@@@@@
</span></span><span class=line><span class=cl><span class=p>@@@@@@@</span>-             -@@@@@@
</span></span><span class=line><span class=cl><span class=p>@@@@@@</span>%.              *@@@@@
</span></span><span class=line><span class=cl><span class=p>@@@@@@</span>#     :==*+==   *@@@@@
</span></span><span class=line><span class=cl><span class=p>@@@@@@</span><span class=nv>%---%</span>%@@@@@@@.  *@@@@@
</span></span><span class=line><span class=cl><span class=p>@@@@@@@@@@@@@@@@@@@</span>+  *@@@@@
</span></span><span class=line><span class=cl><span class=p>@@@@@@@@@@@@@@@@@@@</span>=  *@@@@@
</span></span><span class=line><span class=cl><span class=p>@@@@@@@@@@@@@@@@@@</span>*   *@@@@@
</span></span><span class=line><span class=cl><span class=p>@@@@@</span><span class=nv>%+%</span>@@@@@@@@<span class=nv>%.   .%</span>@@@@@
</span></span><span class=line><span class=cl><span class=p>@@@@@</span>*  .******=    -@@@@@@@
</span></span><span class=line><span class=cl><span class=p>@@@@@</span>*             .#@@@@@@@
</span></span><span class=line><span class=cl><span class=p>@@@@@</span>*            =%@@@@@@@@
</span></span><span class=line><span class=cl><span class=p>@@@@@@</span>%#+++=     =@@@@@@@@@@
</span></span><span class=line><span class=cl><span class=p>@@@@@@@@@@@@@@@@@@@@@@@@@@@@</span>
</span></span><span class=line><span class=cl><span class=p>@@@@@@@@@@@@@@@@@@@@@@@@@@@@</span>
</span></span><span class=line><span class=cl><span class=p>@@@@@@@@@@@@@@@@@@@@@@@@@@@@</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>[04/08/2024-18:12:21] [I] Output:
</span></span><span class=line><span class=cl>[04/08/2024-18:12:21] [I]  Prob 0  0.0000 Class 0:
</span></span><span class=line><span class=cl>[04/08/2024-18:12:21] [I]  Prob 1  0.0000 Class 1:
</span></span><span class=line><span class=cl>[04/08/2024-18:12:21] [I]  Prob 2  0.0000 Class 2:
</span></span><span class=line><span class=cl>[04/08/2024-18:12:21] [I]  Prob 3  0.0000 Class 3:
</span></span><span class=line><span class=cl>[04/08/2024-18:12:21] [I]  Prob 4  0.0000 Class 4:
</span></span><span class=line><span class=cl>[04/08/2024-18:12:21] [I]  Prob 5  1.0000 Class 5: **********
</span></span><span class=line><span class=cl>[04/08/2024-18:12:21] [I]  Prob 6  0.0000 Class 6:
</span></span><span class=line><span class=cl>[04/08/2024-18:12:21] [I]  Prob 7  0.0000 Class 7:
</span></span><span class=line><span class=cl>[04/08/2024-18:12:21] [I]  Prob 8  0.0000 Class 8:
</span></span><span class=line><span class=cl>[04/08/2024-18:12:21] [I]  Prob 9  0.0000 Class 9:
</span></span><span class=line><span class=cl>[04/08/2024-18:12:21] [I]
</span></span><span class=line><span class=cl><span class=p>&amp;&amp;&amp;&amp;</span> PASSED TensorRT.sample_onnx_mnist [TensorRT v100000] # .\sample_onnx_mnist.exe
</span></span></code></pre></div></details><p>If you are using <strong>TensorRT Python API</strong>, make sure <strong>CUDA-Python</strong> is installed in your system or your virtual environment.</p><ul><li>Installing from <strong>PyPI</strong>.</li></ul><div class="highlight command"><pre tabindex=0 class=chroma><code class=language-cmd data-lang=cmd><span class=line><span class=cl>pip install cuda-python
</span></span></code></pre></div><ul><li>Installing from <strong>Conda</strong>.</li></ul><div class="highlight command"><pre tabindex=0 class=chroma><code class=language-cmd data-lang=cmd><span class=line><span class=cl>conda install -c nvidia cuda-python
</span></span></code></pre></div><p>Conda packages are assign a dependency to CUDA Toolkit: <code>cuda-cudart</code> (providing CUDA headers to enable writting NVRTC kernel with CUDA types) and <code>cuda-nvrtc</code> (providing NVTC shared library).</p><p>Then you install <strong>TensorRT</strong> Python wheel.</p><div class="highlight command"><pre tabindex=0 class=chroma><code class=language-cmd data-lang=cmd><span class=line><span class=cl>pip install --pre --upgrade tensorrt
</span></span></code></pre></div><p>Optional, install <strong>TensorRT</strong> lean and dispatch runtime wheels:</p><div class="highlight command"><pre tabindex=0 class=chroma><code class=language-cmd data-lang=cmd><span class=line><span class=cl>pip install tensorrt_lean tensorrt_dispatch
</span></span></code></pre></div><p>To verify the installation is working, use python code.</p><div class="highlight command"><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>tensorrt</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>tensorrt</span><span class=o>.</span><span class=n>__version__</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>assert</span><span class=p>(</span><span class=n>tensorrt</span><span class=o>.</span><span class=n>Builder</span><span class=p>(</span><span class=n>tensorrt</span><span class=o>.</span><span class=n>Logger</span><span class=p>()))</span>
</span></span></code></pre></div><p>Use a similar procedure to verify that the lean and dispatch modules work as expected.</p><div class="highlight command"><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>tensorrt_lean</span> <span class=k>as</span> <span class=nn>trt</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>trt</span><span class=o>.</span><span class=n>__version__</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>trt</span><span class=o>.</span><span class=n>Runtime</span><span class=p>(</span><span class=n>trt</span><span class=o>.</span><span class=n>Logger</span><span class=p>())</span>
</span></span></code></pre></div><div class="highlight command"><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>tensorrt_dispatch</span> <span class=k>as</span> <span class=nn>trt</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>trt</span><span class=o>.</span><span class=n>__version__</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>trt</span><span class=o>.</span><span class=n>Runtime</span><span class=p>(</span><span class=n>trt</span><span class=o>.</span><span class=n>Logger</span><span class=p>())</span>
</span></span></code></pre></div><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>¶</a></h2><p>By following these steps and installing the required software, you will have an CUDA-ready environment in Windows 11 system for further machine learnin/deep learning applications. This environment will provide the necessary tools and libraries for GPU-accelerated computing and Python package management.</p><h2 id=reference>Reference<a hidden class=anchor aria-hidden=true href=#reference>¶</a></h2><ul><li><a href=https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/>CUDA Installation Guide for Microsoft Windows</a>.</li><li><a href=https://medium.com/geekculture/install-cuda-and-cudnn-on-windows-linux-52d1501a8805>Install CUDA and CUDNN on Windows & Linux</a>.</li><li><a href=https://medium.com/@Gunter-Pearson/installing-latest-tensorflow-version-with-cuda-cudnn-and-gpu-support-on-windows-11-pc-e41fac5c5795>Installing Latest TensorFlow version with CUDA, cudNN and GPU support on Windows 11 PC</a>.</li></ul></div><footer class=post-footer><nav class=paginav><a class=prev href=http://visionbike.github.io/posts/working-with-wsl2-on-windows11/><span class=title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-left" style="user-select:text"><line x1="19" y1="12" x2="5" y2="12" style="user-select:text"/><polyline points="12 19 5 12 12 5" style="user-select:text"/></svg>&nbsp;Prev Page</span><br><span>Working With Windows Subsystem for Linux 2 on Windows 11</span>
</a><a class=next href=http://visionbike.github.io/posts/setting-up-personal-blog-with-hugo-and-gh-pages/><span class=title>Next Page&nbsp;<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-right" style="user-select:text"><line x1="5" y1="12" x2="19" y2="12" style="user-select:text"/><polyline points="12 5 19 12 12 19" style="user-select:text"/></svg></span><br><span>Setting Up a Personal Blog With Hugo and GitHub Pages</span></a></nav></footer><div class=comments-separator></div></article></main><footer class=footer><span>&copy; 2024 <a href=http://visionbike.github.io/>Visionbike - Personal Blog of CV | DSP | ML notes</a></span><span style=display:inline-block;margin-left:1em>
<a href=https://creativecommons.org/licenses/by-sa/4.0/>CC BY-SA</a>
</span><span style=display:inline-block;margin-left:1em>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
    <a href=https://github.com/reorx/hugo-PaperModX/ rel=noopener target=_blank>PaperModX</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>(function(){const t="1"=="1";if(t)return;let e=document.getElementById("theme-toggle");e.removeEventListener("click",toggleThemeListener),e.addEventListener("click",toggleThemeListener)})()</script><script>(function(){let e=document.getElementById("menu");e&&(e.scrollLeft=localStorage.getItem("menu-scroll-position"),e.onscroll=function(){localStorage.setItem("menu-scroll-position",e.scrollLeft)});const t=""=="1",n="1"=="1";if(window.matchMedia("(prefers-reduced-motion: reduce)").matches||t||n)return;document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})})()</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>if(window.scrollListeners)for(const e of scrollListeners)window.removeEventListener("scroll",e);window.scrollListeners=[]</script><script src=/js/medium-zoom.min.js data-no-instant></script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerText="copy";function s(){t.innerText="copied!",setTimeout(()=>{t.innerText="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script><script>(function(){const a="1"=="1";if(!a)return;if(!document.querySelector(".toc")){console.log("no toc found, ignore toc scroll");return}const r=window.scrollListeners,t=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id]"),n="active";let e=t[0];o(e).classList.add(n);const c=()=>{const s=[];for(const e of t)if(l(e)<5)s.push(e);else break;s.length>0?newActiveHeading=s[s.length-1]:newActiveHeading=t[0],e!=newActiveHeading&&(o(e).classList.remove(n),e=newActiveHeading,o(e).classList.add(n))};let s=null;const i=()=>{s!==null&&clearTimeout(s),s=setTimeout(c,50)};window.addEventListener("scroll",i,!1),r.push(i);function o(e){const t=encodeURI(e.getAttribute("id")).toLowerCase();return document.querySelector(`.toc ul li a[href="#${t}"]`)}function l(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect();return t.top}})()</script><script>mediumZoom(".entry-cover img"),mediumZoom(".post-content img:not([no-zoom])")</script><script src=/js/instantclick.min.js data-no-instant></script><script data-no-instant>InstantClick.init()</script></body></html>