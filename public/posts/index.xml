<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on Visionbike - Personal Blog of CV | DSP | ML notes</title>
    <link>http://visionbike.github.io/posts/</link>
    <description>Recent content in Posts on Visionbike - Personal Blog of CV | DSP | ML notes</description>
    <image>
      <url>http://visionbike.github.io/images/apple-touch-icon.png</url>
      <link>http://visionbike.github.io/images/apple-touch-icon.png</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Mon, 21 Aug 2023 14:57:01 +0800</lastBuildDate><atom:link href="http://visionbike.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Build OpenCV with CUDA on Windows 11</title>
      <link>http://visionbike.github.io/posts/build-opencv-cuda-on-windows/</link>
      <pubDate>Mon, 21 Aug 2023 14:57:01 +0800</pubDate>
      
      <guid>http://visionbike.github.io/posts/build-opencv-cuda-on-windows/</guid>
      <description>Since pre-built OpenCV binaries do not include CUDA modules, this post is a tutorial for building OpenCV with CUDA on Windows 11. The obvious dvantage of OpenCV CUDA is boosting performance of most functions, you can find evidence here.
1. Prerequisite There are a couples of softwares or libraries having been downloaded and installed before getting started:
  Install the Visual Studio Community 2022 and select Desktop development with C++ workload.</description>
      <content:encoded><![CDATA[<p>Since pre-built OpenCV binaries do not include CUDA modules, this post is a tutorial for building OpenCV with CUDA on Windows 11. The obvious dvantage of OpenCV CUDA is boosting performance of most functions, you can find evidence <a href="https://www.jamesbowley.co.uk/qmd/opencv_cuda_performance.html">here</a>.</p>
<h2 id="1-prerequisite">1. Prerequisite</h2>
<p>There are a couples of softwares or libraries having been downloaded and installed before getting started:</p>
<ol>
<li>
<p>Install the <a href="https://visualstudio.microsoft.com/thank-you-downloading-visual-studio/?sku=Community">Visual Studio Community 2022</a> and select <strong>Desktop development with C++</strong> workload.</p>
</li>
<li>
<p>Download the sources for OpenCV from GitHub by cloning the repositories (<a href="https://github.com/opencv/opencv">opencv</a> and <a href="https://github.com/opencv/opencv_contrib">opencv_contrib</a>).</p>
</li>
</ol>
<p><img loading="lazy" src="/posts/build-opencv-cuda-on-windows/opencv-cuda-folder.png" type="" alt="OpenCV CUDA folder"  /></p>
<p>After downloading, you can indicate the OpenCV&rsquo;s version you want to. For the current version, you can run the following command in <strong>Command Prompt</strong> at OpenCV&rsquo;s repositories:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell">git checkout tags/4.8.0
</code></pre></div><ol start="3">
<li>
<p>Install the latest stable version (not release candidate -rc) of <a href="https://cmake.org/download/">CMake</a>.</p>
</li>
<li>
<p>Install the latest version of <a href="https://developer.nvidia.com/cuda-downloads">NVIDIA CUDA Toolkit</a> and add PATH. You can follow this <a href="https://visionbike.github.io/posts/create-ml-development-environment-on-window11/">tutorial</a>.</p>
</li>
</ol>
<div class="admonition warning"><p class="admonition-title">The latest CUDA Toolkit version</p>
<p>At the time writting this post, the latest <strong>NVIDIA CUDA Toolkit version 12.2</strong> still makes somes error when building with <strong>OpenCV 4.8.0</strong>, like <code>error C2666: 'operator !=': overloaded functions have similar conversions</code>. Therefore, if meeting such problem, try installing older version.</p>
</div>
<ol start="5">
<li>
<p>Register an account and download the latest verson of <a href="https://developer.nvidia.com/rdp/cudnn-download">NVIDIA DNN CUDA backend</a> for the version of CUDA. Extract the downloaded <strong>.zip</strong> file and copy <strong>bin</strong>, <strong>include</strong> and <strong>lib</strong> directories to your CUDA installation directory, i.e., <code>C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\vxx.x</code>.</p>
</li>
<li>
<p>Register an account and download the latest version of <a href="https://developer.nvidia.com/nvidia-video-codec-sdk">NVIDIA VIdeo Codec SDK</a>. Extract the downloaded <strong>.zip</strong> file and copy the contents inside <strong>Interface</strong> and <strong>Lib</strong> to <strong>include</strong> and <strong>lib</strong> directories inside the CUDA installation directory.</p>
</li>
<li>
<p><strong>Optional</strong> - Download and install the latest version of <a href="https://gstreamer.freedesktop.org/download/#toolchain-compatibility-notes">Gstreamer</a>.</p>
</li>
<li>
<p>Download and install the lastest version of <a href="https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-Windows-x86_64.exe">mambaforge</a> to call OpenCV CUDA routines from python.</p>
</li>
</ol>
<h2 id="2-create-virtual-environtment-from-mambaforge">2. Create virtual environtment from Mambaforge</h2>
<p>To bind OpenCV for python3 without any conflit package installation, you should create an new virtual environment for the installation.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell">mamba create -n opencv-cuda <span class="nv">python</span><span class="o">=</span>3.10
</code></pre></div><p>Activate the created environment and install <strong>numpy</strong> package.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell">mamba activate opencv-cuda
mamaba install numpy
</code></pre></div><h2 id="3-build-opencv-with-cmake">3. Build OpenCV with CMake</h2>
<h3 id="preparation">Preparation</h3>
<p>Create a <strong>build</strong> folder with your OpenCV extracted folders.</p>
<p><img loading="lazy" src="/posts/build-opencv-cuda-on-windows/opencv-cuda-folder.png" type="" alt="OpenCV CUDA folder"  /></p>
<p>Edit the prioeiry of Python3 installation in <code>OpenCVDetectPython.cmake</code> file inside <strong>opencv-x.x.x/cmake</strong> folder.</p>
<p><img loading="lazy" src="/posts/build-opencv-cuda-on-windows/python3-prior.png" type="" alt="Python3 Prior"  /></p>
<h3 id="build-gui-build-configuration">Build GUI Build Configuration</h3>
<p>Open Cmake GUI and provide the paths to the OpenCV and target build folders.</p>
<p><img loading="lazy" src="/posts/build-opencv-cuda-on-windows/cmake-build.png" type="" alt="CMake GUI"  /></p>
<p>Hit <strong>Configure</strong> and select <strong>x64</strong> for the <strong>Optional platform for generator</strong>, then hit <strong>finish</strong> to start the configuration.</p>
<p>Once the configuration is done, edit the following parameters:</p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>CMAKE_INSTALL_PREFIX</td>
<td>path of opencv <strong>installation</strong></td>
</tr>
<tr>
<td>ENABLE_FAST_MATH</td>
<td>âœ…</td>
</tr>
<tr>
<td>WITH_CUDA</td>
<td>âœ…</td>
</tr>
<tr>
<td>BUILD_opencv_world</td>
<td>âœ…</td>
</tr>
<tr>
<td>BUILD_opencv_python3</td>
<td>âœ…</td>
</tr>
<tr>
<td>OPENCV_DNN_CUDA</td>
<td>âœ…</td>
</tr>
<tr>
<td>OPENCV_EXTRA_MODULES_PATH</td>
<td>path of <strong>modules</strong> directory in <strong>opencv_contrib-x.x.x</strong></td>
</tr>
<tr>
<td>OPENCV_PYTHON3_VERSION</td>
<td>âœ…</td>
</tr>
<tr>
<td>PYTHON3_EXECUTABLE</td>
<td>path of python3 executable in virtual env, i.e., <strong>C:/Users/ntthi/mambaforge/envs/opencv-cuda/python.exe</strong></td>
</tr>
<tr>
<td>PYTHON3_INCLUDE_DIR</td>
<td>path of <strong>include</strong> folder in the virtual env, i.e., <strong>C:/Users/ntthi/mambaforge/envs/opencv-cuda/include</strong></td>
</tr>
<tr>
<td>PYTHON3_LIBRARY</td>
<td>path of <strong>.lib</strong> file in the virtual env, i.e., <strong>C:/Users/ntthi/mambaforge/envs/opencv-cuda/libs/python310.lib</strong></td>
</tr>
<tr>
<td>PYTHON3_NUMPY_INCLUDE_DIRS</td>
<td>path of <strong>numpy</strong> in the virtual env, i.e., <strong>C:/Users/ntthi/mambaforge/envs/opencv-cuda/Lib/site-pakages/numpy/core/include</strong></td>
</tr>
<tr>
<td>PYTHON3_PACKAGES_PATH</td>
<td>path of <strong>site-packages</strong> in the virtual env, i.e., <strong>C:/Users/ntthi/mambaforge/envs/opencv-cuda/Lib/site-pakages</strong></td>
</tr>
</tbody>
</table>
<p>Note that the path separator hase to be &ldquo;/&rdquo; , <em>not</em> &ldquo;&quot;.</p>
<p>Hit <strong>Configure again</strong> again and check edit more parameters:</p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>CUDA_FAST_MATH</td>
<td>âœ…</td>
</tr>
<tr>
<td>CUDA_ARCH_BIN</td>
<td>version of computing capability, i.e., <strong>8.6</strong></td>
</tr>
<tr>
<td>WITH_CUBLAS</td>
<td>âœ…</td>
</tr>
<tr>
<td>WITH_CUDNN</td>
<td>âœ…</td>
</tr>
<tr>
<td>WITH_CUFFT</td>
<td>âœ…</td>
</tr>
</tbody>
</table>
<p>The CUDA_ARCH_BIN corresponding to your GPU is the value found in the left column of the <a href="https://en.wikipedia.org/wiki/CUDA#GPUs_supported">GPU support table</a>. For instance, &ldquo;8.6&rdquo; fir the RTX 3070 Ti.</p>
<p>If you do not want to create shared lib and make sure the opencv python libraries is installed, edit the following parameters:</p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>BUILD_SHARED_LIBS</td>
<td>ðŸ”³</td>
</tr>
<tr>
<td>OPENCV_FORCE_PYTHON_LIBS</td>
<td>âœ…</td>
</tr>
</tbody>
</table>
<p>Hit <strong>Configure</strong> at the last time and then hit <strong>Generate</strong>.</p>
<h3 id="build-the-project-with-visual-studio">Build the project with Visual Studio</h3>
<p>Open project <code>OpenCV.sln</code> created in the build folder. Go to <strong>Tools &gt; Options&hellip;</strong>, then uncheck the last parameter in <strong>Projects and Solutions &gt; Web Projects</strong>.</p>
<p></p>
<p>This setting may help to prevent the <code>ImportError: DLL load failed while importing cv2: The specified module could not be found.</code> error.</p>
<p>To build the OpenCV project, change <strong>Debug</strong> mode to <strong>Release</strong>. In the solution explorer expand <strong>CMakeTargets</strong>, right-click <strong>ALL_BUILD</strong> and select Build. This will take about an hour.</p>
<p><img loading="lazy" src="/posts/build-opencv-cuda-on-windows/build-opencv.png" type="" alt="OpenCV build"  /></p>
<p>Then repeat the step for <strong>INSTALL</strong> (below <strong>ALL_BUILD</strong>). Check for error in the two building steps. If everything is fine, you are done.</p>
<h3 id="check-installation-and-troubleshooting">Check Installation and Troubleshooting</h3>
<p>To verify the Python installation, activate the virtual environment for OpenCV install and try this code:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cv2</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cv2</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cv2</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">getCudaEnabledDeviceCount</span><span class="p">())</span>
</code></pre></div><p>If it works, congratulations you are good to go!</p>
<p>If you meets the problem <code>ImportError: DLL load failed while importing cv2: The specified module could not be found.</code>, it may lack the library&rsquo;s binaries. One solution is to edit <strong>config.py</strong> in <strong>C:/Users/ntthi/mambaforge/envs/opencv-cuda/Lib/site-packages/cv2</strong>.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">os</span>

<span class="n">BINARIES_PATHS</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">&#39;C:/opencv-cuda-4.8.0&#39;</span><span class="p">,</span> <span class="s1">&#39;x64/vc17/bin&#39;</span><span class="p">),</span>
    <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s1">&#39;CUDA_PATH&#39;</span><span class="p">,</span> <span class="s1">&#39;C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.1&#39;</span><span class="p">),</span> <span class="s1">&#39;bin&#39;</span><span class="p">),</span>
    <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">&#39;C:/gstreamer/1.0/msvc_x86_64&#39;</span><span class="p">,</span> <span class="s1">&#39;bin&#39;</span><span class="p">),</span>
<span class="p">]</span> <span class="o">+</span> <span class="n">BINARIES_PATHS</span>
</code></pre></div><p>These binary paths are from installed OpenCV, CUDA and Gstreamer (if installed).</p>
<p>For other bugs and problems, I refer you to the <a href="https://github.com/chrismeunier/OpenCV-CUDA-installation/blob/main/README.md">chrismeunier</a> and <a href="https://jamesbowley.co.uk/accelerate-opencv-4-4-0-build-with-cuda-and-python-bindings/#troubleshooting">Bowley</a>&rsquo;s troubleshooting tutorial.</p>
<h2 id="reference">Reference</h2>
<ul>
<li><a href="https://www.jamesbowley.co.uk/qmd/opencv_cuda_python_windows.html">Build OpenCV (including Python) with CUDA on Windows</a>.</li>
<li><a href="https://github.com/chrismeunier/OpenCV-CUDA-installation/blob/main/README.md#opencv-cuda-installation">OpenCV CUDA installation</a>.</li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>Setting Up NVIDIA CUDA on WSL 2</title>
      <link>http://visionbike.github.io/posts/setitng-up-nvidia-cuda-on-wsl2/</link>
      <pubDate>Thu, 13 Jul 2023 16:44:33 +0800</pubDate>
      
      <guid>http://visionbike.github.io/posts/setitng-up-nvidia-cuda-on-wsl2/</guid>
      <description>Windows Subsystem for Linux (WSL) is a compatibility layer provided by Microsoft that allows you to run a Linux environment directly on WIndows. It will bring Linux applications, utilities and Bash command-line tool directly on Windows without the overhead of traditional virtual machine or dualboot setup.
1. Prequisites The Windows must be upgraded to Windows 10 version 2004 and higher or Windows 11 to install WSL by a single command line.</description>
      <content:encoded><![CDATA[<p><strong>Windows Subsystem for Linux (WSL)</strong> is a compatibility layer provided by Microsoft that allows you to run a <strong>Linux environment</strong> directly on <strong>WIndows</strong>. It will bring Linux applications, utilities and Bash command-line tool directly on <strong>Windows</strong> without the overhead of traditional virtual machine or dualboot setup.</p>
<h2 id="1-prequisites">1. Prequisites</h2>
<p>The <strong>Windows</strong> must be upgraded to <strong>Windows 10 version 2004 and higher</strong> or <strong>Windows 11</strong> to install <strong>WSL</strong> by a single command line. Open Command Prompt in <strong>Administrator</strong> mode, then run the following command:</p>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-cmd" data-lang="cmd">wsl --install --no-distribution
</code></pre></div><p>The command will setup <strong>WSL version 2</strong> by default without any Linux distribution. Reboot the machine to apply changes.</p>
<p>Open a Command Prompt window and install latest version Ubuntu distro:</p>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-cmd" data-lang="cmd">wsl --install -d Ubuntu
</code></pre></div><p>The installation takes a few minutes and you need to create <strong>user name</strong> and <strong>password</strong>.</p>
<p><img loading="lazy" src="/posts/setitng-up-nvidia-cuda-on-wsl2/wsl-ubuntu-installation.png" type="" alt="WSL-Ubuntu Installation"  /></p>
<details class="admonition note"><summary class="admonition-title">Display available WSL Linux distros</summary>
<p>If you want to install another Linux distro, you can run following command to display a list of available WSL Linux distros:</p>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-cmd" data-lang="cmd">wsl --list --online
</code></pre></div></details>
<p>For the first start, we should to install several Linux updates. The process will depend on the speed of the internet, so be patient if it slow!</p>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash">sudo apt update -y <span class="o">&amp;&amp;</span> sudo apt upgrade -y
</code></pre></div><details class="admonition note"><summary class="admonition-title">Issue: /usr/lib/wsl/lib/libcuda.so.1 is not a symbolic link</summary>
<p>If your machine has integrated <strong>NVIDIA</strong> GPU, you may meet the <code>/usr/lib/wsl/lib/libcuda.so.1 is not a symbolic link</code> issue when updating the system.</p>
<p>In the WSL Ubuntu side, run following commands:</p>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="nb">cd</span> /usr/lib/wsl/lib
sudo rm libcuda.so
sudo rm libcuda.so.1
sudo ln -s libcuda.so.1.1 libcuda.so
sudo ln -s libcuda.so.1.1 libcuda.so.1
</code></pre></div><p>Now reinstall <code>libc-bin</code> package.</p>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash">sudo apt reinstall libc-bin
</code></pre></div><p>In the Windows side, run following comands in <strong>Administrator</strong> mode:</p>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-cmd" data-lang="cmd"><span class="k">cd</span> C:\Windows\System32\lxss\lib
<span class="k">del</span> libcuda.so
<span class="k">del</span> libcuda.so.1
<span class="k">mklink</span> libcuda.so libcuda.so.1.1
<span class="k">mklink</span> libcuda.so.1 libcuda.so.1.1
</code></pre></div><p>Reboot the machine and run the <code>sudo ldconfig</code> command in WSL Ubuntu side to verify the problem is solved.</p>
</details>
<h2 id="2-installing-mamba">2. Installing Mamba</h2>
<p><a href="https://mamba.readthedocs.io/en/latest/user_guide/mamba.html"><strong>Mamba</strong></a> is a command-line interfacer (CLI) to manage <code>conda</code>&rsquo;s environemts. For <code>mamba</code> configuration, please refer to <a href="https://conda.io/projects/conda/en/latest/user-guide/configuration/index.html"><strong>conda documentation</strong></a>.</p>
<p>For the fresh installation, you can install <a href="https://github.com/conda-forge/miniforge"><strong>Miniforge distribution</strong></a> &gt;= <code>Miniforge3-22.3.1.0</code>. <strong>Miniforge</strong> comes with the popular <code>conda-forge</code> channel preconfigured, but you can modify the configuration to use any channel you like.</p>
<p>For Unix-like platforms (MAC OS and Linux), download the installer using curl or wget or your favorite program and run the script</p>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash">curl -L -O <span class="s2">&#34;https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-</span><span class="k">$(</span>uname<span class="k">)</span><span class="s2">-</span><span class="k">$(</span>uname -m<span class="k">)</span><span class="s2">.sh&#34;</span>
bash Miniforge3-<span class="k">$(</span>uname<span class="k">)</span>-<span class="k">$(</span>uname -m<span class="k">)</span>.sh
</code></pre></div><p>After successful installation, you can use use the mamba commands as described in this <a href="https://mamba.readthedocs.io/en/latest/user_guide/mamba.html#mamba"><strong>user guide</strong></a>.</p>
<div class="admonition note"><p class="admonition-title">Working with Conda Environment</p>
<p>The command using <code>mamba</code> is similar to the <code>conda</code> command.</p>
<ul>
<li>Create new environment</li>
</ul>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash">mamba create -n <span class="p">&amp;</span>lt<span class="p">;</span>ENV_NAME<span class="p">&amp;</span>gt<span class="p">;</span> <span class="nv">python</span><span class="o">=</span><span class="p">&amp;</span>lt<span class="p">;</span>VERSION<span class="p">&amp;</span>gt<span class="p">;</span>
</code></pre></div><ul>
<li>Activate an environment</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash">mamba activate <span class="p">&amp;</span>lt<span class="p">;</span>ENV_NAME<span class="p">&amp;</span>gt<span class="p">;</span>
</code></pre></div><ul>
<li>Deactivate environment</li>
</ul>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash">mamba deactivate
</code></pre></div><ul>
<li>Delete an environment</li>
</ul>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash">mamba env remove -n <span class="p">&amp;</span>lt<span class="p">;</span>ENV_NAME<span class="p">&amp;</span>gt<span class="p">;</span>
</code></pre></div><ul>
<li>Show all created environments</li>
</ul>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash">mamba env list
</code></pre></div><p><strong>Remember to activate an environment first, do not install any packages in your base environment!</strong></p>
<ul>
<li>Install python packages</li>
</ul>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash">mamba install <span class="p">&amp;</span>lt<span class="p">;</span>PKG_NAME<span class="p">&amp;</span>gt<span class="p">;</span><span class="o">[=</span>VERSION<span class="o">]</span> <span class="o">[</span>-c <span class="p">&amp;</span>lt<span class="p">;</span>CHANNEL_NAME<span class="p">&amp;</span>gt<span class="p">;</span><span class="o">]</span>
</code></pre></div><p>When installing a package, you can optionally indicate specific additional channel that the packages are posted by community. The <code>conda-forge</code> is one of most common additional channels.</p>
<ul>
<li>Delete packages</li>
</ul>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash">mamba remove <span class="p">&amp;</span>lt<span class="p">;</span>PKG_NAME<span class="p">&amp;</span>gt<span class="p">;</span>
</code></pre></div><ul>
<li>Show all installed packages in the virtual environment</li>
</ul>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash">mamba list <span class="o">[</span>-n <span class="p">&amp;</span>lt<span class="p">;</span>ENV_NAME<span class="p">&amp;</span>gt<span class="p">;</span><span class="o">]</span>
</code></pre></div></div>
<h2 id="2-installing-nvidia-cuda-toolkit-for-wsl-2">2. Installing NVIDIA CUDA Toolkit for WSL 2</h2>
<p>Note that you <strong>only need to install NVIDIA Driver for Windows. Do not install any Linux Driver in WSL</strong>.</p>
<p>The latest NVIDIA Windows GPU Driver will fully support WSL 2. With CUDA support in the driver, existing applications compiled on a Linux system for the same target GPU can run unmodified within the WSL environment. Once NVIDIA Windows GPU Driver is installed in the system, the CUDA driver will be stubbed inside the WSL 2 as <code>libcuda.so</code>. Therefore, you only use a separate CUDA Toolkit for WSL 2 which does not contain the NVIDIA Linux GPU Driver.</p>
<p>To get started with running CUDA on <strong>WSL 2</strong>, you need to instal NVIDIA Driver on Windows 11 with a compatible GeForce or NVIDIA RTX/Quadro card from <a href="https://www.nvidia.com/Download/index.aspx"><strong>here</strong></a>. Once a <strong>Windows NVIDIA GPU driver</strong> is installed on the system, CUDA becomes available within <strong>WSL 2</strong>. The <strong>CUDA driver</strong> installed on <strong>Windows host</strong> will be stubbed inside the WSL 2 as <code>libcuda.so</code>.</p>
<p>However, the <strong>NVIDIA CUDA Toolkit for WSL</strong> need to be install for further use.</p>
<p>First, remove the old GPG key:</p>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash">sudo apt-key del 7fa2af80
</code></pre></div><p>Install the <strong>NVIDIA CUDA Toolkit for WSL-Ubuntu</strong> by following the step in <a href="https://developer.nvidia.com/cuda-downloads?target_os=Linux&amp;target_arch=x86_64&amp;Distribution=WSL-Ubuntu&amp;target_version=2.0&amp;target_type=deb_local"><strong>this link</strong></a></p>
<p>Download <a href="https://developer.nvidia.com/cuda-downloads?target_os=Linux&amp;target_arch=x86_64&amp;Distribution=WSL-Ubuntu&amp;target_version=2.0&amp;target_type=deb_local">CUDA Toolkit for WSL 2</a>.</p>
<p><img loading="lazy" src="/posts/setitng-up-nvidia-cuda-on-wsl2/nvidia-cuda-toolkit-installation.png" type="" alt="NVIDIA CUDA Toolkit Installation"  /></p>
<p>Some actions must be taken after the installation before the CUDA Toolkit and Driver can be used. The <code>PATH</code> and <code>LD_LIBRARY_PATH</code> variables need to be updated.</p>
<p>Open <code>~/.bashrc</code> by running <code>nano ~/.bashrc</code>, add two lines at the end of the file:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="nb">export</span> <span class="nv">PATH</span><span class="o">=</span>/usr/local/cuda-12.4/bin<span class="si">${</span><span class="nv">PATH</span><span class="p">:+:</span><span class="si">${</span><span class="nv">PATH</span><span class="si">}}</span>
<span class="nb">export</span> <span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span>/usr/local/cuda-12.4/lib64<span class="si">${</span><span class="nv">LD_LIBRARY_PATH</span><span class="p">:+:</span><span class="si">${</span><span class="nv">LD_LIBRARY_PATH</span><span class="si">}}</span>
</code></pre></div><p>Then you run <code>source ~/.bashrc</code> for activation changes without reboot.</p>
<details class="admonition info"><summary class="admonition-title">Verify CUDA Toolkit Installation</summary>
<p>You can verify the installation by running the following command:</p>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash">nvcc --version
</code></pre></div><p>If the installation was successful, you should see the CUDA version information displayed.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash">nvcc: NVIDIA <span class="o">(</span>R<span class="o">)</span> Cuda compiler driver
Copyright <span class="o">(</span>c<span class="o">)</span> 2005-2024 NVIDIA Corporation
Built on Thu_Mar_28_02:18:24_PDT_2024
Cuda compilation tools, release 12.4, V12.4.131
Build cuda_12.4.r12.4/compiler.34097967_0
</code></pre></div><p>It is important to verify that the <strong>NVIDIA CUDA Toolkit</strong> can find and communicate correctly with CUDA-compatible hardware. To do this, you need to compile and run some sample programs.</p>
<p>CUDA samples are located in <a href="https://github.com/nvidia/cuda-samples"><strong>https://github.com/nvidia/cuda-samples</strong></a>. To use the samples, clone the project, build the samples in <code>cuda-samples</code> directory using <code>make</code> command and run them following the instruction on the Github page.</p>
<p>To verify a correct configuration of the hardware and software, it is highly recommended that you build and run the <code>deviceQuery</code> sample program.</p>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash">./deviceQuery
</code></pre></div><p>If  CUDA is installed and configured correctly, the output should look similar as below:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash">./deviceQuery Starting...

 CUDA Device Query <span class="o">(</span>Runtime API<span class="o">)</span> version <span class="o">(</span>CUDART static linking<span class="o">)</span>

Detected <span class="m">1</span> CUDA Capable device<span class="o">(</span>s<span class="o">)</span>

Device 0: <span class="s2">&#34;NVIDIA GeForce RTX 3070 Ti Laptop GPU&#34;</span>
  CUDA Driver Version / Runtime Version          12.4 / 12.4
  CUDA Capability Major/Minor version number:    8.6
  Total amount of global memory:                 <span class="m">8192</span> MBytes <span class="o">(</span><span class="m">8589410304</span> bytes<span class="o">)</span>
  <span class="o">(</span>046<span class="o">)</span> Multiprocessors, <span class="o">(</span>128<span class="o">)</span> CUDA Cores/MP:    <span class="m">5888</span> CUDA Cores
  GPU Max Clock rate:                            <span class="m">1410</span> MHz <span class="o">(</span>1.41 GHz<span class="o">)</span>
  Memory Clock rate:                             <span class="m">7001</span> Mhz
  Memory Bus Width:                              256-bit
  L2 Cache Size:                                 <span class="m">4194304</span> bytes
  Maximum Texture Dimension Size <span class="o">(</span>x,y,z<span class="o">)</span>         <span class="nv">1D</span><span class="o">=(</span>131072<span class="o">)</span>, <span class="nv">2D</span><span class="o">=(</span>131072, 65536<span class="o">)</span>, <span class="nv">3D</span><span class="o">=(</span>16384, 16384, 16384<span class="o">)</span>
  Maximum Layered 1D Texture Size, <span class="o">(</span>num<span class="o">)</span> layers  <span class="nv">1D</span><span class="o">=(</span>32768<span class="o">)</span>, <span class="m">2048</span> layers
  Maximum Layered 2D Texture Size, <span class="o">(</span>num<span class="o">)</span> layers  <span class="nv">2D</span><span class="o">=(</span>32768, 32768<span class="o">)</span>, <span class="m">2048</span> layers
  Total amount of constant memory:               <span class="m">65536</span> bytes
  Total amount of shared memory per block:       <span class="m">49152</span> bytes
  Total shared memory per multiprocessor:        <span class="m">102400</span> bytes
  Total number of registers available per block: <span class="m">65536</span>
  Warp size:                                     <span class="m">32</span>
  Maximum number of threads per multiprocessor:  <span class="m">1536</span>
  Maximum number of threads per block:           <span class="m">1024</span>
  Max dimension size of a thread block <span class="o">(</span>x,y,z<span class="o">)</span>: <span class="o">(</span>1024, 1024, 64<span class="o">)</span>
  Max dimension size of a grid size    <span class="o">(</span>x,y,z<span class="o">)</span>: <span class="o">(</span>2147483647, 65535, 65535<span class="o">)</span>
  Maximum memory pitch:                          <span class="m">2147483647</span> bytes
  Texture alignment:                             <span class="m">512</span> bytes
  Concurrent copy and kernel execution:          Yes with <span class="m">1</span> copy engine<span class="o">(</span>s<span class="o">)</span>
  Run <span class="nb">time</span> limit on kernels:                     Yes
  Integrated GPU sharing Host Memory:            No
  Support host page-locked memory mapping:       Yes
  Alignment requirement <span class="k">for</span> Surfaces:            Yes
  Device has ECC support:                        Disabled
  Device supports Unified Addressing <span class="o">(</span>UVA<span class="o">)</span>:      Yes
  Device supports Managed Memory:                Yes
  Device supports Compute Preemption:            Yes
  Supports Cooperative Kernel Launch:            Yes
  Supports MultiDevice Co-op Kernel Launch:      No
  Device PCI Domain ID / Bus ID / location ID:   <span class="m">0</span> / <span class="m">1</span> / <span class="m">0</span>
  Compute Mode:
     &lt; Default <span class="o">(</span>multiple host threads can use ::cudaSetDevice<span class="o">()</span> with device simultaneously<span class="o">)</span> &gt;

deviceQuery, CUDA <span class="nv">Driver</span> <span class="o">=</span> CUDART, CUDA Driver <span class="nv">Version</span> <span class="o">=</span> 12.4, CUDA Runtime <span class="nv">Version</span> <span class="o">=</span> 12.4, <span class="nv">NumDevs</span> <span class="o">=</span> <span class="m">1</span>
<span class="nv">Result</span> <span class="o">=</span> PASS
</code></pre></div><p>By running the <code>bandwidthTest</code> program, you can ensure that the system and CUDA-capable device are able to communicate correctly.</p>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash">./bandwidthText
</code></pre></div><p>The output shoud be here.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="o">[</span>CUDA Bandwidth Test<span class="o">]</span> - Starting...
Running on...

 Device 0: NVIDIA GeForce RTX <span class="m">3070</span> Ti Laptop GPU
 Quick Mode

 Host to Device Bandwidth, <span class="m">1</span> Device<span class="o">(</span>s<span class="o">)</span>
 PINNED Memory Transfers
   Transfer Size <span class="o">(</span>Bytes<span class="o">)</span>        Bandwidth<span class="o">(</span>GB/s<span class="o">)</span>
   <span class="m">32000000</span>                     24.5

 Device to Host Bandwidth, <span class="m">1</span> Device<span class="o">(</span>s<span class="o">)</span>
 PINNED Memory Transfers
   Transfer Size <span class="o">(</span>Bytes<span class="o">)</span>        Bandwidth<span class="o">(</span>GB/s<span class="o">)</span>
   <span class="m">32000000</span>                     26.3

 Device to Device Bandwidth, <span class="m">1</span> Device<span class="o">(</span>s<span class="o">)</span>
 PINNED Memory Transfers
   Transfer Size <span class="o">(</span>Bytes<span class="o">)</span>        Bandwidth<span class="o">(</span>GB/s<span class="o">)</span>
   <span class="m">32000000</span>                     323.6

<span class="nv">Result</span> <span class="o">=</span> PASS

NOTE: The CUDA Samples are not meant <span class="k">for</span> performance measurements. Results may vary when GPU Boost is enabled.
</code></pre></div><p>For CUDA graphic programs, <strong>WSL 2</strong> currently does not support <strong>GL</strong>, <strong>Vulcan</strong>. Hence, you cannot run CUDA graphic programs.</p>
</details>
<h2 id="2-installing-nvidia-cudnn-for-wsl-2">2. Installing NVIDIA CuDNN for WSL 2</h2>
<p>Since <strong>cuDNN version 9</strong> can exist independently with previous version <strong>cuDNN</strong>, you can refer <a href="https://developer.nvidia.com/cudnn-downloads?target_os=Linux&amp;target_arch=x86_64&amp;Distribution=Ubuntu&amp;target_version=22.04&amp;target_type=deb_local"><strong>here</strong></a> and follow the steps for your target platform.</p>
<p><img loading="lazy" src="/posts/setitng-up-nvidia-cuda-on-wsl2/nvidia-cudnn-installation.png" type="" alt="NVIDIA cuDNN Installation"  /></p>
<details class="admonition info"><summary class="admonition-title">Verify cuDNN Installation</summary>
<p>To verify <strong>cuDNN</strong> is install and is running properly, you can download and compile <strong>cuDNN</strong> samples. These samples can be downloaded by running:</p>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash">sudo apt install -y libcudnn9-samples
</code></pre></div><p>The samples is located in <strong>/usr/src/cudnn_samples_v9</strong> directory, copy the directory to <code>$HOME</code> and set the permission.</p>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash">sudo cp -r cudnn_samples_v9/ <span class="nv">$HOME</span>
sudo chmod -R a+rwx <span class="nv">$HOME</span>/cudnn_samples_v9
</code></pre></div><p>For the <code>mnistCUDNN</code> sample, install <code>FreeImage</code> package and compile the sample.</p>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash">sudo apt install -y libfreeimage3 libfreeimage-dev
<span class="nb">cd</span> <span class="nv">$HOME</span>/cudnn_samples_v9/mnistCUDNN
make clean <span class="o">&amp;&amp;</span> make
./mnistCUDNN
</code></pre></div><p>The result should be as:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash">Executing: mnistCUDNN
cudnnGetVersion<span class="o">()</span> : <span class="m">90100</span> , CUDNN_VERSION from cudnn.h : <span class="m">90100</span> <span class="o">(</span>9.1.0<span class="o">)</span>
Host compiler version : GCC 11.4.0

There are <span class="m">1</span> CUDA capable devices on your machine :
device <span class="m">0</span> : sms <span class="m">46</span>  Capabilities 8.6, SmClock 1410.0 Mhz, MemSize <span class="o">(</span>Mb<span class="o">)</span> 8191, MemClock 7001.0 Mhz, <span class="nv">Ecc</span><span class="o">=</span>0, <span class="nv">boardGroupID</span><span class="o">=</span><span class="m">0</span>
Using device <span class="m">0</span>

Testing single precision
Loading binary file data/conv1.bin
Loading binary file data/conv1.bias.bin
Loading binary file data/conv2.bin
Loading binary file data/conv2.bias.bin
Loading binary file data/ip1.bin
Loading binary file data/ip1.bias.bin
Loading binary file data/ip2.bin
Loading binary file data/ip2.bias.bin
Loading image data/one_28x28.pgm
Performing forward propagation ...
Testing cudnnGetConvolutionForwardAlgorithm_v7 ...
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 1: -1.000000 <span class="nb">time</span> requiring <span class="m">0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 0: -1.000000 <span class="nb">time</span> requiring <span class="m">0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 2: -1.000000 <span class="nb">time</span> requiring <span class="m">0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 5: -1.000000 <span class="nb">time</span> requiring <span class="m">178432</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 4: -1.000000 <span class="nb">time</span> requiring <span class="m">184784</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 7: -1.000000 <span class="nb">time</span> requiring <span class="m">2057744</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class="k">for</span> Algo 6: -1.000000 <span class="nb">time</span> requiring <span class="m">0</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class="k">for</span> Algo 3: -1.000000 <span class="nb">time</span> requiring <span class="m">0</span> memory
Testing cudnnFindConvolutionForwardAlgorithm ...
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 0: 0.009216 <span class="nb">time</span> requiring <span class="m">0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 1: 0.009216 <span class="nb">time</span> requiring <span class="m">0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 5: 0.096256 <span class="nb">time</span> requiring <span class="m">178432</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 7: 0.115712 <span class="nb">time</span> requiring <span class="m">2057744</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 4: 0.168960 <span class="nb">time</span> requiring <span class="m">184784</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 2: 0.546016 <span class="nb">time</span> requiring <span class="m">0</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class="k">for</span> Algo 6: -1.000000 <span class="nb">time</span> requiring <span class="m">0</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class="k">for</span> Algo 3: -1.000000 <span class="nb">time</span> requiring <span class="m">0</span> memory
Testing cudnnGetConvolutionForwardAlgorithm_v7 ...
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 1: -1.000000 <span class="nb">time</span> requiring <span class="m">0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 0: -1.000000 <span class="nb">time</span> requiring <span class="m">0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 2: -1.000000 <span class="nb">time</span> requiring <span class="m">0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 4: -1.000000 <span class="nb">time</span> requiring <span class="m">2450080</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 5: -1.000000 <span class="nb">time</span> requiring <span class="m">4656640</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 7: -1.000000 <span class="nb">time</span> requiring <span class="m">1433120</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class="k">for</span> Algo 6: -1.000000 <span class="nb">time</span> requiring <span class="m">0</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class="k">for</span> Algo 3: -1.000000 <span class="nb">time</span> requiring <span class="m">0</span> memory
Testing cudnnFindConvolutionForwardAlgorithm ...
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 0: 0.035840 <span class="nb">time</span> requiring <span class="m">0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 1: 0.058368 <span class="nb">time</span> requiring <span class="m">0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 2: 0.066560 <span class="nb">time</span> requiring <span class="m">0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 7: 0.093184 <span class="nb">time</span> requiring <span class="m">1433120</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 5: 0.109600 <span class="nb">time</span> requiring <span class="m">4656640</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 4: 0.192512 <span class="nb">time</span> requiring <span class="m">2450080</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class="k">for</span> Algo 6: -1.000000 <span class="nb">time</span> requiring <span class="m">0</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class="k">for</span> Algo 3: -1.000000 <span class="nb">time</span> requiring <span class="m">0</span> memory
Resulting weights from Softmax:
0.0000000 0.9999399 0.0000000 0.0000000 0.0000561 0.0000000 0.0000012 0.0000017 0.0000010 0.0000000
Loading image data/three_28x28.pgm
Performing forward propagation ...
Testing cudnnGetConvolutionForwardAlgorithm_v7 ...
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 1: -1.000000 <span class="nb">time</span> requiring <span class="m">0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 0: -1.000000 <span class="nb">time</span> requiring <span class="m">0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 2: -1.000000 <span class="nb">time</span> requiring <span class="m">0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 5: -1.000000 <span class="nb">time</span> requiring <span class="m">178432</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 4: -1.000000 <span class="nb">time</span> requiring <span class="m">184784</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 7: -1.000000 <span class="nb">time</span> requiring <span class="m">2057744</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class="k">for</span> Algo 6: -1.000000 <span class="nb">time</span> requiring <span class="m">0</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class="k">for</span> Algo 3: -1.000000 <span class="nb">time</span> requiring <span class="m">0</span> memory
Testing cudnnFindConvolutionForwardAlgorithm ...
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 1: 0.008192 <span class="nb">time</span> requiring <span class="m">0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 2: 0.009216 <span class="nb">time</span> requiring <span class="m">0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 0: 0.026624 <span class="nb">time</span> requiring <span class="m">0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 7: 0.061248 <span class="nb">time</span> requiring <span class="m">2057744</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 4: 0.073728 <span class="nb">time</span> requiring <span class="m">184784</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 5: 0.106496 <span class="nb">time</span> requiring <span class="m">178432</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class="k">for</span> Algo 6: -1.000000 <span class="nb">time</span> requiring <span class="m">0</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class="k">for</span> Algo 3: -1.000000 <span class="nb">time</span> requiring <span class="m">0</span> memory
Testing cudnnGetConvolutionForwardAlgorithm_v7 ...
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 1: -1.000000 <span class="nb">time</span> requiring <span class="m">0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 0: -1.000000 <span class="nb">time</span> requiring <span class="m">0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 2: -1.000000 <span class="nb">time</span> requiring <span class="m">0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 4: -1.000000 <span class="nb">time</span> requiring <span class="m">2450080</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 5: -1.000000 <span class="nb">time</span> requiring <span class="m">4656640</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 7: -1.000000 <span class="nb">time</span> requiring <span class="m">1433120</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class="k">for</span> Algo 6: -1.000000 <span class="nb">time</span> requiring <span class="m">0</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class="k">for</span> Algo 3: -1.000000 <span class="nb">time</span> requiring <span class="m">0</span> memory
Testing cudnnFindConvolutionForwardAlgorithm ...
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 0: 0.031520 <span class="nb">time</span> requiring <span class="m">0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 1: 0.038912 <span class="nb">time</span> requiring <span class="m">0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 2: 0.066400 <span class="nb">time</span> requiring <span class="m">0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 7: 0.091136 <span class="nb">time</span> requiring <span class="m">1433120</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 5: 0.129024 <span class="nb">time</span> requiring <span class="m">4656640</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 4: 0.145408 <span class="nb">time</span> requiring <span class="m">2450080</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class="k">for</span> Algo 6: -1.000000 <span class="nb">time</span> requiring <span class="m">0</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class="k">for</span> Algo 3: -1.000000 <span class="nb">time</span> requiring <span class="m">0</span> memory
Resulting weights from Softmax:
0.0000000 0.0000000 0.0000000 0.9999288 0.0000000 0.0000711 0.0000000 0.0000000 0.0000000 0.0000000
Loading image data/five_28x28.pgm
Performing forward propagation ...
Resulting weights from Softmax:
0.0000000 0.0000008 0.0000000 0.0000002 0.0000000 0.9999820 0.0000154 0.0000000 0.0000012 0.0000006

Result of classification: <span class="m">1</span> <span class="m">3</span> <span class="m">5</span>

Test passed!

Testing half precision <span class="o">(</span>math in single precision<span class="o">)</span>
Loading binary file data/conv1.bin
Loading binary file data/conv1.bias.bin
Loading binary file data/conv2.bin
Loading binary file data/conv2.bias.bin
Loading binary file data/ip1.bin
Loading binary file data/ip1.bias.bin
Loading binary file data/ip2.bin
Loading binary file data/ip2.bias.bin
Loading image data/one_28x28.pgm
Performing forward propagation ...
Testing cudnnGetConvolutionForwardAlgorithm_v7 ...
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 1: -1.000000 <span class="nb">time</span> requiring <span class="m">0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 0: -1.000000 <span class="nb">time</span> requiring <span class="m">0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 2: -1.000000 <span class="nb">time</span> requiring <span class="m">0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 5: -1.000000 <span class="nb">time</span> requiring <span class="m">178432</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 4: -1.000000 <span class="nb">time</span> requiring <span class="m">184784</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 7: -1.000000 <span class="nb">time</span> requiring <span class="m">2057744</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class="k">for</span> Algo 6: -1.000000 <span class="nb">time</span> requiring <span class="m">0</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class="k">for</span> Algo 3: -1.000000 <span class="nb">time</span> requiring <span class="m">0</span> memory
Testing cudnnFindConvolutionForwardAlgorithm ...
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 2: 0.009216 <span class="nb">time</span> requiring <span class="m">0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 0: 0.010240 <span class="nb">time</span> requiring <span class="m">0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 1: 0.017408 <span class="nb">time</span> requiring <span class="m">0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 7: 0.068512 <span class="nb">time</span> requiring <span class="m">2057744</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 5: 0.070784 <span class="nb">time</span> requiring <span class="m">178432</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 4: 0.103424 <span class="nb">time</span> requiring <span class="m">184784</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class="k">for</span> Algo 6: -1.000000 <span class="nb">time</span> requiring <span class="m">0</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class="k">for</span> Algo 3: -1.000000 <span class="nb">time</span> requiring <span class="m">0</span> memory
Testing cudnnGetConvolutionForwardAlgorithm_v7 ...
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 7: -1.000000 <span class="nb">time</span> requiring <span class="m">1433120</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 1: -1.000000 <span class="nb">time</span> requiring <span class="m">1536</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 0: -1.000000 <span class="nb">time</span> requiring <span class="m">0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 2: -1.000000 <span class="nb">time</span> requiring <span class="m">64000</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 5: -1.000000 <span class="nb">time</span> requiring <span class="m">4656640</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 4: -1.000000 <span class="nb">time</span> requiring <span class="m">2450080</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class="k">for</span> Algo 6: -1.000000 <span class="nb">time</span> requiring <span class="m">0</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class="k">for</span> Algo 3: -1.000000 <span class="nb">time</span> requiring <span class="m">0</span> memory
Testing cudnnFindConvolutionForwardAlgorithm ...
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 0: 0.079872 <span class="nb">time</span> requiring <span class="m">0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 1: 0.079872 <span class="nb">time</span> requiring <span class="m">1536</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 2: 0.101376 <span class="nb">time</span> requiring <span class="m">64000</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 5: 0.101440 <span class="nb">time</span> requiring <span class="m">4656640</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 4: 0.105472 <span class="nb">time</span> requiring <span class="m">2450080</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 7: 0.126976 <span class="nb">time</span> requiring <span class="m">1433120</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class="k">for</span> Algo 6: -1.000000 <span class="nb">time</span> requiring <span class="m">0</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class="k">for</span> Algo 3: -1.000000 <span class="nb">time</span> requiring <span class="m">0</span> memory
Resulting weights from Softmax:
0.0000001 1.0000000 0.0000001 0.0000000 0.0000563 0.0000001 0.0000012 0.0000017 0.0000010 0.0000001
Loading image data/three_28x28.pgm
Performing forward propagation ...
Testing cudnnGetConvolutionForwardAlgorithm_v7 ...
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 1: -1.000000 <span class="nb">time</span> requiring <span class="m">0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 0: -1.000000 <span class="nb">time</span> requiring <span class="m">0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 2: -1.000000 <span class="nb">time</span> requiring <span class="m">0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 5: -1.000000 <span class="nb">time</span> requiring <span class="m">178432</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 4: -1.000000 <span class="nb">time</span> requiring <span class="m">184784</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 7: -1.000000 <span class="nb">time</span> requiring <span class="m">2057744</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class="k">for</span> Algo 6: -1.000000 <span class="nb">time</span> requiring <span class="m">0</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class="k">for</span> Algo 3: -1.000000 <span class="nb">time</span> requiring <span class="m">0</span> memory
Testing cudnnFindConvolutionForwardAlgorithm ...
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 2: 0.010240 <span class="nb">time</span> requiring <span class="m">0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 0: 0.023552 <span class="nb">time</span> requiring <span class="m">0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 1: 0.064480 <span class="nb">time</span> requiring <span class="m">0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 7: 0.087008 <span class="nb">time</span> requiring <span class="m">2057744</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 4: 0.103424 <span class="nb">time</span> requiring <span class="m">184784</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 5: 0.138144 <span class="nb">time</span> requiring <span class="m">178432</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class="k">for</span> Algo 6: -1.000000 <span class="nb">time</span> requiring <span class="m">0</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class="k">for</span> Algo 3: -1.000000 <span class="nb">time</span> requiring <span class="m">0</span> memory
Testing cudnnGetConvolutionForwardAlgorithm_v7 ...
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 7: -1.000000 <span class="nb">time</span> requiring <span class="m">1433120</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 1: -1.000000 <span class="nb">time</span> requiring <span class="m">1536</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 0: -1.000000 <span class="nb">time</span> requiring <span class="m">0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 2: -1.000000 <span class="nb">time</span> requiring <span class="m">64000</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 5: -1.000000 <span class="nb">time</span> requiring <span class="m">4656640</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 4: -1.000000 <span class="nb">time</span> requiring <span class="m">2450080</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class="k">for</span> Algo 6: -1.000000 <span class="nb">time</span> requiring <span class="m">0</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class="k">for</span> Algo 3: -1.000000 <span class="nb">time</span> requiring <span class="m">0</span> memory
Testing cudnnFindConvolutionForwardAlgorithm ...
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 0: 0.083904 <span class="nb">time</span> requiring <span class="m">0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 7: 0.090112 <span class="nb">time</span> requiring <span class="m">1433120</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 5: 0.094976 <span class="nb">time</span> requiring <span class="m">4656640</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 4: 0.095232 <span class="nb">time</span> requiring <span class="m">2450080</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 2: 0.111616 <span class="nb">time</span> requiring <span class="m">64000</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for</span> Algo 1: 0.186368 <span class="nb">time</span> requiring <span class="m">1536</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class="k">for</span> Algo 6: -1.000000 <span class="nb">time</span> requiring <span class="m">0</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class="k">for</span> Algo 3: -1.000000 <span class="nb">time</span> requiring <span class="m">0</span> memory
Resulting weights from Softmax:
0.0000000 0.0000000 0.0000000 1.0000000 0.0000000 0.0000714 0.0000000 0.0000000 0.0000000 0.0000000
Loading image data/five_28x28.pgm
Performing forward propagation ...
Resulting weights from Softmax:
0.0000000 0.0000008 0.0000000 0.0000002 0.0000000 1.0000000 0.0000154 0.0000000 0.0000012 0.0000006

Result of classification: <span class="m">1</span> <span class="m">3</span> <span class="m">5</span>

Test passed!
</code></pre></div></details>
<h2 id="3-installing-nvidia-tensorrt-optional">3. Installing NVIDIA TensorRT (Optional)</h2>
<p><strong>NVIDIA TensorRT</strong> is a C++ library that facilitates high-performance inference NVIDIA graphic processing units (GPUs). <strong>TensorRT</strong> takes a trained network, which consists of a network definition and a set of trained parameters, and produces a highly optimized runtime engine that performs inference for that network.</p>
<p><strong>TensorRT</strong> provides APIs via C++ and Python that help to express deep learning model via the Network Definition API or load a pre-defined model via the ONNX parser that allow <strong>TensorRT</strong> to optimize and run them on the NVIDIA GPU. <strong>TensorRT</strong> also include optional high speed mixed precision capabilities with difference NVIDIA architectures.</p>
<p>You can download <strong>TensorRT</strong> local repo that matches the Ubuntu version and CPU architecture of your machine from <a href="https://developer.nvidia.com/tensorrt/download/10x"><strong>here</strong></a>.</p>
<p>Install the Debian local package.</p>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="nv">os</span><span class="o">=</span><span class="s2">&#34;ubuntuxx04&#34;</span>
<span class="nv">tag</span><span class="o">=</span><span class="s2">&#34;10.x.x-cuda-x.x&#34;</span>
sudo dpkg -i nv-tensorrt-local-repo-<span class="si">${</span><span class="nv">os</span><span class="si">}</span>-<span class="si">${</span><span class="nv">tag</span><span class="si">}</span>_1.0-1_amd64.deb
sudo cp /var/nv-tensorrt-local-repo-<span class="si">${</span><span class="nv">os</span><span class="si">}</span>-<span class="si">${</span><span class="nv">tag</span><span class="si">}</span>/*-keyring.gpg /usr/share/keyrings/
sudo apt-get update
</code></pre></div><p>Replace <code>ubuntuxx04</code>, <code>10.x.x</code> and <code>cuda-x.x</code> with your specific OS version, TensorRT version, and CUDA version. The full <strong>TensorRT</strong> C++ and Python runtimes can be installed by:</p>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash">sudo apt install -y tensorrt
</code></pre></div><details class="admonition info"><summary class="admonition-title">Verify TensorRT Installation</summary>
<p>To verify the installation, run the command:</p>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash">dpkg-query -W tensorrt
</code></pre></div><p>The ouput should be as following:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash">tensorrt        10.0.0.6-1+cuda12.4
</code></pre></div><p>The samples is located in <strong>/usr/src/tensorrt</strong> directory, copy the directory to <code>$HOME</code> and set the permission.</p>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash">sudo cp -r tensorrt/ <span class="nv">$HOME</span>
sudo chmod -R a+rwx <span class="nv">$HOME</span>/tensorrt
</code></pre></div><p>For the <code>sampleOnnxMNIST</code> sample, compile the sample and run the program.</p>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="nb">cd</span> <span class="nv">$HOME</span>/tensorrt/samples/sampleOnnxMNIST
make clean <span class="o">&amp;&amp;</span> make
<span class="nb">cd</span> ../../bin
./sampleOnnxMNIST
</code></pre></div><p>The output should be as following:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="o">&amp;&amp;&amp;&amp;</span> RUNNING TensorRT.sample_onnx_mnist <span class="o">[</span>TensorRT v100000<span class="o">]</span> <span class="c1"># ./sample_onnx_mnist</span>
<span class="o">[</span>04/15/2024-18:24:17<span class="o">]</span> <span class="o">[</span>I<span class="o">]</span> Building and running a GPU inference engine <span class="k">for</span> Onnx MNIST
<span class="o">[</span>04/15/2024-18:24:17<span class="o">]</span> <span class="o">[</span>I<span class="o">]</span> <span class="o">[</span>TRT<span class="o">]</span> <span class="o">[</span>MemUsageChange<span class="o">]</span> Init CUDA: CPU +19, GPU +0, now: CPU 21, GPU <span class="m">1091</span> <span class="o">(</span>MiB<span class="o">)</span>
<span class="o">[</span>04/15/2024-18:24:19<span class="o">]</span> <span class="o">[</span>I<span class="o">]</span> <span class="o">[</span>TRT<span class="o">]</span> <span class="o">[</span>MemUsageChange<span class="o">]</span> Init builder kernel library: CPU +1765, GPU +312, now: CPU 1922, GPU <span class="m">1403</span> <span class="o">(</span>MiB<span class="o">)</span>
<span class="o">[</span>04/15/2024-18:24:19<span class="o">]</span> <span class="o">[</span>I<span class="o">]</span> <span class="o">[</span>TRT<span class="o">]</span> ----------------------------------------------------------------
<span class="o">[</span>04/15/2024-18:24:19<span class="o">]</span> <span class="o">[</span>I<span class="o">]</span> <span class="o">[</span>TRT<span class="o">]</span> Input filename:   ../data/mnist/mnist.onnx
<span class="o">[</span>04/15/2024-18:24:19<span class="o">]</span> <span class="o">[</span>I<span class="o">]</span> <span class="o">[</span>TRT<span class="o">]</span> ONNX IR version:  0.0.3
<span class="o">[</span>04/15/2024-18:24:19<span class="o">]</span> <span class="o">[</span>I<span class="o">]</span> <span class="o">[</span>TRT<span class="o">]</span> Opset version:    <span class="m">8</span>
<span class="o">[</span>04/15/2024-18:24:19<span class="o">]</span> <span class="o">[</span>I<span class="o">]</span> <span class="o">[</span>TRT<span class="o">]</span> Producer name:    CNTK
<span class="o">[</span>04/15/2024-18:24:19<span class="o">]</span> <span class="o">[</span>I<span class="o">]</span> <span class="o">[</span>TRT<span class="o">]</span> Producer version: 2.5.1
<span class="o">[</span>04/15/2024-18:24:19<span class="o">]</span> <span class="o">[</span>I<span class="o">]</span> <span class="o">[</span>TRT<span class="o">]</span> Domain:           ai.cntk
<span class="o">[</span>04/15/2024-18:24:19<span class="o">]</span> <span class="o">[</span>I<span class="o">]</span> <span class="o">[</span>TRT<span class="o">]</span> Model version:    <span class="m">1</span>
<span class="o">[</span>04/15/2024-18:24:19<span class="o">]</span> <span class="o">[</span>I<span class="o">]</span> <span class="o">[</span>TRT<span class="o">]</span> Doc string:
<span class="o">[</span>04/15/2024-18:24:19<span class="o">]</span> <span class="o">[</span>I<span class="o">]</span> <span class="o">[</span>TRT<span class="o">]</span> ----------------------------------------------------------------
<span class="o">[</span>04/15/2024-18:24:19<span class="o">]</span> <span class="o">[</span>I<span class="o">]</span> <span class="o">[</span>TRT<span class="o">]</span> Local timing cache in use. Profiling results in this builder pass will not be stored.
<span class="o">[</span>04/15/2024-18:24:21<span class="o">]</span> <span class="o">[</span>I<span class="o">]</span> <span class="o">[</span>TRT<span class="o">]</span> Detected <span class="m">1</span> inputs and <span class="m">1</span> output network tensors.
<span class="o">[</span>04/15/2024-18:24:21<span class="o">]</span> <span class="o">[</span>I<span class="o">]</span> <span class="o">[</span>TRT<span class="o">]</span> Total Host Persistent Memory: <span class="m">26272</span>
<span class="o">[</span>04/15/2024-18:24:21<span class="o">]</span> <span class="o">[</span>I<span class="o">]</span> <span class="o">[</span>TRT<span class="o">]</span> Total Device Persistent Memory: <span class="m">0</span>
<span class="o">[</span>04/15/2024-18:24:21<span class="o">]</span> <span class="o">[</span>I<span class="o">]</span> <span class="o">[</span>TRT<span class="o">]</span> Total Scratch Memory: <span class="m">0</span>
<span class="o">[</span>04/15/2024-18:24:21<span class="o">]</span> <span class="o">[</span>I<span class="o">]</span> <span class="o">[</span>TRT<span class="o">]</span> <span class="o">[</span>BlockAssignment<span class="o">]</span> Started assigning block shifts. This will take <span class="m">6</span> steps to complete.
<span class="o">[</span>04/15/2024-18:24:21<span class="o">]</span> <span class="o">[</span>I<span class="o">]</span> <span class="o">[</span>TRT<span class="o">]</span> <span class="o">[</span>BlockAssignment<span class="o">]</span> Algorithm ShiftNTopDown took 0.02021ms to assign <span class="m">3</span> blocks to <span class="m">6</span> nodes requiring <span class="m">32256</span> bytes.
<span class="o">[</span>04/15/2024-18:24:21<span class="o">]</span> <span class="o">[</span>I<span class="o">]</span> <span class="o">[</span>TRT<span class="o">]</span> Total Activation Memory: <span class="m">31744</span>
<span class="o">[</span>04/15/2024-18:24:21<span class="o">]</span> <span class="o">[</span>I<span class="o">]</span> <span class="o">[</span>TRT<span class="o">]</span> Total Weights Memory: <span class="m">26152</span>
<span class="o">[</span>04/15/2024-18:24:21<span class="o">]</span> <span class="o">[</span>I<span class="o">]</span> <span class="o">[</span>TRT<span class="o">]</span> Engine generation completed in 1.46118 seconds.
<span class="o">[</span>04/15/2024-18:24:21<span class="o">]</span> <span class="o">[</span>I<span class="o">]</span> <span class="o">[</span>TRT<span class="o">]</span> <span class="o">[</span>MemUsageStats<span class="o">]</span> Peak memory usage of TRT CPU/GPU memory allocators: CPU <span class="m">0</span> MiB, GPU <span class="m">5</span> MiB
<span class="o">[</span>04/15/2024-18:24:21<span class="o">]</span> <span class="o">[</span>I<span class="o">]</span> <span class="o">[</span>TRT<span class="o">]</span> <span class="o">[</span>MemUsageStats<span class="o">]</span> Peak memory usage during Engine building and serialization: CPU: <span class="m">3052</span> MiB
<span class="o">[</span>04/15/2024-18:24:21<span class="o">]</span> <span class="o">[</span>I<span class="o">]</span> <span class="o">[</span>TRT<span class="o">]</span> Loaded engine size: <span class="m">0</span> MiB
<span class="o">[</span>04/15/2024-18:24:21<span class="o">]</span> <span class="o">[</span>I<span class="o">]</span> <span class="o">[</span>TRT<span class="o">]</span> <span class="o">[</span>MemUsageChange<span class="o">]</span> TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU <span class="m">0</span> <span class="o">(</span>MiB<span class="o">)</span>
<span class="o">[</span>04/15/2024-18:24:21<span class="o">]</span> <span class="o">[</span>I<span class="o">]</span> Input:
<span class="o">[</span>04/15/2024-18:24:21<span class="o">]</span> <span class="o">[</span>I<span class="o">]</span> @@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@%<span class="o">=</span><span class="c1">#@@@@@%=%@@@@@@@@@@</span>
@@@@@@@           %@@@@@@@@@
@@@@@@@           %@@@@@@@@@
@@@@@@@#:-#-.     %@@@@@@@@@
@@@@@@@@@@@@#    <span class="c1">#@@@@@@@@@@</span>
@@@@@@@@@@@@@    <span class="c1">#@@@@@@@@@@</span>
@@@@@@@@@@@@@:  :@@@@@@@@@@@
@@@@@@@@@%+<span class="o">==</span>   *%%%%%%%%%@@
@@@@@@@@%                 -@
@@@@@@@@@#+.          .:-%@@
@@@@@@@@@@@*     :-###@@@@@@
@@@@@@@@@@@*   -%@@@@@@@@@@@
@@@@@@@@@@@*   *@@@@@@@@@@@@
@@@@@@@@@@@*   @@@@@@@@@@@@@
@@@@@@@@@@@*   <span class="c1">#@@@@@@@@@@@@</span>
@@@@@@@@@@@*   *@@@@@@@@@@@@
@@@@@@@@@@@*   *@@@@@@@@@@@@
@@@@@@@@@@@*   @@@@@@@@@@@@@
@@@@@@@@@@@*   @@@@@@@@@@@@@
@@@@@@@@@@@@+<span class="o">=</span><span class="c1">#@@@@@@@@@@@@@</span>
@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@

<span class="o">[</span>04/15/2024-18:24:21<span class="o">]</span> <span class="o">[</span>I<span class="o">]</span> Output:
<span class="o">[</span>04/15/2024-18:24:21<span class="o">]</span> <span class="o">[</span>I<span class="o">]</span>  Prob <span class="m">0</span>  0.0000 Class 0:
<span class="o">[</span>04/15/2024-18:24:21<span class="o">]</span> <span class="o">[</span>I<span class="o">]</span>  Prob <span class="m">1</span>  0.0000 Class 1:
<span class="o">[</span>04/15/2024-18:24:21<span class="o">]</span> <span class="o">[</span>I<span class="o">]</span>  Prob <span class="m">2</span>  0.0000 Class 2:
<span class="o">[</span>04/15/2024-18:24:21<span class="o">]</span> <span class="o">[</span>I<span class="o">]</span>  Prob <span class="m">3</span>  0.0000 Class 3:
<span class="o">[</span>04/15/2024-18:24:21<span class="o">]</span> <span class="o">[</span>I<span class="o">]</span>  Prob <span class="m">4</span>  0.0000 Class 4:
<span class="o">[</span>04/15/2024-18:24:21<span class="o">]</span> <span class="o">[</span>I<span class="o">]</span>  Prob <span class="m">5</span>  0.0000 Class 5:
<span class="o">[</span>04/15/2024-18:24:21<span class="o">]</span> <span class="o">[</span>I<span class="o">]</span>  Prob <span class="m">6</span>  0.0000 Class 6:
<span class="o">[</span>04/15/2024-18:24:21<span class="o">]</span> <span class="o">[</span>I<span class="o">]</span>  Prob <span class="m">7</span>  1.0000 Class 7: **********
<span class="o">[</span>04/15/2024-18:24:21<span class="o">]</span> <span class="o">[</span>I<span class="o">]</span>  Prob <span class="m">8</span>  0.0000 Class 8:
<span class="o">[</span>04/15/2024-18:24:21<span class="o">]</span> <span class="o">[</span>I<span class="o">]</span>  Prob <span class="m">9</span>  0.0000 Class 9:
<span class="o">[</span>04/15/2024-18:24:21<span class="o">]</span> <span class="o">[</span>I<span class="o">]</span>
<span class="o">&amp;&amp;&amp;&amp;</span> PASSED TensorRT.sample_onnx_mnist <span class="o">[</span>TensorRT v100000<span class="o">]</span> <span class="c1"># ./sample_onnx_mnist</span>
</code></pre></div></details>
<p>If you are using <strong>TensorRT Python API</strong>, make sure <strong>CUDA-Python</strong> is installed in your system or your virtual environment.</p>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash">mamba create -n tensorrt_env pip wheel
mamba activate tensorrt_env
</code></pre></div><ul>
<li>Installing from <strong>PyPI</strong>, <strong>TensorRT</strong> runtime wheels</li>
</ul>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash">pip install cuda-python
pip install --pre --upgrade tensorrt
pip install --pre --upgrade tensorrt_lean tensorrt_dispatch
</code></pre></div><p>To verify the installation is working, use python code.</p>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">tensorrt</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tensorrt</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
<span class="k">assert</span><span class="p">(</span><span class="n">tensorrt</span><span class="o">.</span><span class="n">Builder</span><span class="p">(</span><span class="n">tensorrt</span><span class="o">.</span><span class="n">Logger</span><span class="p">()))</span>

<span class="kn">import</span> <span class="nn">tensorrt_lean</span> <span class="k">as</span> <span class="nn">trtl</span>
<span class="nb">print</span><span class="p">(</span><span class="n">trtl</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">trtl</span><span class="o">.</span><span class="n">Runtime</span><span class="p">(</span><span class="n">trt</span><span class="o">.</span><span class="n">Logger</span><span class="p">())</span>

<span class="kn">import</span> <span class="nn">tensorrt_dispatch</span> <span class="k">as</span> <span class="nn">trtd</span>
<span class="nb">print</span><span class="p">(</span><span class="n">trtd</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">trtd</span><span class="o">.</span><span class="n">Runtime</span><span class="p">(</span><span class="n">trt</span><span class="o">.</span><span class="n">Logger</span><span class="p">())</span>
</code></pre></div><p>The <code>tensorrt/samples</code> directory also provides the samples that utilize with the <strong>TensorRT</strong>, you can follow instructions from <code>README.md</code> to run samples.</p>
<h2 id="3-set-up-python-environment-using-mambaforge">3. Set up Python environment using Mambaforge</h2>
<p>Python environment allows to manage separatelly different installations of Python and modules. It is useful when you have many projects running different version of Python and modules. It also help to manage installed modules for publish or reproduce.</p>
<p>There are different ways to create a python virtual environment, including built-in <code>venv</code>, <code>Conda</code> and <code>Anaconda</code>.</p>
<p><code>Conda</code> is a packaging tool and installer that aims to handle library dependencies outside of the Python packages as well as the Python packages themselves. For non preinstalled package manager, <code>Miniconda</code>, an installation of Conda, will be a good option.</p>
<p><code>Anacond</code>a is an installation of Conda that comes pre-loaded with a bunch of packages for scientific computing, i.e., <code>numpy</code>, <code>matplotlib</code>, <code>scipy</code>, etc. It also comes with IDE, Jupyter notebooks out of the box. This is helpful for beginers, but doesn&rsquo;t give much control.</p>
<p><code>Mamba</code> is a package manager which can be used with Python. Unlike <code>Conda</code>, it uses the C/C++ implementation to speed up the package installation. Read more about <code>mamba</code> in <a href="https://focalplane.biologists.com/2022/12/08/managing-scientific-python-environments-using-conda-mamba-and-friends/">here</a>. To install <code>mamba</code>, access <a href="https://github.com/conda-forge/miniforge">its repo</a> and pick the Mabaforge installer for your operating system.</p>
<p>Remember to run <code>conda init</code> at the end of your installation in your bash to activate the <code>mamba</code> command.</p>
<p><img loading="lazy" src="/posts/setitng-up-nvidia-cuda-on-wsl2/mambaforge-install.png" type="" alt="Mambaforge install"  /></p>
<h2 id="conclusion">Conclusion</h2>
<p>By following these steps and installing the required software, you will have an CUDA-ready environment in WSL for further machine learnin/deep learning applications. This environment will provide the necessary tools and libraries for GPU-accelerated computing and Python package management.</p>
<h2 id="reference">Reference</h2>
<ul>
<li><a href="https://medium.com/geekculture/install-cuda-and-cudnn-on-windows-linux-52d1501a8805">Install CUDA and CUDNN on Windows &amp; Linux</a>.</li>
<li><a href="https://docs.nvidia.com/cuda/wsl-user-guide/index.html">CUDA on WSL User Guide</a>.</li>
<li><a href="https://visualstudio.microsoft.com/free-developer-offers/">Machine learning environment build: WLS2+Ubuntu+CUDA+cuDNN</a>.</li>
<li><a href="https://biapol.github.io/blog/mara_lampert/getting_started_with_mambaforge_and_python/readme.html">Getting started with Mambaforge and Python</a>.</li>
<li><a href="https://ross-dobson.github.io/posts/2021/01/setting-up-python-virtual-environments-with-mambaforge/">Tutorial: Setting up Python enviroments with Mambaforge</a>.</li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>Setting Up NVIDIA CUDA on Windows 11</title>
      <link>http://visionbike.github.io/posts/setting-up-nvidia-cuda-on-windows-11/</link>
      <pubDate>Thu, 06 Jul 2023 12:41:36 +0800</pubDate>
      
      <guid>http://visionbike.github.io/posts/setting-up-nvidia-cuda-on-windows-11/</guid>
      <description>In this post, I will cover how to setup NVIDIA CUDA on Windows 11. To ensure a smooth setup process, it is crucial to follow the following steps:
 NVIDIA Drivers. Microsoft Visual Studio 2022 Community. NVIDIA CUDA Toolkit. NVIDIA cuDNN. TensorRT (optional). Miniforge (optional).  1. System Requirements To use CUDA, make sure your machine has a CUDA-capable GPU inside and the Microsoft Windows 11 should be updated 21H2 version.</description>
      <content:encoded><![CDATA[<p>In this post, I will cover how to setup <strong>NVIDIA CUDA</strong> on Windows 11. To ensure a smooth setup process, it is crucial to follow the following steps:</p>
<ul>
<li>NVIDIA Drivers.</li>
<li>Microsoft Visual Studio 2022 Community.</li>
<li>NVIDIA CUDA Toolkit.</li>
<li>NVIDIA cuDNN.</li>
<li>TensorRT (optional).</li>
<li>Miniforge (optional).</li>
</ul>
<h2 id="1-system-requirements">1. System Requirements</h2>
<p>To use CUDA, make sure your machine has a CUDA-capable GPU inside and the <strong>Microsoft Windows 11</strong> should be updated <strong>21H2</strong> version.</p>
<p>You can verify if your machine has CUDA-supported GPU through <strong>Display Adapters</strong> section in the <strong>Windows Device Manager</strong>. Here you will find the vendor name and model of your GPU.</p>
<p><img loading="lazy" src="/posts/setting-up-nvidia-cuda-on-windows-11/device-manager.png" type="" alt="device manager"  /></p>
<p>You also need to install <strong>Microsft Visual Studio 2022 Community</strong> as the native compilter for x86_64 application. The download link os the latest version is <a href="https://visualstudio.microsoft.com/thank-you-downloading-visual-studio/?sku=Community"><strong>here</strong></a> and install the <strong>Desktop development with C++</strong> workload.</p>
<p><img loading="lazy" src="/posts/setting-up-nvidia-cuda-on-windows-11/visual-studio-installation.png" type="" alt="Visual Studio Installation"  /></p>
<p>If there is any NVIDIA CUDA Toolkit installed before, you need to uninstall before proceeding further, following these steps:</p>
<ol>
<li>Open the <strong>Settings &gt; Apps &gt; Installed Apps</strong>.</li>
<li>Scroll down and find NVIDIA CUDA applications.</li>
</ol>
<p><img loading="lazy" src="/posts/setting-up-nvidia-cuda-on-windows-11/nvidia-apps-list.png" type="" alt="NVIDIA apps list"  /></p>
<ol start="3">
<li>Click to &ldquo;&hellip;&rdquo; button on the right and uninstall all NVIDIA GPU drivers and any associated software.</li>
</ol>
<p><img loading="lazy" src="/posts/setting-up-nvidia-cuda-on-windows-11/nvidia-app-uninstallation.png" type="" alt="NVIDIA app uninstallation"  /></p>
<p>Then you need to install NVIDIA driver to communicate your computer with NVIDIA devices. You can find the suitable driver from this <a href="https://www.nvidia.com/download/index.aspx?lang=en-us"><strong>website</strong></a>.</p>
<p><img loading="lazy" src="/posts/setting-up-nvidia-cuda-on-windows-11/nvidia-driver-installation.png" type="" alt="NVIDIA Driver Installation"  /></p>
<p>After the installation is complete, reboot your system.</p>
<h2 id="2-installing-mamba">2. Installing Mamba</h2>
<p><a href="https://mamba.readthedocs.io/en/latest/user_guide/mamba.html"><strong>Mamba</strong></a> is a command-line interfacer (CLI) to manage <code>conda</code>&rsquo;s environemts. For <code>mamba</code> configuration, please refer to <a href="https://conda.io/projects/conda/en/latest/user-guide/configuration/index.html"><strong>conda documentation</strong></a>.</p>
<p>For the fresh installation, you can install <a href="https://github.com/conda-forge/miniforge"><strong>Miniforge distribution</strong></a> &gt;= <code>Miniforge3-22.3.1.0</code>. <strong>Miniforge</strong> comes with the popular <code>conda-forge</code> channel preconfigured, but you can modify the configuration to use any channel you like.</p>
<div class="admonition note"><p class="admonition-title">Installation</p>
<p>Follow the instaltion prompts, taking note of options to <strong>Create start menu shortcuts</strong> and <strong>Add Miniforge3 to my PATH environment variable</strong>.</p>
<p><img loading="lazy" src="/posts/setting-up-nvidia-cuda-on-windows-11/miniforge-installation.png" type="" alt="Miniforge Installation"  /></p>
</div>
<p>After successful installation, you can use use the mamba commands as described in this <a href="https://mamba.readthedocs.io/en/latest/user_guide/mamba.html#mamba"><strong>user guide</strong></a>.</p>
<div class="admonition note"><p class="admonition-title">Working with Conda Environment</p>
<p>The command using <code>mamba</code> is similar to the <code>conda</code> command.</p>
<ul>
<li>Create new environment</li>
</ul>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash">mamba create -n <span class="p">&amp;</span>lt<span class="p">;</span>ENV_NAME<span class="p">&amp;</span>gt<span class="p">;</span> <span class="nv">python</span><span class="o">=</span><span class="p">&amp;</span>lt<span class="p">;</span>VERSION<span class="p">&amp;</span>gt<span class="p">;</span>
</code></pre></div><ul>
<li>Activate an environment</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash">mamba activate <span class="p">&amp;</span>lt<span class="p">;</span>ENV_NAME<span class="p">&amp;</span>gt<span class="p">;</span>
</code></pre></div><ul>
<li>Deactivate environment</li>
</ul>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash">mamba deactivate
</code></pre></div><ul>
<li>Delete an environment</li>
</ul>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash">mamba env remove -n <span class="p">&amp;</span>lt<span class="p">;</span>ENV_NAME<span class="p">&amp;</span>gt<span class="p">;</span>
</code></pre></div><ul>
<li>Show all created environments</li>
</ul>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash">mamba env list
</code></pre></div><p><strong>Remember to activate an environment first, do not install any packages in your base environment!</strong></p>
<ul>
<li>Install python packages</li>
</ul>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash">mamba install <span class="p">&amp;</span>lt<span class="p">;</span>PKG_NAME<span class="p">&amp;</span>gt<span class="p">;</span><span class="o">[=</span>VERSION<span class="o">]</span> <span class="o">[</span>-c <span class="p">&amp;</span>lt<span class="p">;</span>CHANNEL_NAME<span class="p">&amp;</span>gt<span class="p">;</span><span class="o">]</span>
</code></pre></div><p>When installing a package, you can optionally indicate specific additional channel that the packages are posted by community. The <code>conda-forge</code> is one of most common additional channels.</p>
<ul>
<li>Delete packages</li>
</ul>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash">mamba remove <span class="p">&amp;</span>lt<span class="p">;</span>PKG_NAME<span class="p">&amp;</span>gt<span class="p">;</span>
</code></pre></div><ul>
<li>Show all installed packages in the virtual environment</li>
</ul>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash">mamba list <span class="o">[</span>-n <span class="p">&amp;</span>lt<span class="p">;</span>ENV_NAME<span class="p">&amp;</span>gt<span class="p">;</span><span class="o">]</span>
</code></pre></div></div>
<div class="admonition note"><p class="admonition-title">Post-installation</p>
<p>After installation, you make sure that the <strong>Anaconda</strong> is not the default configured channel, seeing <a href="https://mamba.readthedocs.io/en/latest/user_guide/troubleshooting.html#defaults-channels"><strong>this</strong></a>.</p>
<p><strong>DO NOT</strong> install anything into the <code>base</code> environment as this might break your installation. See <a href="https://mamba.readthedocs.io/en/latest/user_guide/troubleshooting.html#base-packages"><strong>here</strong></a> for details.</p>
<p>The most convenient way to use the <strong>Mamba</strong> will be via the <strong>Miniforge Prompt</strong> installed in the <strong>Start Menu</strong>.</p>
</div>
<h2 id="3-installing-nvidia-cuda-toolkit">3. Installing NVIDIA CUDA Toolkit</h2>
<p>You can visit the <a href="https://developer.nvidia.com/cuda-downloads"><strong>NVIDIA Developer website for CUDA Toolkit</strong></a> TO get the latest version of NVIDIA CUDA Toolkit. For previous versions, you can check from the <a href="https://developer.nvidia.com/cuda-toolkit-archive"><strong>Archive of Previous CUDA Releases</strong></a> page.</p>
<p>On the dowload page, you choose the appropriate version based on your system.</p>
<p><img loading="lazy" src="/posts/setting-up-nvidia-cuda-on-windows-11/nvidia-cuda-toolkit-download-page.png" type="" alt="Download CUDA Toolkit"  /></p>
<p>Then, you locate the downloaded installer file and double-click on it to start the installation process. Follow the on-screen instructions provided by the installer.</p>
<p><img loading="lazy" src="/posts/setting-up-nvidia-cuda-on-windows-11/nvidia-cuda-toolkit-installation.png" type="" alt="NVIDIA CUDA Toolkit install"  /></p>
<p>Once the installation is completed, you check environment variables <code>CUDA_PATH</code> and <code>PATH</code> to ensure that your system recognizes <strong>NVIDIA CUDA Toolkit</strong>.</p>
<p><img loading="lazy" src="/posts/setting-up-nvidia-cuda-on-windows-11/nvidia-cuda-path-1.png" type="" alt="NVIDIA CUDA Path 1"  /></p>
<p><img loading="lazy" src="/posts/setting-up-nvidia-cuda-on-windows-11/nvidia-cuda-path-2.png" type="" alt="NVIDIA CUDA Path 2"  /></p>
<details class="admonition info"><summary class="admonition-title">Verify CUDA Toolkit Installation</summary>
<p>You can verify the installation by running the following command in command prompt.</p>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-cmd" data-lang="cmd">nvcc --version
</code></pre></div><p>If the installation was successful, you should see the CUDA version information displayed.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cmd" data-lang="cmd">nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2024 NVIDIA Corporation
Built on Tue_Feb_27_16:28:36_Pacific_Standard_Time_2024
Cuda compilation tools, release 12.4, V12.4.99
Build cuda_12.4.r12.4/compiler.33961263_0
</code></pre></div><p>It is important to verify that the <strong>NVIDIA CUDA Toolkit</strong> can find and communicate correctly with CUDA-compatible hardware. To do this, you need to compile and run some sample programs.</p>
<p>CUDA samples are located in <a href="https://github.com/nvidia/cuda-samples"><strong>https://github.com/nvidia/cuda-samples</strong></a>. To use the samples, clone the project, build the samples in <code>cuda-samples</code> directory using <strong>MVSC 2022 compiler</strong> and run them following the instruction on the Github page.</p>
<p>To verify a correct configuration of the hardware and software, it is highly recommended that you build and run the <code>deviceQuery</code> sample program.</p>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-cmd" data-lang="cmd">deviceQuery.exe
</code></pre></div><p>If  CUDA is installed and configured correctly, the output should look similar as below:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cmd" data-lang="cmd">deviceQuery.exe Starting...

 CUDA Device Query (Runtime API) version (CUDART static linking)

Detected 1 CUDA Capable device(s)

Device 0: <span class="s2">&#34;NVIDIA GeForce RTX 3070 Ti Laptop GPU&#34;</span>
  CUDA Driver Version / Runtime Version          12.4 / 12.4
  CUDA Capability Major/Minor version number:    8.6
  Total amount of global memory:                 8192 MBytes (8589410304 bytes)
  <span class="p">(</span>046<span class="p">)</span> Multiprocessors, (128) CUDA Cores/MP:    5888 CUDA Cores
  GPU Max Clock rate:                            1410 MHz (1.41 GHz)
  Memory Clock rate:                             7001 Mhz
  Memory Bus Width:                              256-bit
  L2 Cache Size:                                 4194304 bytes
  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)
  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers
  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers
  Total amount of constant memory:               65536 bytes
  Total amount of shared memory per block:       49152 bytes
  Total shared memory per multiprocessor:        102400 bytes
  Total number of registers available per block: 65536
  Warp size:                                     32
  Maximum number of threads per multiprocessor:  1536
  Maximum number of threads per block:           1024
  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)
  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)
  Maximum memory pitch:                          2147483647 bytes
  Texture alignment:                             512 bytes
  Concurrent copy and kernel execution:          Yes with 1 copy engine(s)
  Run time limit on kernels:                     Yes
  Integrated GPU sharing Host Memory:            No
  Support host page-locked memory mapping:       Yes
  Alignment requirement for Surfaces:            Yes
  Device has ECC support:                        Disabled
  CUDA Device Driver Mode (TCC or WDDM):         WDDM (Windows Display Driver Model)
  Device supports Unified Addressing (UVA):      Yes
  Device supports Managed Memory:                Yes
  Device supports Compute Preemption:            Yes
  Supports Cooperative Kernel Launch:            Yes
  Supports MultiDevice Co-op Kernel Launch:      No
  Device PCI Domain ID / Bus ID / location ID:   0 / 1 / 0
  Compute Mode:
     <span class="p">&lt;</span> Default <span class="p">(</span>multiple host threads can use ::cudaSetDevice(<span class="p">)</span> with device simultaneously) &gt;

deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 12.4, CUDA Runtime Version = 12.4, NumDevs = 1
Result = PASS
</code></pre></div><p>By running the <code>bandwidthTest</code> program, you can ensure that the system and CUDA-capable device are able to communicate correctly.</p>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-cmd" data-lang="cmd">bandwidthText.exe
</code></pre></div><p>The output shoud be here.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cmd" data-lang="cmd">[CUDA Bandwidth Test] - Starting...
Running on...

 Device 0: NVIDIA GeForce RTX 3070 Ti Laptop GPU
 Quick Mode

 Host to Device Bandwidth, 1 Device(s)
 PINNED Memory Transfers
   Transfer Size (Bytes)        Bandwidth(GB/s)
   32000000                     11.3

 Device to Host Bandwidth, 1 Device(s)
 PINNED Memory Transfers
   Transfer Size (Bytes)        Bandwidth(GB/s)
   32000000                     13.9

 Device to Device Bandwidth, 1 Device(s)
 PINNED Memory Transfers
   Transfer Size (Bytes)        Bandwidth(GB/s)
   32000000                     361.1

Result = PASS

NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.
</code></pre></div><p>To see a graphical representation, you can run the <code>particles</code> sample program.</p>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-cmd" data-lang="cmd">particles.exe
</code></pre></div><p><img loading="lazy" src="/posts/setting-up-nvidia-cuda-on-windows-11/nvidia-cuda-particle-sample.png" type="" alt="NVIDIA CUDA Particle Sample"  /></p>
</details>
<p>The installed <strong>NVIDIA CUDA Toolkit</strong> provides the necessary libraries, compilers, and tools for developing and running CUDA-accelerated applications and machine learning models.</p>
<h2 id="4-installing-nvidia-cudnn">4. Installing NVIDIA cuDNN</h2>
<p><strong>cuDNN (CUDA Deep Neural Network Library)</strong> is a GPU-accelerated library specifically designed and colaborated with <strong>NVIDA CUDA Toolkit</strong> to accelerate deep neural network computations. By utilizing <strong>cuDNN</strong>, deep learning frameworks can leverage the parallel processing capabilities of NVIDIA GPUs, leading to significant speed improvements in training and inference of deep neural networks.</p>
<p>You can visit <a href="https://developer.nvidia.com/rdp/cudnn-download"><strong>NVIDIA Developer website for cuDNN</strong></a> for the latest version. You will need to register or log in to your NVIDIA Developer account in order to access the cuDNN download files. If you don&rsquo;t have an account, you can create one for free.</p>
<p>Once you are logged in, choose the appropriate <strong>cuDNN</strong> version based on the <strong>NVIDIA CUDA Toolkit</strong> version and operating system. There are two main installation options:</p>
<ol>
<li><strong>Graphical installation</strong> (executable): the graphical installer bundles the available per-CUDA cuDNN verions in one package.</li>
<li><strong>Tarball installation</strong> (zip): per-CUDA cuDNN versions are provided as saperate tarballs (zip). These <code>.zip</code> archives do not replace the graphical installer and are not meant for general consumption, as they are not installers. These <code>zip</code> archives can be found at <a href="https://developer.download.nvidia.com/compute/cudnn/redist/cudnn/windows-x86_64/"><strong>this</strong></a>.</li>
</ol>
<p>Select one of two options for installing <strong>cuDNN</strong>. In this post, I will install <strong>cuDNN</strong> via the Tarball installation option.</p>
<p>Once download the <code>zip</code> archive, you unzip the <strong>cuDNN</strong> package.</p>
<p><img loading="lazy" src="/posts/setting-up-nvidia-cuda-on-windows-11/nvidia-cudnn-unzip.png" type="" alt="NVIDIA cuDNN Unzip"  /></p>
<p>Copy the following files from the unzipped package into the <strong>NVIDIA cuDNN</strong> directory created by yourself.</p>
<ul>
<li>Copy <code>bin\cudnn*.h</code> to <strong>C:Program Files\NVIDIA\CUDNN\vx.y\bin</strong>.</li>
<li>Copy <code>include\cudnn*.h</code> to <strong>C:\Program Files\NVIDIA\CUDNN\vx.y\include</strong>.</li>
<li>Copy <code>lib\x64\cudnn*.lib</code> to <strong>C:\Program Files\NVIDIA\CUDNN\vx.y\lib</strong>.</li>
</ul>
<p>You must replace <code>x.y</code> with your specific <strong>cuDNN</strong> version.</p>
<p>Set the environment variable to point to where <strong>cuDNN</strong> is located and add <code>bin</code> directory path to <code>PATH</code> environment variable.</p>
<p><img loading="lazy" src="/posts/setting-up-nvidia-cuda-on-windows-11/nvidia-cudnn-path.png" type="" alt="NVIDIA cuDNN Path"  /></p>
<p>For upgrading <strong>cuDNN</strong>, the remove the path to the directory containing <strong>cuDNN</strong> from <code>PATH</code> environment variable.</p>
<details class="admonition info"><summary class="admonition-title">Verify cuDNN Installation</summary>
<p>The <strong>cuDNN</strong> samples can be found <a href="https://developer.download.nvidia.com/compute/cudnn/redist/cudnn_samples/source/"><strong>here</strong></a> and download and extract the <code>tar.xz</code> archive.</p>
<p><img loading="lazy" src="/posts/setting-up-nvidia-cuda-on-windows-11/nvidia-cudnn-samples.png" type="" alt="NVIDIA cuDNN Samples"  /></p>
<p>Since this is cross-platform LINUX samples, you need to install <a href="https://cmake.org/download/"><strong>CMAKE</strong></a> and use it to compile the <strong>cuDNN</strong> samples.</p>
<p>Inside the <code>cuda_sample_vx</code> directory (<code>x</code> as the <strong>cuDNN</strong> version), make the comment to line 21 <code># add_subdirectory(mnistCUDNN)</code>.</p>
<p>Run the following command in the command prompt.</p>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-cmd" data-lang="cmd"><span class="k">mkdir</span> build <span class="p">&amp;&amp;</span> <span class="k">cd</span> build
cmake -G <span class="s2">&#34;Visual Studio 17 2022&#34;</span> -D cuDNN_INCLUDE_DIR=<span class="s2">&#34;C:\Program Files\NVIDIA\CUDNN\v9.0\include&#34;</span> -D cuDNN_LIBRARY_DIR=<span class="s2">&#34;C:\Program Files\NVIDIA\CUDNN\v9.0\lib&#34;</span> ..
cmake --build . --config Release
</code></pre></div><p>By running the <code>conv_sample</code> program,  you can ensure the <strong>cuDNN</strong> work in your system.</p>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-cmd" data-lang="cmd">.\conv_sample\Release\conv_sample.exe
</code></pre></div><p>The result should be as following:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cmd" data-lang="cmd">Executing: conv_sample.exe
Using format CUDNN_TENSOR_NCHW (for INT8x4 and INT8x32 tests use CUDNN_TENSOR_NCHW_VECT_C)
Testing single precision
====USER DIMENSIONS====
input dims are 1, 32, 4, 4
filter dims are 32, 32, 1, 1
output dims are 1, 32, 4, 4
====PADDING DIMENSIONS====
padded input dims are 1, 32, 4, 4
padded filter dims are 32, 32, 1, 1
padded output dims are 1, 32, 4, 4
Testing conv
<span class="se">^^^^</span> CUDA : elapsed = 0.0127218 sec,
Test PASSED
Testing half precision (math in single precision)
====USER DIMENSIONS====
input dims are 1, 32, 4, 4
filter dims are 32, 32, 1, 1
output dims are 1, 32, 4, 4
====PADDING DIMENSIONS====
padded input dims are 1, 32, 4, 4
padded filter dims are 32, 32, 1, 1
padded output dims are 1, 32, 4, 4
Testing conv
<span class="se">^^^^</span> CUDA : elapsed = 0.0029165 sec,
Test PASSED
</code></pre></div></details>
<p>For <strong>Visual Studio</strong> project, add <strong>cuDNN</strong> by following steps:</p>
<ul>
<li>Right-click on the project name in <strong>Solution Explorer</strong> and choose **Properties.</li>
<li>Click <strong>VC++ Directories</strong> and append <code>C:\Program Files\NVIDIA\CUDNN\v9.x\include</code> to the <strong>Include Direcotries</strong> field.</li>
<li>Click <strong>Linker &gt; General</strong> and append <code>C:\Program Files\NVIDIA\CUDNN\v9.x\lib</code> to the <strong>Additional Library Directories</strong> field.</li>
<li>Click <strong>Linker &gt; Input</strong> and append <code>cudnn.h</code> to the <strong>Additional Dependencies</strong> field and click <strong>OK</strong>.</li>
</ul>
<h2 id="5-installing-nvidia-tensorrt-optional">5. Installing NVIDIA TensorRT (Optional)</h2>
<p><strong>NVIDIA TensorRT</strong> is a C++ library that facilitates high-performance inference NVIDIA graphic processing units (GPUs). <strong>TensorRT</strong> takes a trained network, which consists of a network definition and a set of trained parameters, and produces a highly optimized runtime engine that performs inference for that network.</p>
<p><strong>TensorRT</strong> provides APIs via C++ and Python that help to express deep learning model via the Network Definition API or load a pre-defined model via the ONNX parser that allow <strong>TensorRT</strong> to optimize and run them on the NVIDIA GPU.</p>
<p><strong>TensorRT</strong> also include optional high speed mixed precision capabilities with difference NVIDIA architectures.</p>
<p>You can download the <strong>TensorRT</strong> at <a href="https://developer.nvidia.com/tensorrt/download/10x"><strong>here</strong></a>. For Windows architecture, there is only <code>zip</code> archive installation.</p>
<p>Unzip the <code>zip</code> archive and copy files in <code>lib</code>, <code>include</code> direcotries to <strong>C:\Program Files\NVIDIA\TensorRT\v10.0</strong> directory created by yourself. Then, you add <code>lib</code> directory path to <code>PATH</code> environment variable.</p>
<p><img loading="lazy" src="/posts/setting-up-nvidia-cuda-on-windows-11/nvidia-tensorrt-path.png" type="" alt="NVIDIA TensorRT Path"  /></p>
<details class="admonition info"><summary class="admonition-title">Verify TensorRT Installation</summary>
<p>Inside the <code>zip</code> archive also include the sample programs. To verify the installation is working, you should open a Visual Studio file from one of the samples, such as <code>sampleOnnxMNIST</code>.In the project, ensure that following is presented in the Visual Studio Solution project properties:</p>
<ul>
<li>Add <code>C:\Program Files\NVIDIA\TensorRT\v10.0\lib</code> to <code>PATH</code> and <strong>VC++ Directories &gt; Executable Directories</strong>.</li>
<li>Add <code>C:\Program Files\NVIDIA\TensorRT\v10.0\include</code> to <strong>C/C++ &gt; General &gt; Additional Directories</strong>.</li>
<li>Add <code>nvinfer.lib</code> and <code>.lib</code> files that that the projects requires to <strong>Linker &gt; Input &gt; Additional Dependencies</strong>.</li>
</ul>
<p>Compile the source code and run the example.</p>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-cmd" data-lang="cmd">.\bin\sample_onnx_mnist.exe
</code></pre></div><p>The output should be as following:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cmd" data-lang="cmd"><span class="p">&amp;&amp;&amp;&amp;</span> RUNNING TensorRT.sample_onnx_mnist [TensorRT v100000] # .\sample_onnx_mnist.exe
[04/08/2024-18:12:10] [I] Building and running a GPU inference engine for Onnx MNIST
[04/08/2024-18:12:10] [I] [TRT] [MemUsageChange] Init CUDA: CPU +109, GPU +0, now: CPU 18227, GPU 1091 (MiB)
[04/08/2024-18:12:18] [I] [TRT] [MemUsageChange] Init builder kernel library: CPU +2597, GPU +310, now: CPU 21109, GPU 1401 (MiB)
[04/08/2024-18:12:18] [I] [TRT] ----------------------------------------------------------------
[04/08/2024-18:12:18] [I] [TRT] Input filename:   ../data/mnist/mnist.onnx
[04/08/2024-18:12:18] [I] [TRT] ONNX IR version:  0.0.3
[04/08/2024-18:12:18] [I] [TRT] Opset version:    8
[04/08/2024-18:12:18] [I] [TRT] Producer name:    CNTK
[04/08/2024-18:12:18] [I] [TRT] Producer version: 2.5.1
[04/08/2024-18:12:18] [I] [TRT] Domain:           ai.cntk
[04/08/2024-18:12:18] [I] [TRT] Model version:    1
[04/08/2024-18:12:18] [I] [TRT] Doc string:
[04/08/2024-18:12:18] [I] [TRT] ----------------------------------------------------------------
[04/08/2024-18:12:18] [I] [TRT] Local timing cache in use. Profiling results in this builder pass will not be stored.
[04/08/2024-18:12:20] [I] [TRT] Detected 1 inputs and 1 output network tensors.
[04/08/2024-18:12:20] [I] [TRT] Total Host Persistent Memory: 26400
[04/08/2024-18:12:20] [I] [TRT] Total Device Persistent Memory: 0
[04/08/2024-18:12:20] [I] [TRT] Total Scratch Memory: 0
[04/08/2024-18:12:20] [I] [TRT] [BlockAssignment] Started assigning block shifts. This will take 6 steps to complete.
[04/08/2024-18:12:20] [I] [TRT] [BlockAssignment] Algorithm ShiftNTopDown took 0.8572ms to assign 3 blocks to 6 nodes requiring 32256 bytes.
[04/08/2024-18:12:20] [I] [TRT] Total Activation Memory: 31744
[04/08/2024-18:12:20] [I] [TRT] Total Weights Memory: 26152
[04/08/2024-18:12:20] [I] [TRT] Engine generation completed in 1.68333 seconds.
[04/08/2024-18:12:20] [I] [TRT] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 5 MiB
[04/08/2024-18:12:20] [I] [TRT] [MemUsageStats] Peak memory usage during Engine building and serialization: CPU: 3036 MiB
[04/08/2024-18:12:20] [I] [TRT] Loaded engine size: 0 MiB
[04/08/2024-18:12:21] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 0 (MiB)
[04/08/2024-18:12:21] [I] Input:
[04/08/2024-18:12:21] [I] @@@@@@@@@@@@@@@@@@@@@@@@@@@@
<span class="p">@@@@@@@@@@@@@@@@@@@@@@@@@@@@</span>
<span class="p">@@@@@@@@@@@@@@@@@@@@@@@@@@@@</span>
<span class="p">@@@@@@@@@@@@@@@@@@@@@@@@@@@@</span>
<span class="p">@@@@@@@@@@@@@@@@@@@@@@@@@@@@</span>
<span class="p">@@@@@@@@@@</span>=   ++++#++=*@@@@@
<span class="p">@@@@@@@@</span>#.            *@@@@@
<span class="p">@@@@@@@@</span>=             *@@@@@
<span class="p">@@@@@@@@</span>.   .. ...****%@@@@@
<span class="p">@@@@@@@@</span>: .%@@#@@@@@@@@@@@@@
<span class="p">@@@@@@@</span>%  -@@@@@@@@@@@@@@@@@
<span class="p">@@@@@@@</span>%  -@@*@@@*@@@@@@@@@@
<span class="p">@@@@@@@</span>#  :#- ::. ::=@@@@@@@
<span class="p">@@@@@@@</span>-             -@@@@@@
<span class="p">@@@@@@</span>%.              *@@@@@
<span class="p">@@@@@@</span>#     :==*+==   *@@@@@
<span class="p">@@@@@@</span><span class="nv">%---%</span>%@@@@@@@.  *@@@@@
<span class="p">@@@@@@@@@@@@@@@@@@@</span>+  *@@@@@
<span class="p">@@@@@@@@@@@@@@@@@@@</span>=  *@@@@@
<span class="p">@@@@@@@@@@@@@@@@@@</span>*   *@@@@@
<span class="p">@@@@@</span><span class="nv">%+%</span>@@@@@@@@<span class="nv">%.   .%</span>@@@@@
<span class="p">@@@@@</span>*  .******=    -@@@@@@@
<span class="p">@@@@@</span>*             .#@@@@@@@
<span class="p">@@@@@</span>*            =%@@@@@@@@
<span class="p">@@@@@@</span>%#+++=     =@@@@@@@@@@
<span class="p">@@@@@@@@@@@@@@@@@@@@@@@@@@@@</span>
<span class="p">@@@@@@@@@@@@@@@@@@@@@@@@@@@@</span>
<span class="p">@@@@@@@@@@@@@@@@@@@@@@@@@@@@</span>

[04/08/2024-18:12:21] [I] Output:
[04/08/2024-18:12:21] [I]  Prob 0  0.0000 Class 0:
[04/08/2024-18:12:21] [I]  Prob 1  0.0000 Class 1:
[04/08/2024-18:12:21] [I]  Prob 2  0.0000 Class 2:
[04/08/2024-18:12:21] [I]  Prob 3  0.0000 Class 3:
[04/08/2024-18:12:21] [I]  Prob 4  0.0000 Class 4:
[04/08/2024-18:12:21] [I]  Prob 5  1.0000 Class 5: **********
[04/08/2024-18:12:21] [I]  Prob 6  0.0000 Class 6:
[04/08/2024-18:12:21] [I]  Prob 7  0.0000 Class 7:
[04/08/2024-18:12:21] [I]  Prob 8  0.0000 Class 8:
[04/08/2024-18:12:21] [I]  Prob 9  0.0000 Class 9:
[04/08/2024-18:12:21] [I]
<span class="p">&amp;&amp;&amp;&amp;</span> PASSED TensorRT.sample_onnx_mnist [TensorRT v100000] # .\sample_onnx_mnist.exe
</code></pre></div></details>
<p>If you are using <strong>TensorRT Python API</strong>, make sure <strong>CUDA-Python</strong> is installed in your system or your virtual environment.</p>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-cmd" data-lang="cmd">mamba create -n tensorrt_env pip wheel
mamba activate tensorrt_env
</code></pre></div><ul>
<li>Installing from <strong>PyPI</strong>, <strong>TensorRT</strong> runtime wheels</li>
</ul>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-cmd" data-lang="cmd">pip install cuda-python
pip install --pre --upgrade tensorrt
pip install --pre --upgrade tensorrt_lean tensorrt_dispatch
</code></pre></div><p>To verify the installation is working, use python code.</p>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">tensorrt</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tensorrt</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
<span class="k">assert</span><span class="p">(</span><span class="n">tensorrt</span><span class="o">.</span><span class="n">Builder</span><span class="p">(</span><span class="n">tensorrt</span><span class="o">.</span><span class="n">Logger</span><span class="p">()))</span>

<span class="kn">import</span> <span class="nn">tensorrt_lean</span> <span class="k">as</span> <span class="nn">trtl</span>
<span class="nb">print</span><span class="p">(</span><span class="n">trtl</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">trtl</span><span class="o">.</span><span class="n">Runtime</span><span class="p">(</span><span class="n">trt</span><span class="o">.</span><span class="n">Logger</span><span class="p">())</span>

<span class="kn">import</span> <span class="nn">tensorrt_dispatch</span> <span class="k">as</span> <span class="nn">trtd</span>
<span class="nb">print</span><span class="p">(</span><span class="n">trtd</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">trtd</span><span class="o">.</span><span class="n">Runtime</span><span class="p">(</span><span class="n">trt</span><span class="o">.</span><span class="n">Logger</span><span class="p">())</span>
</code></pre></div><p>The <code>tensorrt/samples</code> directory also provides the samples that utilize with the <strong>TensorRT</strong>, you can follow instructions from <code>README.md</code> to run samples.</p>
<h2 id="conclusion">Conclusion</h2>
<p>By following these steps and installing the required software, you will have an CUDA-ready environment in Windows 11 system for further machine learnin/deep learning applications. This environment will provide the necessary tools and libraries for GPU-accelerated computing and Python package management.</p>
<h2 id="reference">Reference</h2>
<ul>
<li><a href="https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/">CUDA Installation Guide for Microsoft Windows</a>.</li>
<li><a href="https://medium.com/geekculture/install-cuda-and-cudnn-on-windows-linux-52d1501a8805">Install CUDA and CUDNN on Windows &amp; Linux</a>.</li>
<li><a href="https://medium.com/@Gunter-Pearson/installing-latest-tensorflow-version-with-cuda-cudnn-and-gpu-support-on-windows-11-pc-e41fac5c5795">Installing Latest TensorFlow version with CUDA, cudNN and GPU support on Windows 11 PC</a>.</li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>Setting Up a Personal Blog With Hugo and GitHub Pages</title>
      <link>http://visionbike.github.io/posts/setting-up-personal-blog-with-hugo-and-gh-pages/</link>
      <pubDate>Tue, 04 Jul 2023 01:53:57 +0800</pubDate>
      
      <guid>http://visionbike.github.io/posts/setting-up-personal-blog-with-hugo-and-gh-pages/</guid>
      <description>Creating a personal blog with technical content is a excellent way to enhance the writting skill, keep memorial notes and share personal experience with others. Ideally, these goals need to be achieved when creating and mantaining a blog:
 Low-cost - Free or as close to free as posisble. Productive - Easy to write and maintain. Cloud Native - Utilizes public cloud services for hosting, allowing for infinite scaling.  After researching, I found that using Markdown, Hugo and GitHub Pages is indeed a powerful combination for creating and maintaining a cost-effective, productive, and cloud-native blog.</description>
      <content:encoded><![CDATA[<p>Creating a personal blog with technical content is a excellent way to enhance the writting skill, keep memorial notes and share personal experience with others. Ideally, these goals need to be achieved when creating and mantaining a blog:</p>
<ol>
<li><strong>Low-cost</strong> - Free or as close to free as posisble.</li>
<li><strong>Productive</strong> - Easy to write and maintain.</li>
<li><strong>Cloud Native</strong> - Utilizes public cloud services for hosting, allowing for infinite scaling.</li>
</ol>
<p>After researching, I found that using <strong>Markdown</strong>,  <strong>Hugo</strong> and <strong>GitHub Pages</strong>  is indeed a powerful combination for creating and maintaining a cost-effective, productive, and cloud-native blog.</p>
<ul>
<li><a href="https://daringfireball.net/projects/markdown/"><strong>Markdown</strong></a> is markup language that is extremely easy to read, write natively and can be converted into HTML.</li>
<li><a href="https://gohugo.io/"><strong>Hugo</strong></a> is a static site generator written in the Go language that allows for content written in Markdown to be rendered into HTML webpages.</li>
<li><a href="https://pages.github.com/"><strong>GitHub Pages</strong></a> is the GitHub service that hosts web contentstored in a GitHub repository.</li>
</ul>
<p>In this post, I will show how to create an simple personal blog for FREE using above technologies. The blog was developed in Window Subsystem for Linux (WSL2).</p>
<p>Here&rsquo;s an outline of the steps you can follow to create the personal blog using these technologies:</p>
<h2 id="1-setting-up-github-account">1. Setting up GitHub Account</h2>
<p>If you don&rsquo;t have one already, creating a GitHub account. GitHub Pages allows you to host your blog for free using a GitHub repository.</p>
<h2 id="2-installing-hugo">2. Installing Hugo</h2>
<p>Before starting, make sure <code>git</code> is installed in the local machine.</p>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash">sudo apt install -y git
</code></pre></div><p>Configure Git with your <u><strong>username</strong></u> and <u><strong>email address</strong></u>.</p>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash">git config --global user.name <span class="s2">&#34;Your Name&#34;</span>
</code></pre></div><div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash">git config --global user.email <span class="s2">&#34;your.email@example.com&#34;</span>
</code></pre></div><p>To verify that Git has been installed successfully, you can check the version using <code>git --version</code>. This command will display the installed version of Git.</p>
<p>For Ubuntu user, you can  install Hugo on your host by this command:</p>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash">sudo apt install -y hugo
</code></pre></div><p>Run the folloing command for verification:</p>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash">hugo version
</code></pre></div><p>The Hugo version should be shown if the installation is successfull.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash">hugo v0.92.2+extended linux/amd64 <span class="nv">BuildDate</span><span class="o">=</span>2023-01-31T11:11:57Z <span class="nv">VendorInfo</span><span class="o">=</span>ubuntu:0.92.2-1ubuntu0.1
</code></pre></div><h2 id="3-creating-a-new-hugo-site">3. Creating a new Hugo site</h2>
<p>You can run <code>hugo new site</code> command to create a new Hugo site:</p>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash">hugo new site <span class="p">&amp;</span>lt<span class="p">;</span>USERNAME<span class="p">&amp;</span>gt<span class="p">;</span>-Hugo -f yml
</code></pre></div><p>This command will set up the basic directory structure and configuration file in <code>*.yml</code> format for your blog.</p>
<div class="admonition tip"><p class="admonition-title">What is `&lt;USERNAME&gt;`?</p>
<p>For convenient management and organization, you should name the your blog project as above format with <strong>&lt;USERNAME&gt;</strong> as the your GitHub&rsquo;s username, i.e., <code>Visionbike-Hugo</code>. It&rsquo;s helpful to keep track your project and ensuring clarity when managing multiple repositories.</p>
</div>
<p>The site will be associated with a GitHub repository where you can store the source code of your blog. Hence, you need to initialize <code>git</code> in the local project for further use.</p>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="nb">cd</span> <span class="p">&amp;</span>lt<span class="p">;</span>USERNAME<span class="p">&amp;</span>gt<span class="p">;</span>-Hugo
git init
</code></pre></div><p>You also need to create a new repository on GitHub storage for your blog&rsquo;s source code.</p>
<p><img loading="lazy" src="/posts/setting-up-personal-blog-with-hugo-and-gh-pages/create-source-code-repo.png" type="" alt="Create source code repository"  /></p>
<div class="admonition tip"><p class="admonition-title">Creating Github Repository Without README File</p>
<p>By creating a repository without a <strong>README</strong> file, you can avoid accidental history conflicts when pushing your local project to a fresh repository. You can always add a <strong>README</strong> file later if needed.</p>
</div>
<p>Now, you link the local project to the GitHub repository by the <code>git remote</code> command:</p>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash">git remote add origin https://github.com/<span class="p">&amp;</span>lt<span class="p">;</span>USERNAME<span class="p">&amp;</span>gt<span class="p">;</span>/<span class="p">&amp;</span>lt<span class="p">;</span>USERNAME<span class="p">&amp;</span>gt<span class="p">;</span>.github.io.git
git banrch -M master
</code></pre></div><p>By completing these steps, you have linked your local Hugo site to the GitHub repository. Now you can continue working on your site locally, commit any changes, and push them to the remote repository when ready.</p>
<h2 id="4-installing-hugo-theme">4. Installing Hugo Theme</h2>
<p>Installing a Hugo theme is a fantastic way to personalize your blog and enhance its visual appeal. You can access free Hugo themes via this <a href="https://themes.gohugo.io/"><strong>website</strong></a>.</p>
<p>For my blog, I chose the <a href="https://github.com/reorx/hugo-PaperModX"><strong>PaperModX</strong></a> theme because of fonding its style and awesome features. I added its source code by the <code>git submodule</code> command.</p>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash">git submodule add --depth <span class="m">1</span> https://github.com/reorx/hugo-PaperModX themes/PaperModX
</code></pre></div><p>The command will add the <code>PaperModX</code> theme repository as a submodule in the <u><strong>themes/PaperModX</strong></u> directory of your Hugo site.</p>
<div class="admonition tip"><p class="admonition-title">Updating submodules</p>
<p>If you have already added the submodule before, you can run the following command to reclone it.</p>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash">git submodule update --init --recursive
</code></pre></div><p>For updating the theme, run this command.</p>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash">git submodule update --remote --merge
</code></pre></div></div>
<h2 id="5-modify-hugo-configuration">5. Modify Hugo Configuration</h2>
<p>Once you have added the theme, you can configure it in your Hugo site&rsquo;s configuration file (<code>config.yml</code>). Refer to the theme&rsquo;s documentation for specific instructions on customization and configuration options. You will most likely want to modify the following fields:</p>
<ul>
<li>
<p><u><strong>baseURL</strong></u>: This should be set into the URL GitHub Pages for hosting your blog. If the GitHub repository is named <strong>&lt;USERNAME&gt;.github.io</strong>, then the value of baseURL will be <strong>https://&lt;USERNAME&gt;.github.io/</strong>. If the GitHub repository has any other name, then the value will be <strong>https://&lt;USERNAME&gt;.github.io/&lt;REPOSITORY_NAME&gt;/</strong>. For instance, my GitHub username is <code>visionbike</code>, then:</p>
<ul>
<li>If the GitHub repository is named <code>visionbike.github.io</code>, then the baseURL will be <code>https://visionbike.github.io/</code>.</li>
<li>If the GitHub repository is named <code>visionbike-hugo</code>, then the baseURL will be <code>https://visionbike.github.io/visionbike-hugo/</code>.</li>
</ul>
</li>
<li>
<p><u><strong>title</strong></u>: This will be the title of your blog site as it appears at the top of a visitorâ€™s web browser when your site is open. It will also appear underneath your avatar, if one is present.</p>
</li>
<li>
<p><u><strong>theme</strong></u>: The name of the theme Hugo should use to render your site. In my example, this will be set to <code>PaperModX</code>, since that is the name of the theme I am using.</p>
</li>
</ul>
<p>Example contents of the <code>config.yml</code> file can be found below.</p>
<details class="admonition note"><summary class="admonition-title">config.yml</summary>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="c"># base URL</span><span class="w">
</span><span class="w"></span><span class="nt">baseURL</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;http://visionbike.github.io/&#34;</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="c"># site title</span><span class="w">
</span><span class="w"></span><span class="nt">title</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;Visionbike - Personal Blog of CV | DSP | ML notes&#34;</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="c"># paginate</span><span class="w">
</span><span class="w"></span><span class="nt">paginate</span><span class="p">:</span><span class="w"> </span><span class="m">5</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="c"># theme config</span><span class="w">
</span><span class="w"></span><span class="nt">theme</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;PaperModX&#34;</span><span class="w">
</span><span class="w"></span><span class="nt">themesdir</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;themes&#34;</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="c"># global config</span><span class="w">
</span><span class="w"></span><span class="nt">enableInlineShortcodes</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w"></span><span class="nt">enableRobotsTXT</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w"></span><span class="nt">buildDrafts</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="w">
</span><span class="w"></span><span class="nt">buildFuture</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="w">
</span><span class="w"></span><span class="nt">buildExpired</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="w">
</span><span class="w"></span><span class="nt">enableEmoji</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="c"># css minify for speeding up site</span><span class="w">
</span><span class="w"></span><span class="nt">minify</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">disableXML</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">  </span><span class="nt">minifyOutput</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="c"># site param config</span><span class="w">
</span><span class="w"></span><span class="nt">params</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="c"># environment</span><span class="w">
</span><span class="w">  </span><span class="nt">env</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;production&#34;</span><span class="w">
</span><span class="w">  </span><span class="nt">description</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;Visionbike - Personal Blog of CV | DSP | ML notes&#34;</span><span class="w">
</span><span class="w">
</span><span class="w">  </span><span class="c"># color scheme: auto, dark, light</span><span class="w">
</span><span class="w">  </span><span class="nt">defaultTheme</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;dark&#34;</span><span class="w">
</span><span class="w">  </span><span class="nt">disableThemeToggle</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">
</span><span class="w">  </span><span class="c"># header logo</span><span class="w">
</span><span class="w">  </span><span class="nt">logo</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">text</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;Visionbike&#34;</span><span class="w">
</span><span class="w">    </span><span class="nt">icon</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;images/apple-touch-icon.png&#34;</span><span class="w">
</span><span class="w">    </span><span class="nt">iconHeight</span><span class="p">:</span><span class="w"> </span><span class="m">35</span><span class="w">
</span><span class="w">    </span><span class="nt">iconWidth</span><span class="p">:</span><span class="w"> </span><span class="m">35</span><span class="w">
</span><span class="w">
</span><span class="w">  </span><span class="c"># profile-mode</span><span class="w">
</span><span class="w">  </span><span class="nt">profileMode</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">enabled</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">    </span><span class="nt">title</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;Phuc Thanh-Thien Nguyen&#34;</span><span class="w">
</span><span class="w">    </span><span class="nt">subtitle</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;AI Researcher - Personal Blog of CV | DSP | ML notes&#34;</span><span class="w">
</span><span class="w">    </span><span class="nt">imageUrl</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;images/avatar-real.png&#34;</span><span class="w">
</span><span class="w">    </span><span class="nt">imageTitle</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;avatar-real&#34;</span><span class="w">
</span><span class="w">    </span><span class="nt">imageWidth</span><span class="p">:</span><span class="w"> </span><span class="m">180</span><span class="w">
</span><span class="w">    </span><span class="nt">imageHeight</span><span class="p">:</span><span class="w"> </span><span class="m">180</span><span class="w">
</span><span class="w">    </span><span class="nt">buttons</span><span class="p">:</span><span class="w">
</span><span class="w">      </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">Archives</span><span class="w">
</span><span class="w">        </span><span class="nt">url</span><span class="p">:</span><span class="w"> </span><span class="l">/archives/</span><span class="w">
</span><span class="w">      </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">Tags</span><span class="w">
</span><span class="w">        </span><span class="nt">url</span><span class="p">:</span><span class="w"> </span><span class="l">/tags/</span><span class="w">
</span><span class="w">  </span><span class="c"># home-info-mode</span><span class="w">
</span><span class="w">  </span><span class="nt">homeInfoParams</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">title</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;Phuc Thanh-Thien Nguyen&#34;</span><span class="w">
</span><span class="w">    </span><span class="nt">content</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;AI Researcher - Personal Blog of CV | DSP | ML notes&#34;</span><span class="w">
</span><span class="w">
</span><span class="w">  </span><span class="c"># home social icons</span><span class="w">
</span><span class="w">  </span><span class="nt">socialIcons</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">github</span><span class="w">
</span><span class="w">      </span><span class="nt">url</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;https://github.com/visionbike/&#34;</span><span class="w">
</span><span class="w">    </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">linkedIn</span><span class="w">
</span><span class="w">      </span><span class="nt">url</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;https://linkedin.com/in/nttphuc/&#34;</span><span class="w">
</span><span class="w">
</span><span class="w">  </span><span class="c"># post config</span><span class="w">
</span><span class="w">  </span><span class="nt">author</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;Visionbike&#34;</span><span class="w">
</span><span class="w">  </span><span class="nt">showCodeCopyButtons</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">  </span><span class="nt">displayFullLangName</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">  </span><span class="nt">showReadingTime</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">  </span><span class="nt">showWordCount</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">  </span><span class="nt">showPostNavLinks</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">  </span><span class="nt">showBreadCrumbs</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">  </span><span class="nt">enableImageZoom</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">
</span><span class="w">  </span><span class="c"># assets images</span><span class="w">
</span><span class="w">  </span><span class="nt">assets</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="c"># disableFingerprinting: true</span><span class="w">
</span><span class="w">    </span><span class="nt">favicon</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;images/favicon.ico&#34;</span><span class="w">
</span><span class="w">    </span><span class="nt">favicon16x16</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;images/favicon-16x16.png&#34;</span><span class="w">
</span><span class="w">    </span><span class="nt">favicon32x32</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;images/favicon-32x32.png&#34;</span><span class="w">
</span><span class="w">    </span><span class="nt">appleTouchIcon</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;images/apple-touch-icon.png&#34;</span><span class="w">
</span><span class="w">    </span><span class="nt">safarPinnedTab</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;images/safari-pinned-tab.svg&#34;</span><span class="w">
</span><span class="w">
</span><span class="w">  </span><span class="c"># search page</span><span class="w">
</span><span class="w">  </span><span class="nt">fuseOpts</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">isCaseSensitive</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="w">
</span><span class="w">    </span><span class="nt">shouldSort</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">    </span><span class="nt">location</span><span class="p">:</span><span class="w"> </span><span class="m">0</span><span class="w">
</span><span class="w">    </span><span class="nt">distance</span><span class="p">:</span><span class="w"> </span><span class="m">1000</span><span class="w">
</span><span class="w">    </span><span class="nt">threshold</span><span class="p">:</span><span class="w"> </span><span class="m">0.4</span><span class="w">
</span><span class="w">    </span><span class="nt">minMatchCharLength</span><span class="p">:</span><span class="w"> </span><span class="m">0</span><span class="w">
</span><span class="w">    </span><span class="nt">keys</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">&#34;title&#34;</span><span class="p">,</span><span class="w"> </span><span class="s2">&#34;permalink&#34;</span><span class="p">,</span><span class="w"> </span><span class="s2">&#34;summary&#34;</span><span class="p">,</span><span class="w"> </span><span class="s2">&#34;content&#34;</span><span class="p">]</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="c"># laguage config</span><span class="w">
</span><span class="w"></span><span class="nt">languages</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">en</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">weight</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">    </span><span class="c"># language code</span><span class="w">
</span><span class="w">    </span><span class="nt">languageCode</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;en&#34;</span><span class="w">
</span><span class="w">    </span><span class="c"># determines default content language: &#34;en&#34;, &#34;zh-cn&#34;, &#34;fr&#34;, &#34;vi&#34;, ...</span><span class="w">
</span><span class="w">    </span><span class="nt">defaultContentLanguage</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;en&#34;</span><span class="w">
</span><span class="w">    </span><span class="c"># menu language</span><span class="w">
</span><span class="w">    </span><span class="nt">menu</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">main</span><span class="p">:</span><span class="w">
</span><span class="w">        </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;Posts&#34;</span><span class="w">
</span><span class="w">          </span><span class="nt">weight</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">          </span><span class="nt">url</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;/posts/&#34;</span><span class="w">
</span><span class="w">        </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;Tags&#34;</span><span class="w">
</span><span class="w">          </span><span class="nt">url</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;/tags/&#34;</span><span class="w">
</span><span class="w">          </span><span class="nt">weight</span><span class="p">:</span><span class="w"> </span><span class="m">3</span><span class="w">
</span><span class="w">        </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;Archive&#34;</span><span class="w">
</span><span class="w">          </span><span class="nt">url</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;/archives/&#34;</span><span class="w">
</span><span class="w">          </span><span class="nt">weight</span><span class="p">:</span><span class="w"> </span><span class="m">4</span><span class="w">
</span><span class="w">        </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;Publish&#34;</span><span class="w">
</span><span class="w">          </span><span class="nt">url</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;/publish/&#34;</span><span class="w">
</span><span class="w">          </span><span class="nt">weight</span><span class="p">:</span><span class="w"> </span><span class="m">5</span><span class="w">
</span><span class="w">        </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;About&#34;</span><span class="w">
</span><span class="w">          </span><span class="nt">weight</span><span class="p">:</span><span class="w"> </span><span class="m">10</span><span class="w">
</span><span class="w">          </span><span class="nt">url</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;/about/&#34;</span><span class="w">
</span><span class="w">        </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;Search&#34;</span><span class="w">
</span><span class="w">          </span><span class="nt">weight</span><span class="p">:</span><span class="w"> </span><span class="m">100</span><span class="w">
</span><span class="w">          </span><span class="nt">url</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;/search/&#34;</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="c"># for search page</span><span class="w">
</span><span class="w"></span><span class="nt">outputs</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">home</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="l">HTML</span><span class="w">
</span><span class="w">    </span>- <span class="l">RSS</span><span class="w">
</span><span class="w">    </span>- <span class="l">JSON</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="c"># syntax highlight</span><span class="w">
</span><span class="w"></span><span class="nt">pygmentsUseClasses</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w"></span><span class="nt">pygmentsCodeFences</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w"></span><span class="nt">markup</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">goldmark</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">renderer</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">unsafe</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">    </span><span class="nt">highlight</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">lineNos</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="w">
</span><span class="w">      </span><span class="nt">codeFences</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">      </span><span class="nt">noClasses</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="w">
</span></code></pre></div></details>
<p>After modifying the configuration file accordingly, you can commit and push changes from your local repository to GitHub.</p>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash">git add config.yml
git commit -m <span class="s2">&#34;modify configuration file&#34;</span>
git push -u origin master
</code></pre></div><h2 id="6-creating-new-hugo-post">6. Creating New Hugo Post</h2>
<p>You are almost done finishing your personal blog!</p>
<p>To create the first post, you execute the <code>hugo new</code> command in the terminal.</p>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash">hugo new posts/first-post/index.md
</code></pre></div><p>The command will create a new folder named <code>first-post</code> with new Markdown file <code>index.md</code>, inside the <u><strong>content/posts</strong></u> directory. Creating a new directory for each single post helps you manage your resource better when images, media sources can be store directly in this directory. The Markdown file will contain the template for your first blog post and you can start  writing your content using Markdown syntax.</p>
<p>The contents of the <code>first-post.md</code> file will look like as:</p>
<div class="admonition note"><p class="admonition-title">first-post.md</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-markdown" data-lang="markdown">---
title: &#34;First Post&#34;
date: 2023-07-04T01:53:57+08:00
draft: true
---
</code></pre></div></div>
<p>You need to add content to the Markdown file and update the metadata header. In the metadata header, you will find information such as the post&rsquo;s title, publishing date and draft status. Change the value of the <strong>draft</strong> field from <code>true</code> to <code>false</code> to indicate that the post is ready to be published on your blog site. Your can also add other features supported by the installed theme for your post, i.e., comments, share buttons, navigation, etc.</p>
<p>The, you add the desired content to the body of the post. For instance, I added the line <strong>&ldquo;This is my first post! Hello world!&quot;</strong> at the bottom of the file. Feel free to customize the content to reflect your own thoughts and ideas in Markdown syntax.</p>
<p>After modifying the first blog post, you can use <code>git commit</code> to commit and push the changes from your local repository to GitHub.</p>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash">git add content/posts/first-post.md
git commit -m <span class="s2">&#34;add the first post&#34;</span>
git push -u origin master
</code></pre></div><h2 id="7-testing-the-hugo-configuration">7. Testing the Hugo Configuration</h2>
<p>Before hosting your blog to GitHub pages, make ensure Hugo can parse the configuration file and build our new blog post successfully.</p>
<p>In the local machine, you can run <code>hugo server</code> command to serve your site locally.</p>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash">hugo server --disableFastRender
</code></pre></div><p>It will watch for any changes in your files and automatically rebuild your site whenever there are updates. Hugo will provide a local development server address, i.e., <code>http://localhost:1313</code>, where you can access your site locally.</p>
<p>To parse the configuration and build your site, you simply run <code>hugo</code> command.</p>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash">hugo
</code></pre></div><p>If Hugo encounters any errors, they will be reported here. If the site is successfully built, then you will see output similar to the following.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash">Start building sites â€¦ 
hugo v0.92.2+extended linux/amd64 <span class="nv">BuildDate</span><span class="o">=</span>2023-01-31T11:11:57Z <span class="nv">VendorInfo</span><span class="o">=</span>ubuntu:0.92.2-1ubuntu0.1

                   <span class="p">|</span> EN  
-------------------+-----
  Pages            <span class="p">|</span> <span class="m">13</span>  
  Paginator pages  <span class="p">|</span>  <span class="m">0</span>  
  Non-page files   <span class="p">|</span>  <span class="m">0</span>  
  Static files     <span class="p">|</span> <span class="m">18</span>  
  Processed images <span class="p">|</span>  <span class="m">0</span>  
  Aliases          <span class="p">|</span>  <span class="m">0</span>  
  Sitemaps         <span class="p">|</span>  <span class="m">1</span>  
  Cleaned          <span class="p">|</span>  <span class="m">0</span>  

Total in <span class="m">38</span> ms
</code></pre></div><h2 id="8-setting-up-github-actions-workflow">8. Setting Up GitHub Actions Workflow</h2>
<p>Lastly, the GitHub Actions workflow has to be prepared for automatically building and deploying your blog to GitHub Pages. This workflow is defined by a YAML file in the <u><strong>.github/workflows</strong></u> directory structure at the root of the project.</p>
<p>First, create the <code>workflows</code> folder.</p>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash">mkdir -p .github/workflows
</code></pre></div><p>The command will create the <u><strong>.github/workflows</strong></u> directory if it doesn&rsquo;t already exist. The <code>-p</code> option ensures that the parent directories are created if needed.</p>
<p>Then, we create a new file within the created folder directory, named <code>deploy_gh_pages.yaml</code> with the following contents.</p>
<div class="admonition note"><p class="admonition-title">deploy_gh_pages.yaml</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nn">---</span><span class="w">
</span><span class="w"></span><span class="nn">---</span><span class="w">
</span><span class="w"></span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">Deploy Hugo site via GitHub Pages</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="nt">on</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">push</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">branches</span><span class="p">:</span><span class="w">
</span><span class="w">      </span>- <span class="l">master</span><span class="w"> </span><span class="c"># Set a branch to deploy</span><span class="w">
</span><span class="w">  </span><span class="nt">pull_request</span><span class="p">:</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="nt">jobs</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">deploy</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">runs-on</span><span class="p">:</span><span class="w"> </span><span class="l">ubuntu-22.04</span><span class="w">
</span><span class="w">    </span><span class="nt">permissions</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">contents</span><span class="p">:</span><span class="w"> </span><span class="l">write</span><span class="w">
</span><span class="w">    </span><span class="nt">concurrency</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">group</span><span class="p">:</span><span class="w"> </span><span class="l">${{ github.workflow }}-${{ github.ref }}</span><span class="w">
</span><span class="w">    </span><span class="nt">steps</span><span class="p">:</span><span class="w">
</span><span class="w">      </span>- <span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l">actions/checkout@v3</span><span class="w">
</span><span class="w">        </span><span class="nt">with</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">submodules</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w"> </span><span class="c"># Fetch Hugo themes (true OR recursive)</span><span class="w">
</span><span class="w">          </span><span class="nt">fetch-depth</span><span class="p">:</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="c"># Fetch all history for .GitInfo and .Lastmod</span><span class="w">
</span><span class="w">
</span><span class="w">      </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">Setup Hugo</span><span class="w">
</span><span class="w">        </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l">peaceiris/actions-hugo@v2</span><span class="w">
</span><span class="w">        </span><span class="nt">with</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">hugo-version</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;latest&#34;</span><span class="w">
</span><span class="w">          </span><span class="nt">extended</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">
</span><span class="w">      </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">Build</span><span class="w">
</span><span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="l">hugo --minify</span><span class="w">
</span><span class="w">
</span><span class="w">      </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">Deploy</span><span class="w">
</span><span class="w">        </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l">peaceiris/actions-gh-pages@v3</span><span class="w">
</span><span class="w">        </span><span class="c"># If you&#39;re changing the branch from main,</span><span class="w">
</span><span class="w">        </span><span class="c"># also change the `master` in `refs/heads/master`</span><span class="w">
</span><span class="w">        </span><span class="c"># below accordingly.</span><span class="w">
</span><span class="w">        </span><span class="nt">if</span><span class="p">:</span><span class="w"> </span><span class="l">github.ref == &#39;refs/heads/master&#39;</span><span class="w">
</span><span class="w">        </span><span class="nt">with</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">github_token</span><span class="p">:</span><span class="w"> </span><span class="l">${{ secrets.GITHUB_TOKEN }}</span><span class="w">
</span><span class="w">          </span><span class="nt">publish_dir</span><span class="p">:</span><span class="w"> </span><span class="l">./public</span><span class="w">
</span></code></pre></div></div>
<p>The YAML file sets up the deployment process using Hugo and GitHub Pages. The workflow is triggered on a push to the <code>master</code> branch, and it uses the specified actions to build and deploy your blog. You can find more in <a href="https://github.com/peaceiris/actions-gh-pages"><strong>this</strong></a>.</p>
<p>Finally, we use <code>git</code> commands to commit and push the changes from your local repository to GitHub.</p>
<div class="highlight command"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash">git add .github/workflows/deploy_gh_pages.yaml
git commit -m <span class="s2">&#34;Add GitHub Actions workflow&#34;</span>
git push -u origin master
</code></pre></div><h2 id="9-configuring-github-pages">9. Configuring GitHub Pages</h2>
<p>The GitHub Pages will allow GitHub to build automatically and serve our website whenever changes are made to the underlying repository.</p>
<p>First, we create new branch named <code>gh-page</code>. This branch will be used by GitHub Pages to build and serve your website. You can create the branch using the branch creation feature in your GitHub repository.</p>
<p><img loading="lazy" src="/posts/setting-up-personal-blog-with-hugo-and-gh-pages/create-gh-pages.png" type="" alt="Create gh-pages"  /></p>
<p>Then, go to the <code>Settings</code> tab near the top of your repository.</p>
<p><img loading="lazy" src="/posts/setting-up-personal-blog-with-hugo-and-gh-pages/repository-setting.png" type="" alt="Repository setting"  /></p>
<p>In the left hand pane, locate and click on the <code>Pages</code> category.</p>
<p><img loading="lazy" src="/posts/setting-up-personal-blog-with-hugo-and-gh-pages/gh-pages-setting.png" type="" alt="GitHub pages setting"  /></p>
<p>By default, GitHub Pages will be disabled for your repository. To enable it, we need to select a branch for GitHub Pages to build and serve our website from. Under the <code>Source</code> section in the middle pane, you will see a dropdown menu labeled <code>None</code>. Click on the dropdown menu and select the <code>gh-pages</code> branch. This tells GitHub Pages to build and serve your website from the <code>gh-pages</code> branch.</p>
<p><img loading="lazy" src="/posts/setting-up-personal-blog-with-hugo-and-gh-pages/select-deployment-branch.png" type="" alt="Select deployment branch"  /></p>
<p>After selecting the deployment branch, you will see a notification indicating where your site will be published. It will provide you with a URL where your website can be accessed.</p>
<p><img loading="lazy" src="/posts/setting-up-personal-blog-with-hugo-and-gh-pages/published-url.png" type="" alt="Published URL"  /></p>
<p>Wait for a few minutes to allow GitHub Pages to build and deploy your website. When the deployment completes, you can click on the URL provided in the notification to view your website. It may take some time for the changes to propagate and for your website to become accessible.</p>
<p><img loading="lazy" src="/posts/setting-up-personal-blog-with-hugo-and-gh-pages/site-demo.png" type="" alt="Site demo"  /></p>
<h2 id="conclusion">Conclusion</h2>
<p>Congratulations on setting up your blog using <strong>Hugo</strong>, <strong>Markdown</strong>, and <strong>GitHub Pages</strong>! This free and accessible solution enables you to create and share your technical knowledge with a wide audience. Happy blogging!</p>
<h2 id="reference">Reference</h2>
<ul>
<li>
<p><a href="https://chrisjhart.com/Creating-A-Simple-Free-Blog-Hugo/">How to Create a Simple, Free Blog with Hugo and GitHub Pages</a>.</p>
</li>
<li>
<p><a href="https://github.com/reorx/hugo-PaperModX">https://github.com/reorx/hugo-PaperModX</a>.</p>
</li>
<li>
<p><a href="https://gohugo.io/">https://gohugo.io/</a>.</p>
</li>
<li>
<p><a href="https://github.com/olOwOlo/hugo-theme-even/">https://github.com/olOwOlo/hugo-theme-even/</a>.</p>
</li>
</ul>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
