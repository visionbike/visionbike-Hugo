<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>Setting Up NVIDIA CUDA on WSL 2 | Visionbike - Personal Blog of CV | DSP | ML notes</title>
<meta name=keywords content>
<meta name=description content="Windows Subsystem for Linux (WSL) is a compatibility layer provided by Microsoft that allows you to run a Linux environment directly on WIndows. It will bring Linux applications, utilities and Bash command-line tool directly on Windows without the overhead of traditional virtual machine or dualboot setup.
1. Prequisites The Windows must be upgraded to Windows 10 version 2004 and higher or Windows 11 to install WSL by a single command line.">
<meta name=author content="Visionbike">
<link rel=canonical href=http://visionbike.github.io/posts/setitng-up-nvidia-cuda-on-wsl2/>
<link crossorigin=anonymous href=/assets/css/stylesheet.min.bfeaceca36889d51e73d9931f189f3cfb4e0393ea69bad26f042ab99d668e1d1.css integrity="sha256-v+rOyjaInVHnPZkx8Ynzz7TgOT6mm60m8EKrmdZo4dE=" rel="preload stylesheet" as=style>
<link rel=icon type=image/png href=/images/favicon.ico>
<link rel=apple-touch-icon href=/images/apple-touch-icon.png>
<link rel=icon type=image/png sizes=16x16 href=/images/favicon-16x16.png>
<link rel=icon type=image/png sizes=32x32 href=/images/favicon-32x32.png>
<meta name=twitter:title content="Setting Up NVIDIA CUDA on WSL 2 | Visionbike - Personal Blog of CV | DSP | ML notes">
<meta name=twitter:description content="Windows Subsystem for Linux (WSL) is a compatibility layer provided by Microsoft that allows you to run a Linux environment directly on WIndows. It will bring Linux applications, utilities and Bash command-line tool directly on Windows without the overhead of traditional virtual machine or dualboot setup.
1. Prequisites The Windows must be upgraded to Windows 10 version 2004 and higher or Windows 11 to install WSL by a single command line.">
<meta property="og:title" content="Setting Up NVIDIA CUDA on WSL 2 | Visionbike - Personal Blog of CV | DSP | ML notes">
<meta property="og:description" content="Windows Subsystem for Linux (WSL) is a compatibility layer provided by Microsoft that allows you to run a Linux environment directly on WIndows. It will bring Linux applications, utilities and Bash command-line tool directly on Windows without the overhead of traditional virtual machine or dualboot setup.
1. Prequisites The Windows must be upgraded to Windows 10 version 2004 and higher or Windows 11 to install WSL by a single command line.">
<meta property="og:type" content="article">
<meta property="og:url" content="http://visionbike.github.io/posts/setitng-up-nvidia-cuda-on-wsl2/">
<meta property="article:section" content="posts">
<meta property="article:published_time" content="2023-07-13T16:44:33+08:00">
<meta property="article:modified_time" content="2023-07-13T16:44:33+08:00">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://visionbike.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Setting Up NVIDIA CUDA on WSL 2","item":"http://visionbike.github.io/posts/setitng-up-nvidia-cuda-on-wsl2/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Setting Up NVIDIA CUDA on WSL 2 | Visionbike - Personal Blog of CV | DSP | ML notes","name":"Setting Up NVIDIA CUDA on WSL 2","description":"Windows Subsystem for Linux (WSL) is a compatibility layer provided by Microsoft that allows you to run a Linux environment directly on WIndows. It will bring Linux applications, utilities and Bash command-line tool directly on Windows without the overhead of traditional virtual machine or dualboot setup.\n1. Prequisites The Windows must be upgraded to Windows 10 version 2004 and higher or Windows 11 to install WSL by a single command line.","keywords":[],"wordCount":"4104","inLanguage":"en","datePublished":"2023-07-13T16:44:33+08:00","dateModified":"2023-07-13T16:44:33+08:00","author":{"@type":"Person","name":"Visionbike"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://visionbike.github.io/posts/setitng-up-nvidia-cuda-on-wsl2/"},"publisher":{"@type":"Organization","name":"Visionbike - Personal Blog of CV | DSP | ML notes","logo":{"@type":"ImageObject","url":"http://visionbike.github.io/favicon.ico"}}}</script>
<noscript>
<style>#theme-toggle,.top-link{display:none}</style>
</noscript><link rel=stylesheet href=lib/fontawesome-free/all.min.css>
</head>
<body class="dark type-posts kind-page layout-" id=top><script data-no-instant>function switchTheme(a){switch(a){case'light':document.body.classList.remove('dark');break;case'dark':document.body.classList.add('dark');break;default:window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')}}function isDarkTheme(){return document.body.className.includes("dark")}function getPrefTheme(){return localStorage.getItem("pref-theme")}function setPrefTheme(a){switchTheme(a),localStorage.setItem("pref-theme",a)}const toggleThemeCallbacks={};toggleThemeCallbacks.main=a=>{a?setPrefTheme('light'):setPrefTheme('dark')},window.addEventListener('toggle-theme',function(){const a=isDarkTheme();for(const b in toggleThemeCallbacks)toggleThemeCallbacks[b](a)});function toggleThemeListener(){window.dispatchEvent(new CustomEvent('toggle-theme'))}</script>
<script>(function(){const b='dark',a=getPrefTheme(),c=a||b;switchTheme(c)})()</script>
<script>document.addEventListener('DOMContentLoaded',function(){var a=document.querySelectorAll('.command code');a.forEach(function(a){var b=a.textContent.split('\n'),c,d;a.textContent='',c=a.dataset.lang||'bash',d=c==='cmd'?'>':'$',c==='python'&&(d='>>>'),b.forEach(function(c,e){c.trim()!==''&&(a.innerHTML+='<span class="prompt">'+d+'</span> '+c,e!==b.length-1&&(a.innerHTML+='<br>'))})})})</script>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=http://visionbike.github.io/ accesskey=h title="Visionbike (Alt + H)">
<img src=http://visionbike.github.io/images/apple-touch-icon.png alt=logo aria-label=logo height=35 width=35>Visionbike</a>
<span class=logo-switches>
</span>
</div>
<ul id=menu>
<li>
<a href=http://visionbike.github.io/posts/ title=Posts class=active>Posts
</a>
</li>
<li>
<a href=http://visionbike.github.io/tags/ title=Tags>Tags
</a>
</li>
<li>
<a href=http://visionbike.github.io/archives/ title=Archive>Archive
</a>
</li>
<li>
<a href=http://visionbike.github.io/publish/ title=Publish>Publish
</a>
</li>
<li>
<a href=http://visionbike.github.io/about/ title=About>About
</a>
</li>
<li>
<a href=http://visionbike.github.io/search/ title="Search (Alt + /)" data-no-instant accesskey=/>Search
</a>
</li>
</ul>
</nav>
</header>
<main class="main post">
<article class=post-single>
<header class=post-header>
<div class=breadcrumbs><a href=http://visionbike.github.io/>Home</a>&nbsp;Â»&nbsp;<a href=http://visionbike.github.io/posts/>Posts</a></div><h1 class=post-title>Setting Up NVIDIA CUDA on WSL 2</h1>
<div class=post-meta><span class=meta-item><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar" style="user-select:text"><rect x="3" y="4" width="18" height="18" rx="2" ry="2" style="user-select:text"/><line x1="16" y1="2" x2="16" y2="6" style="user-select:text"/><line x1="8" y1="2" x2="8" y2="6" style="user-select:text"/><line x1="3" y1="10" x2="21" y2="10" style="user-select:text"/></svg>
<span>July 13, 2023</span></span><span class=meta-item><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text" style="user-select:text"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z" style="user-select:text"/><polyline points="14 2 14 8 20 8" style="user-select:text"/><line x1="16" y1="13" x2="8" y2="13" style="user-select:text"/><line x1="16" y1="17" x2="8" y2="17" style="user-select:text"/><polyline points="10 9 9 9 8 9" style="user-select:text"/></svg>
<span>4104 words</span></span><span class=meta-item><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke="currentcolor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<span>20 min</span></span>
</div>
</header> <div class="toc side right">
<details>
<summary accesskey=c title="(Alt + C)">
<span class=details>Table of Contents</span>
</summary>
<div class=inner><ul>
<li>
<a href=#1-prequisites aria-label="1. Prequisites">1. Prequisites</a></li>
<li>
<a href=#2-installing-mamba aria-label="2. Installing Mamba">2. Installing Mamba</a></li>
<li>
<a href=#2-installing-nvidia-cuda-toolkit-for-wsl-2 aria-label="2. Installing NVIDIA CUDA Toolkit for WSL 2">2. Installing NVIDIA CUDA Toolkit for WSL 2</a></li>
<li>
<a href=#2-installing-nvidia-cudnn-for-wsl-2 aria-label="2. Installing NVIDIA CuDNN for WSL 2">2. Installing NVIDIA CuDNN for WSL 2</a></li>
<li>
<a href=#3-installing-nvidia-tensorrt-optional aria-label="3. Installing NVIDIA TensorRT (Optional)">3. Installing NVIDIA TensorRT (Optional)</a></li>
<li>
<a href=#3-set-up-python-environment-using-mambaforge aria-label="3. Set up Python environment using Mambaforge">3. Set up Python environment using Mambaforge</a></li>
<li>
<a href=#conclusion aria-label=Conclusion>Conclusion</a></li>
<li>
<a href=#reference aria-label=Reference>Reference</a>
</li>
</ul>
</div>
</details>
</div>
<div class=post-content><p><strong>Windows Subsystem for Linux (WSL)</strong> is a compatibility layer provided by Microsoft that allows you to run a <strong>Linux environment</strong> directly on <strong>WIndows</strong>. It will bring Linux applications, utilities and Bash command-line tool directly on <strong>Windows</strong> without the overhead of traditional virtual machine or dualboot setup.</p>
<h2 id=1-prequisites>1. Prequisites<a hidden class=anchor aria-hidden=true href=#1-prequisites>Â¶</a></h2>
<p>The <strong>Windows</strong> must be upgraded to <strong>Windows 10 version 2004 and higher</strong> or <strong>Windows 11</strong> to install <strong>WSL</strong> by a single command line. Open Command Prompt in <strong>Administrator</strong> mode, then run the following command:</p>
<div class="highlight command"><pre tabindex=0 class=chroma><code class=language-cmd data-lang=cmd>wsl --install --no-distribution
</code></pre></div><p>The command will setup <strong>WSL version 2</strong> by default without any Linux distribution. Reboot the machine to apply changes.</p>
<p>Open a Command Prompt window and install latest version Ubuntu distro:</p>
<div class="highlight command"><pre tabindex=0 class=chroma><code class=language-cmd data-lang=cmd>wsl --install -d Ubuntu
</code></pre></div><p>The installation takes a few minutes and you need to create <strong>user name</strong> and <strong>password</strong>.</p>
<p><img loading=lazy src=/posts/setitng-up-nvidia-cuda-on-wsl2/wsl-ubuntu-installation.png type alt="WSL-Ubuntu Installation"></p>
<details class="admonition note"><summary class=admonition-title>Display available WSL Linux distros</summary>
<p>If you want to install another Linux distro, you can run following command to display a list of available WSL Linux distros:</p>
<div class="highlight command"><pre tabindex=0 class=chroma><code class=language-cmd data-lang=cmd>wsl --list --online
</code></pre></div></details>
<p>For the first start, we should to install several Linux updates. The process will depend on the speed of the internet, so be patient if it slow!</p>
<div class="highlight command"><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>sudo apt update -y <span class=o>&amp;&amp;</span> sudo apt upgrade -y
</code></pre></div><details class="admonition note"><summary class=admonition-title>Issue: /usr/lib/wsl/lib/libcuda.so.1 is not a symbolic link</summary>
<p>If your machine has integrated <strong>NVIDIA</strong> GPU, you may meet the <code>/usr/lib/wsl/lib/libcuda.so.1 is not a symbolic link</code> issue when updating the system.</p>
<p>In the WSL Ubuntu side, run following commands:</p>
<div class="highlight command"><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=nb>cd</span> /usr/lib/wsl/lib
sudo rm libcuda.so
sudo rm libcuda.so.1
sudo ln -s libcuda.so.1.1 libcuda.so
sudo ln -s libcuda.so.1.1 libcuda.so.1
</code></pre></div><p>Now reinstall <code>libc-bin</code> package.</p>
<div class="highlight command"><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>sudo apt reinstall libc-bin
</code></pre></div><p>In the Windows side, run following comands in <strong>Administrator</strong> mode:</p>
<div class="highlight command"><pre tabindex=0 class=chroma><code class=language-cmd data-lang=cmd><span class=k>cd</span> C:\Windows\System32\lxss\lib
<span class=k>del</span> libcuda.so
<span class=k>del</span> libcuda.so.1
<span class=k>mklink</span> libcuda.so libcuda.so.1.1
<span class=k>mklink</span> libcuda.so.1 libcuda.so.1.1
</code></pre></div><p>Reboot the machine and run the <code>sudo ldconfig</code> command in WSL Ubuntu side to verify the problem is solved.</p>
</details>
<h2 id=2-installing-mamba>2. Installing Mamba<a hidden class=anchor aria-hidden=true href=#2-installing-mamba>Â¶</a></h2>
<p><a href=https://mamba.readthedocs.io/en/latest/user_guide/mamba.html><strong>Mamba</strong></a> is a command-line interfacer (CLI) to manage <code>conda</code>&rsquo;s environemts. For <code>mamba</code> configuration, please refer to <a href=https://conda.io/projects/conda/en/latest/user-guide/configuration/index.html><strong>conda documentation</strong></a>.</p>
<p>For the fresh installation, you can install <a href=https://github.com/conda-forge/miniforge><strong>Miniforge distribution</strong></a> >= <code>Miniforge3-22.3.1.0</code>. <strong>Miniforge</strong> comes with the popular <code>conda-forge</code> channel preconfigured, but you can modify the configuration to use any channel you like.</p>
<p>For Unix-like platforms (MAC OS and Linux), download the installer using curl or wget or your favorite program and run the script</p>
<div class="highlight command"><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>curl -L -O <span class=s2>&#34;https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-</span><span class=k>$(</span>uname<span class=k>)</span><span class=s2>-</span><span class=k>$(</span>uname -m<span class=k>)</span><span class=s2>.sh&#34;</span>
bash Miniforge3-<span class=k>$(</span>uname<span class=k>)</span>-<span class=k>$(</span>uname -m<span class=k>)</span>.sh
</code></pre></div><p>After successful installation, you can use use the mamba commands as described in this <a href=https://mamba.readthedocs.io/en/latest/user_guide/mamba.html#mamba><strong>user guide</strong></a>.</p>
<div class="admonition note"><p class=admonition-title>Working with Conda Environment</p>
<p>The command using <code>mamba</code> is similar to the <code>conda</code> command.</p>
<ul>
<li>Create new environment</li>
</ul>
<div class="highlight command"><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>mamba create -n <span class=p>&amp;</span>lt<span class=p>;</span>ENV_NAME<span class=p>&amp;</span>gt<span class=p>;</span> <span class=nv>python</span><span class=o>=</span><span class=p>&amp;</span>lt<span class=p>;</span>VERSION<span class=p>&amp;</span>gt<span class=p>;</span>
</code></pre></div><ul>
<li>Activate an environment</li>
</ul>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>mamba activate <span class=p>&amp;</span>lt<span class=p>;</span>ENV_NAME<span class=p>&amp;</span>gt<span class=p>;</span>
</code></pre></div><ul>
<li>Deactivate environment</li>
</ul>
<div class="highlight command"><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>mamba deactivate
</code></pre></div><ul>
<li>Delete an environment</li>
</ul>
<div class="highlight command"><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>mamba env remove -n <span class=p>&amp;</span>lt<span class=p>;</span>ENV_NAME<span class=p>&amp;</span>gt<span class=p>;</span>
</code></pre></div><ul>
<li>Show all created environments</li>
</ul>
<div class="highlight command"><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>mamba env list
</code></pre></div><p><strong>Remember to activate an environment first, do not install any packages in your base environment!</strong></p>
<ul>
<li>Install python packages</li>
</ul>
<div class="highlight command"><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>mamba install <span class=p>&amp;</span>lt<span class=p>;</span>PKG_NAME<span class=p>&amp;</span>gt<span class=p>;</span><span class=o>[=</span>VERSION<span class=o>]</span> <span class=o>[</span>-c <span class=p>&amp;</span>lt<span class=p>;</span>CHANNEL_NAME<span class=p>&amp;</span>gt<span class=p>;</span><span class=o>]</span>
</code></pre></div><p>When installing a package, you can optionally indicate specific additional channel that the packages are posted by community. The <code>conda-forge</code> is one of most common additional channels.</p>
<ul>
<li>Delete packages</li>
</ul>
<div class="highlight command"><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>mamba remove <span class=p>&amp;</span>lt<span class=p>;</span>PKG_NAME<span class=p>&amp;</span>gt<span class=p>;</span>
</code></pre></div><ul>
<li>Show all installed packages in the virtual environment</li>
</ul>
<div class="highlight command"><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>mamba list <span class=o>[</span>-n <span class=p>&amp;</span>lt<span class=p>;</span>ENV_NAME<span class=p>&amp;</span>gt<span class=p>;</span><span class=o>]</span>
</code></pre></div></div>
<h2 id=2-installing-nvidia-cuda-toolkit-for-wsl-2>2. Installing NVIDIA CUDA Toolkit for WSL 2<a hidden class=anchor aria-hidden=true href=#2-installing-nvidia-cuda-toolkit-for-wsl-2>Â¶</a></h2>
<p>Note that you <strong>only need to install NVIDIA Driver for Windows. Do not install any Linux Driver in WSL</strong>.</p>
<p>The latest NVIDIA Windows GPU Driver will fully support WSL 2. With CUDA support in the driver, existing applications compiled on a Linux system for the same target GPU can run unmodified within the WSL environment. Once NVIDIA Windows GPU Driver is installed in the system, the CUDA driver will be stubbed inside the WSL 2 as <code>libcuda.so</code>. Therefore, you only use a separate CUDA Toolkit for WSL 2 which does not contain the NVIDIA Linux GPU Driver.</p>
<p>To get started with running CUDA on <strong>WSL 2</strong>, you need to instal NVIDIA Driver on Windows 11 with a compatible GeForce or NVIDIA RTX/Quadro card from <a href=https://www.nvidia.com/Download/index.aspx><strong>here</strong></a>. Once a <strong>Windows NVIDIA GPU driver</strong> is installed on the system, CUDA becomes available within <strong>WSL 2</strong>. The <strong>CUDA driver</strong> installed on <strong>Windows host</strong> will be stubbed inside the WSL 2 as <code>libcuda.so</code>.</p>
<p>However, the <strong>NVIDIA CUDA Toolkit for WSL</strong> need to be install for further use.</p>
<p>First, remove the old GPG key:</p>
<div class="highlight command"><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>sudo apt-key del 7fa2af80
</code></pre></div><p>Install the <strong>NVIDIA CUDA Toolkit for WSL-Ubuntu</strong> by following the step in <a href="https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&Distribution=WSL-Ubuntu&target_version=2.0&target_type=deb_local"><strong>this link</strong></a></p>
<p>Download <a href="https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&Distribution=WSL-Ubuntu&target_version=2.0&target_type=deb_local">CUDA Toolkit for WSL 2</a>.</p>
<p><img loading=lazy src=/posts/setitng-up-nvidia-cuda-on-wsl2/nvidia-cuda-toolkit-installation.png type alt="NVIDIA CUDA Toolkit Installation"></p>
<p>Some actions must be taken after the installation before the CUDA Toolkit and Driver can be used. The <code>PATH</code> and <code>LD_LIBRARY_PATH</code> variables need to be updated.</p>
<p>Open <code>~/.bashrc</code> by running <code>nano ~/.bashrc</code>, add two lines at the end of the file:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=nb>export</span> <span class=nv>PATH</span><span class=o>=</span>/usr/local/cuda-12.4/bin<span class=si>${</span><span class=nv>PATH</span><span class=p>:+:</span><span class=si>${</span><span class=nv>PATH</span><span class=si>}}</span>
<span class=nb>export</span> <span class=nv>LD_LIBRARY_PATH</span><span class=o>=</span>/usr/local/cuda-12.4/lib64<span class=si>${</span><span class=nv>LD_LIBRARY_PATH</span><span class=p>:+:</span><span class=si>${</span><span class=nv>LD_LIBRARY_PATH</span><span class=si>}}</span>
</code></pre></div><p>Then you run <code>source ~/.bashrc</code> for activation changes without reboot.</p>
<details class="admonition info"><summary class=admonition-title>Verify CUDA Toolkit Installation</summary>
<p>You can verify the installation by running the following command:</p>
<div class="highlight command"><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>nvcc --version
</code></pre></div><p>If the installation was successful, you should see the CUDA version information displayed.</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>nvcc: NVIDIA <span class=o>(</span>R<span class=o>)</span> Cuda compiler driver
Copyright <span class=o>(</span>c<span class=o>)</span> 2005-2024 NVIDIA Corporation
Built on Thu_Mar_28_02:18:24_PDT_2024
Cuda compilation tools, release 12.4, V12.4.131
Build cuda_12.4.r12.4/compiler.34097967_0
</code></pre></div><p>It is important to verify that the <strong>NVIDIA CUDA Toolkit</strong> can find and communicate correctly with CUDA-compatible hardware. To do this, you need to compile and run some sample programs.</p>
<p>CUDA samples are located in <a href=https://github.com/nvidia/cuda-samples><strong>https://github.com/nvidia/cuda-samples</strong></a>. To use the samples, clone the project, build the samples in <code>cuda-samples</code> directory using <code>make</code> command and run them following the instruction on the Github page.</p>
<p>To verify a correct configuration of the hardware and software, it is highly recommended that you build and run the <code>deviceQuery</code> sample program.</p>
<div class="highlight command"><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>./deviceQuery
</code></pre></div><p>If CUDA is installed and configured correctly, the output should look similar as below:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>./deviceQuery Starting...

 CUDA Device Query <span class=o>(</span>Runtime API<span class=o>)</span> version <span class=o>(</span>CUDART static linking<span class=o>)</span>

Detected <span class=m>1</span> CUDA Capable device<span class=o>(</span>s<span class=o>)</span>

Device 0: <span class=s2>&#34;NVIDIA GeForce RTX 3070 Ti Laptop GPU&#34;</span>
  CUDA Driver Version / Runtime Version          12.4 / 12.4
  CUDA Capability Major/Minor version number:    8.6
  Total amount of global memory:                 <span class=m>8192</span> MBytes <span class=o>(</span><span class=m>8589410304</span> bytes<span class=o>)</span>
  <span class=o>(</span>046<span class=o>)</span> Multiprocessors, <span class=o>(</span>128<span class=o>)</span> CUDA Cores/MP:    <span class=m>5888</span> CUDA Cores
  GPU Max Clock rate:                            <span class=m>1410</span> MHz <span class=o>(</span>1.41 GHz<span class=o>)</span>
  Memory Clock rate:                             <span class=m>7001</span> Mhz
  Memory Bus Width:                              256-bit
  L2 Cache Size:                                 <span class=m>4194304</span> bytes
  Maximum Texture Dimension Size <span class=o>(</span>x,y,z<span class=o>)</span>         <span class=nv>1D</span><span class=o>=(</span>131072<span class=o>)</span>, <span class=nv>2D</span><span class=o>=(</span>131072, 65536<span class=o>)</span>, <span class=nv>3D</span><span class=o>=(</span>16384, 16384, 16384<span class=o>)</span>
  Maximum Layered 1D Texture Size, <span class=o>(</span>num<span class=o>)</span> layers  <span class=nv>1D</span><span class=o>=(</span>32768<span class=o>)</span>, <span class=m>2048</span> layers
  Maximum Layered 2D Texture Size, <span class=o>(</span>num<span class=o>)</span> layers  <span class=nv>2D</span><span class=o>=(</span>32768, 32768<span class=o>)</span>, <span class=m>2048</span> layers
  Total amount of constant memory:               <span class=m>65536</span> bytes
  Total amount of shared memory per block:       <span class=m>49152</span> bytes
  Total shared memory per multiprocessor:        <span class=m>102400</span> bytes
  Total number of registers available per block: <span class=m>65536</span>
  Warp size:                                     <span class=m>32</span>
  Maximum number of threads per multiprocessor:  <span class=m>1536</span>
  Maximum number of threads per block:           <span class=m>1024</span>
  Max dimension size of a thread block <span class=o>(</span>x,y,z<span class=o>)</span>: <span class=o>(</span>1024, 1024, 64<span class=o>)</span>
  Max dimension size of a grid size    <span class=o>(</span>x,y,z<span class=o>)</span>: <span class=o>(</span>2147483647, 65535, 65535<span class=o>)</span>
  Maximum memory pitch:                          <span class=m>2147483647</span> bytes
  Texture alignment:                             <span class=m>512</span> bytes
  Concurrent copy and kernel execution:          Yes with <span class=m>1</span> copy engine<span class=o>(</span>s<span class=o>)</span>
  Run <span class=nb>time</span> limit on kernels:                     Yes
  Integrated GPU sharing Host Memory:            No
  Support host page-locked memory mapping:       Yes
  Alignment requirement <span class=k>for</span> Surfaces:            Yes
  Device has ECC support:                        Disabled
  Device supports Unified Addressing <span class=o>(</span>UVA<span class=o>)</span>:      Yes
  Device supports Managed Memory:                Yes
  Device supports Compute Preemption:            Yes
  Supports Cooperative Kernel Launch:            Yes
  Supports MultiDevice Co-op Kernel Launch:      No
  Device PCI Domain ID / Bus ID / location ID:   <span class=m>0</span> / <span class=m>1</span> / <span class=m>0</span>
  Compute Mode:
     &lt; Default <span class=o>(</span>multiple host threads can use ::cudaSetDevice<span class=o>()</span> with device simultaneously<span class=o>)</span> &gt;

deviceQuery, CUDA <span class=nv>Driver</span> <span class=o>=</span> CUDART, CUDA Driver <span class=nv>Version</span> <span class=o>=</span> 12.4, CUDA Runtime <span class=nv>Version</span> <span class=o>=</span> 12.4, <span class=nv>NumDevs</span> <span class=o>=</span> <span class=m>1</span>
<span class=nv>Result</span> <span class=o>=</span> PASS
</code></pre></div><p>By running the <code>bandwidthTest</code> program, you can ensure that the system and CUDA-capable device are able to communicate correctly.</p>
<div class="highlight command"><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>./bandwidthText
</code></pre></div><p>The output shoud be here.</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=o>[</span>CUDA Bandwidth Test<span class=o>]</span> - Starting...
Running on...

 Device 0: NVIDIA GeForce RTX <span class=m>3070</span> Ti Laptop GPU
 Quick Mode

 Host to Device Bandwidth, <span class=m>1</span> Device<span class=o>(</span>s<span class=o>)</span>
 PINNED Memory Transfers
   Transfer Size <span class=o>(</span>Bytes<span class=o>)</span>        Bandwidth<span class=o>(</span>GB/s<span class=o>)</span>
   <span class=m>32000000</span>                     24.5

 Device to Host Bandwidth, <span class=m>1</span> Device<span class=o>(</span>s<span class=o>)</span>
 PINNED Memory Transfers
   Transfer Size <span class=o>(</span>Bytes<span class=o>)</span>        Bandwidth<span class=o>(</span>GB/s<span class=o>)</span>
   <span class=m>32000000</span>                     26.3

 Device to Device Bandwidth, <span class=m>1</span> Device<span class=o>(</span>s<span class=o>)</span>
 PINNED Memory Transfers
   Transfer Size <span class=o>(</span>Bytes<span class=o>)</span>        Bandwidth<span class=o>(</span>GB/s<span class=o>)</span>
   <span class=m>32000000</span>                     323.6

<span class=nv>Result</span> <span class=o>=</span> PASS

NOTE: The CUDA Samples are not meant <span class=k>for</span> performance measurements. Results may vary when GPU Boost is enabled.
</code></pre></div><p>For CUDA graphic programs, <strong>WSL 2</strong> currently does not support <strong>GL</strong>, <strong>Vulcan</strong>. Hence, you cannot run CUDA graphic programs.</p>
</details>
<h2 id=2-installing-nvidia-cudnn-for-wsl-2>2. Installing NVIDIA CuDNN for WSL 2<a hidden class=anchor aria-hidden=true href=#2-installing-nvidia-cudnn-for-wsl-2>Â¶</a></h2>
<p>Since <strong>cuDNN version 9</strong> can exist independently with previous version <strong>cuDNN</strong>, you can refer <a href="https://developer.nvidia.com/cudnn-downloads?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=22.04&target_type=deb_local"><strong>here</strong></a> and follow the steps for your target platform.</p>
<p><img loading=lazy src=/posts/setitng-up-nvidia-cuda-on-wsl2/nvidia-cudnn-installation.png type alt="NVIDIA cuDNN Installation"></p>
<details class="admonition info"><summary class=admonition-title>Verify cuDNN Installation</summary>
<p>To verify <strong>cuDNN</strong> is install and is running properly, you can download and compile <strong>cuDNN</strong> samples. These samples can be downloaded by running:</p>
<div class="highlight command"><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>sudo apt install -y libcudnn9-samples
</code></pre></div><p>The samples is located in <strong>/usr/src/cudnn_samples_v9</strong> directory, copy the directory to <code>$HOME</code> and set the permission.</p>
<div class="highlight command"><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>sudo cp -r cudnn_samples_v9/ <span class=nv>$HOME</span>
sudo chmod -R a+rwx <span class=nv>$HOME</span>/cudnn_samples_v9
</code></pre></div><p>For the <code>mnistCUDNN</code> sample, install <code>FreeImage</code> package and compile the sample.</p>
<div class="highlight command"><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>sudo apt install -y libfreeimage3 libfreeimage-dev
<span class=nb>cd</span> <span class=nv>$HOME</span>/cudnn_samples_v9/mnistCUDNN
make clean <span class=o>&amp;&amp;</span> make
./mnistCUDNN
</code></pre></div><p>The result should be as:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>Executing: mnistCUDNN
cudnnGetVersion<span class=o>()</span> : <span class=m>90100</span> , CUDNN_VERSION from cudnn.h : <span class=m>90100</span> <span class=o>(</span>9.1.0<span class=o>)</span>
Host compiler version : GCC 11.4.0

There are <span class=m>1</span> CUDA capable devices on your machine :
device <span class=m>0</span> : sms <span class=m>46</span>  Capabilities 8.6, SmClock 1410.0 Mhz, MemSize <span class=o>(</span>Mb<span class=o>)</span> 8191, MemClock 7001.0 Mhz, <span class=nv>Ecc</span><span class=o>=</span>0, <span class=nv>boardGroupID</span><span class=o>=</span><span class=m>0</span>
Using device <span class=m>0</span>

Testing single precision
Loading binary file data/conv1.bin
Loading binary file data/conv1.bias.bin
Loading binary file data/conv2.bin
Loading binary file data/conv2.bias.bin
Loading binary file data/ip1.bin
Loading binary file data/ip1.bias.bin
Loading binary file data/ip2.bin
Loading binary file data/ip2.bias.bin
Loading image data/one_28x28.pgm
Performing forward propagation ...
Testing cudnnGetConvolutionForwardAlgorithm_v7 ...
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 1: -1.000000 <span class=nb>time</span> requiring <span class=m>0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 0: -1.000000 <span class=nb>time</span> requiring <span class=m>0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 2: -1.000000 <span class=nb>time</span> requiring <span class=m>0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 5: -1.000000 <span class=nb>time</span> requiring <span class=m>178432</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 4: -1.000000 <span class=nb>time</span> requiring <span class=m>184784</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 7: -1.000000 <span class=nb>time</span> requiring <span class=m>2057744</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class=k>for</span> Algo 6: -1.000000 <span class=nb>time</span> requiring <span class=m>0</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class=k>for</span> Algo 3: -1.000000 <span class=nb>time</span> requiring <span class=m>0</span> memory
Testing cudnnFindConvolutionForwardAlgorithm ...
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 0: 0.009216 <span class=nb>time</span> requiring <span class=m>0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 1: 0.009216 <span class=nb>time</span> requiring <span class=m>0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 5: 0.096256 <span class=nb>time</span> requiring <span class=m>178432</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 7: 0.115712 <span class=nb>time</span> requiring <span class=m>2057744</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 4: 0.168960 <span class=nb>time</span> requiring <span class=m>184784</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 2: 0.546016 <span class=nb>time</span> requiring <span class=m>0</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class=k>for</span> Algo 6: -1.000000 <span class=nb>time</span> requiring <span class=m>0</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class=k>for</span> Algo 3: -1.000000 <span class=nb>time</span> requiring <span class=m>0</span> memory
Testing cudnnGetConvolutionForwardAlgorithm_v7 ...
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 1: -1.000000 <span class=nb>time</span> requiring <span class=m>0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 0: -1.000000 <span class=nb>time</span> requiring <span class=m>0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 2: -1.000000 <span class=nb>time</span> requiring <span class=m>0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 4: -1.000000 <span class=nb>time</span> requiring <span class=m>2450080</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 5: -1.000000 <span class=nb>time</span> requiring <span class=m>4656640</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 7: -1.000000 <span class=nb>time</span> requiring <span class=m>1433120</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class=k>for</span> Algo 6: -1.000000 <span class=nb>time</span> requiring <span class=m>0</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class=k>for</span> Algo 3: -1.000000 <span class=nb>time</span> requiring <span class=m>0</span> memory
Testing cudnnFindConvolutionForwardAlgorithm ...
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 0: 0.035840 <span class=nb>time</span> requiring <span class=m>0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 1: 0.058368 <span class=nb>time</span> requiring <span class=m>0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 2: 0.066560 <span class=nb>time</span> requiring <span class=m>0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 7: 0.093184 <span class=nb>time</span> requiring <span class=m>1433120</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 5: 0.109600 <span class=nb>time</span> requiring <span class=m>4656640</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 4: 0.192512 <span class=nb>time</span> requiring <span class=m>2450080</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class=k>for</span> Algo 6: -1.000000 <span class=nb>time</span> requiring <span class=m>0</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class=k>for</span> Algo 3: -1.000000 <span class=nb>time</span> requiring <span class=m>0</span> memory
Resulting weights from Softmax:
0.0000000 0.9999399 0.0000000 0.0000000 0.0000561 0.0000000 0.0000012 0.0000017 0.0000010 0.0000000
Loading image data/three_28x28.pgm
Performing forward propagation ...
Testing cudnnGetConvolutionForwardAlgorithm_v7 ...
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 1: -1.000000 <span class=nb>time</span> requiring <span class=m>0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 0: -1.000000 <span class=nb>time</span> requiring <span class=m>0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 2: -1.000000 <span class=nb>time</span> requiring <span class=m>0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 5: -1.000000 <span class=nb>time</span> requiring <span class=m>178432</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 4: -1.000000 <span class=nb>time</span> requiring <span class=m>184784</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 7: -1.000000 <span class=nb>time</span> requiring <span class=m>2057744</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class=k>for</span> Algo 6: -1.000000 <span class=nb>time</span> requiring <span class=m>0</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class=k>for</span> Algo 3: -1.000000 <span class=nb>time</span> requiring <span class=m>0</span> memory
Testing cudnnFindConvolutionForwardAlgorithm ...
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 1: 0.008192 <span class=nb>time</span> requiring <span class=m>0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 2: 0.009216 <span class=nb>time</span> requiring <span class=m>0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 0: 0.026624 <span class=nb>time</span> requiring <span class=m>0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 7: 0.061248 <span class=nb>time</span> requiring <span class=m>2057744</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 4: 0.073728 <span class=nb>time</span> requiring <span class=m>184784</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 5: 0.106496 <span class=nb>time</span> requiring <span class=m>178432</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class=k>for</span> Algo 6: -1.000000 <span class=nb>time</span> requiring <span class=m>0</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class=k>for</span> Algo 3: -1.000000 <span class=nb>time</span> requiring <span class=m>0</span> memory
Testing cudnnGetConvolutionForwardAlgorithm_v7 ...
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 1: -1.000000 <span class=nb>time</span> requiring <span class=m>0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 0: -1.000000 <span class=nb>time</span> requiring <span class=m>0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 2: -1.000000 <span class=nb>time</span> requiring <span class=m>0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 4: -1.000000 <span class=nb>time</span> requiring <span class=m>2450080</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 5: -1.000000 <span class=nb>time</span> requiring <span class=m>4656640</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 7: -1.000000 <span class=nb>time</span> requiring <span class=m>1433120</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class=k>for</span> Algo 6: -1.000000 <span class=nb>time</span> requiring <span class=m>0</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class=k>for</span> Algo 3: -1.000000 <span class=nb>time</span> requiring <span class=m>0</span> memory
Testing cudnnFindConvolutionForwardAlgorithm ...
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 0: 0.031520 <span class=nb>time</span> requiring <span class=m>0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 1: 0.038912 <span class=nb>time</span> requiring <span class=m>0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 2: 0.066400 <span class=nb>time</span> requiring <span class=m>0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 7: 0.091136 <span class=nb>time</span> requiring <span class=m>1433120</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 5: 0.129024 <span class=nb>time</span> requiring <span class=m>4656640</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 4: 0.145408 <span class=nb>time</span> requiring <span class=m>2450080</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class=k>for</span> Algo 6: -1.000000 <span class=nb>time</span> requiring <span class=m>0</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class=k>for</span> Algo 3: -1.000000 <span class=nb>time</span> requiring <span class=m>0</span> memory
Resulting weights from Softmax:
0.0000000 0.0000000 0.0000000 0.9999288 0.0000000 0.0000711 0.0000000 0.0000000 0.0000000 0.0000000
Loading image data/five_28x28.pgm
Performing forward propagation ...
Resulting weights from Softmax:
0.0000000 0.0000008 0.0000000 0.0000002 0.0000000 0.9999820 0.0000154 0.0000000 0.0000012 0.0000006

Result of classification: <span class=m>1</span> <span class=m>3</span> <span class=m>5</span>

Test passed!

Testing half precision <span class=o>(</span>math in single precision<span class=o>)</span>
Loading binary file data/conv1.bin
Loading binary file data/conv1.bias.bin
Loading binary file data/conv2.bin
Loading binary file data/conv2.bias.bin
Loading binary file data/ip1.bin
Loading binary file data/ip1.bias.bin
Loading binary file data/ip2.bin
Loading binary file data/ip2.bias.bin
Loading image data/one_28x28.pgm
Performing forward propagation ...
Testing cudnnGetConvolutionForwardAlgorithm_v7 ...
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 1: -1.000000 <span class=nb>time</span> requiring <span class=m>0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 0: -1.000000 <span class=nb>time</span> requiring <span class=m>0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 2: -1.000000 <span class=nb>time</span> requiring <span class=m>0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 5: -1.000000 <span class=nb>time</span> requiring <span class=m>178432</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 4: -1.000000 <span class=nb>time</span> requiring <span class=m>184784</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 7: -1.000000 <span class=nb>time</span> requiring <span class=m>2057744</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class=k>for</span> Algo 6: -1.000000 <span class=nb>time</span> requiring <span class=m>0</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class=k>for</span> Algo 3: -1.000000 <span class=nb>time</span> requiring <span class=m>0</span> memory
Testing cudnnFindConvolutionForwardAlgorithm ...
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 2: 0.009216 <span class=nb>time</span> requiring <span class=m>0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 0: 0.010240 <span class=nb>time</span> requiring <span class=m>0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 1: 0.017408 <span class=nb>time</span> requiring <span class=m>0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 7: 0.068512 <span class=nb>time</span> requiring <span class=m>2057744</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 5: 0.070784 <span class=nb>time</span> requiring <span class=m>178432</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 4: 0.103424 <span class=nb>time</span> requiring <span class=m>184784</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class=k>for</span> Algo 6: -1.000000 <span class=nb>time</span> requiring <span class=m>0</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class=k>for</span> Algo 3: -1.000000 <span class=nb>time</span> requiring <span class=m>0</span> memory
Testing cudnnGetConvolutionForwardAlgorithm_v7 ...
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 7: -1.000000 <span class=nb>time</span> requiring <span class=m>1433120</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 1: -1.000000 <span class=nb>time</span> requiring <span class=m>1536</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 0: -1.000000 <span class=nb>time</span> requiring <span class=m>0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 2: -1.000000 <span class=nb>time</span> requiring <span class=m>64000</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 5: -1.000000 <span class=nb>time</span> requiring <span class=m>4656640</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 4: -1.000000 <span class=nb>time</span> requiring <span class=m>2450080</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class=k>for</span> Algo 6: -1.000000 <span class=nb>time</span> requiring <span class=m>0</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class=k>for</span> Algo 3: -1.000000 <span class=nb>time</span> requiring <span class=m>0</span> memory
Testing cudnnFindConvolutionForwardAlgorithm ...
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 0: 0.079872 <span class=nb>time</span> requiring <span class=m>0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 1: 0.079872 <span class=nb>time</span> requiring <span class=m>1536</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 2: 0.101376 <span class=nb>time</span> requiring <span class=m>64000</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 5: 0.101440 <span class=nb>time</span> requiring <span class=m>4656640</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 4: 0.105472 <span class=nb>time</span> requiring <span class=m>2450080</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 7: 0.126976 <span class=nb>time</span> requiring <span class=m>1433120</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class=k>for</span> Algo 6: -1.000000 <span class=nb>time</span> requiring <span class=m>0</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class=k>for</span> Algo 3: -1.000000 <span class=nb>time</span> requiring <span class=m>0</span> memory
Resulting weights from Softmax:
0.0000001 1.0000000 0.0000001 0.0000000 0.0000563 0.0000001 0.0000012 0.0000017 0.0000010 0.0000001
Loading image data/three_28x28.pgm
Performing forward propagation ...
Testing cudnnGetConvolutionForwardAlgorithm_v7 ...
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 1: -1.000000 <span class=nb>time</span> requiring <span class=m>0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 0: -1.000000 <span class=nb>time</span> requiring <span class=m>0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 2: -1.000000 <span class=nb>time</span> requiring <span class=m>0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 5: -1.000000 <span class=nb>time</span> requiring <span class=m>178432</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 4: -1.000000 <span class=nb>time</span> requiring <span class=m>184784</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 7: -1.000000 <span class=nb>time</span> requiring <span class=m>2057744</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class=k>for</span> Algo 6: -1.000000 <span class=nb>time</span> requiring <span class=m>0</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class=k>for</span> Algo 3: -1.000000 <span class=nb>time</span> requiring <span class=m>0</span> memory
Testing cudnnFindConvolutionForwardAlgorithm ...
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 2: 0.010240 <span class=nb>time</span> requiring <span class=m>0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 0: 0.023552 <span class=nb>time</span> requiring <span class=m>0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 1: 0.064480 <span class=nb>time</span> requiring <span class=m>0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 7: 0.087008 <span class=nb>time</span> requiring <span class=m>2057744</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 4: 0.103424 <span class=nb>time</span> requiring <span class=m>184784</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 5: 0.138144 <span class=nb>time</span> requiring <span class=m>178432</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class=k>for</span> Algo 6: -1.000000 <span class=nb>time</span> requiring <span class=m>0</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class=k>for</span> Algo 3: -1.000000 <span class=nb>time</span> requiring <span class=m>0</span> memory
Testing cudnnGetConvolutionForwardAlgorithm_v7 ...
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 7: -1.000000 <span class=nb>time</span> requiring <span class=m>1433120</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 1: -1.000000 <span class=nb>time</span> requiring <span class=m>1536</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 0: -1.000000 <span class=nb>time</span> requiring <span class=m>0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 2: -1.000000 <span class=nb>time</span> requiring <span class=m>64000</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 5: -1.000000 <span class=nb>time</span> requiring <span class=m>4656640</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 4: -1.000000 <span class=nb>time</span> requiring <span class=m>2450080</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class=k>for</span> Algo 6: -1.000000 <span class=nb>time</span> requiring <span class=m>0</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class=k>for</span> Algo 3: -1.000000 <span class=nb>time</span> requiring <span class=m>0</span> memory
Testing cudnnFindConvolutionForwardAlgorithm ...
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 0: 0.083904 <span class=nb>time</span> requiring <span class=m>0</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 7: 0.090112 <span class=nb>time</span> requiring <span class=m>1433120</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 5: 0.094976 <span class=nb>time</span> requiring <span class=m>4656640</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 4: 0.095232 <span class=nb>time</span> requiring <span class=m>2450080</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 2: 0.111616 <span class=nb>time</span> requiring <span class=m>64000</span> memory
^^^^ CUDNN_STATUS_SUCCESS <span class=k>for</span> Algo 1: 0.186368 <span class=nb>time</span> requiring <span class=m>1536</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class=k>for</span> Algo 6: -1.000000 <span class=nb>time</span> requiring <span class=m>0</span> memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED <span class=k>for</span> Algo 3: -1.000000 <span class=nb>time</span> requiring <span class=m>0</span> memory
Resulting weights from Softmax:
0.0000000 0.0000000 0.0000000 1.0000000 0.0000000 0.0000714 0.0000000 0.0000000 0.0000000 0.0000000
Loading image data/five_28x28.pgm
Performing forward propagation ...
Resulting weights from Softmax:
0.0000000 0.0000008 0.0000000 0.0000002 0.0000000 1.0000000 0.0000154 0.0000000 0.0000012 0.0000006

Result of classification: <span class=m>1</span> <span class=m>3</span> <span class=m>5</span>

Test passed!
</code></pre></div></details>
<h2 id=3-installing-nvidia-tensorrt-optional>3. Installing NVIDIA TensorRT (Optional)<a hidden class=anchor aria-hidden=true href=#3-installing-nvidia-tensorrt-optional>Â¶</a></h2>
<p><strong>NVIDIA TensorRT</strong> is a C++ library that facilitates high-performance inference NVIDIA graphic processing units (GPUs). <strong>TensorRT</strong> takes a trained network, which consists of a network definition and a set of trained parameters, and produces a highly optimized runtime engine that performs inference for that network.</p>
<p><strong>TensorRT</strong> provides APIs via C++ and Python that help to express deep learning model via the Network Definition API or load a pre-defined model via the ONNX parser that allow <strong>TensorRT</strong> to optimize and run them on the NVIDIA GPU. <strong>TensorRT</strong> also include optional high speed mixed precision capabilities with difference NVIDIA architectures.</p>
<p>You can download <strong>TensorRT</strong> local repo that matches the Ubuntu version and CPU architecture of your machine from <a href=https://developer.nvidia.com/tensorrt/download/10x><strong>here</strong></a>.</p>
<p>Install the Debian local package.</p>
<div class="highlight command"><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=nv>os</span><span class=o>=</span><span class=s2>&#34;ubuntuxx04&#34;</span>
<span class=nv>tag</span><span class=o>=</span><span class=s2>&#34;10.x.x-cuda-x.x&#34;</span>
sudo dpkg -i nv-tensorrt-local-repo-<span class=si>${</span><span class=nv>os</span><span class=si>}</span>-<span class=si>${</span><span class=nv>tag</span><span class=si>}</span>_1.0-1_amd64.deb
sudo cp /var/nv-tensorrt-local-repo-<span class=si>${</span><span class=nv>os</span><span class=si>}</span>-<span class=si>${</span><span class=nv>tag</span><span class=si>}</span>/*-keyring.gpg /usr/share/keyrings/
sudo apt-get update
</code></pre></div><p>Replace <code>ubuntuxx04</code>, <code>10.x.x</code> and <code>cuda-x.x</code> with your specific OS version, TensorRT version, and CUDA version. The full <strong>TensorRT</strong> C++ and Python runtimes can be installed by:</p>
<div class="highlight command"><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>sudo apt install -y tensorrt
</code></pre></div><details class="admonition info"><summary class=admonition-title>Verify TensorRT Installation</summary>
<p>To verify the installation, run the command:</p>
<div class="highlight command"><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>dpkg-query -W tensorrt
</code></pre></div><p>The ouput should be as following:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>tensorrt        10.0.0.6-1+cuda12.4
</code></pre></div><p>The samples is located in <strong>/usr/src/tensorrt</strong> directory, copy the directory to <code>$HOME</code> and set the permission.</p>
<div class="highlight command"><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>sudo cp -r tensorrt/ <span class=nv>$HOME</span>
sudo chmod -R a+rwx <span class=nv>$HOME</span>/tensorrt
</code></pre></div><p>For the <code>sampleOnnxMNIST</code> sample, compile the sample and run the program.</p>
<div class="highlight command"><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=nb>cd</span> <span class=nv>$HOME</span>/tensorrt/samples/sampleOnnxMNIST
make clean <span class=o>&amp;&amp;</span> make
<span class=nb>cd</span> ../../bin
./sampleOnnxMNIST
</code></pre></div><p>The output should be as following:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=o>&amp;&amp;&amp;&amp;</span> RUNNING TensorRT.sample_onnx_mnist <span class=o>[</span>TensorRT v100000<span class=o>]</span> <span class=c1># ./sample_onnx_mnist</span>
<span class=o>[</span>04/15/2024-18:24:17<span class=o>]</span> <span class=o>[</span>I<span class=o>]</span> Building and running a GPU inference engine <span class=k>for</span> Onnx MNIST
<span class=o>[</span>04/15/2024-18:24:17<span class=o>]</span> <span class=o>[</span>I<span class=o>]</span> <span class=o>[</span>TRT<span class=o>]</span> <span class=o>[</span>MemUsageChange<span class=o>]</span> Init CUDA: CPU +19, GPU +0, now: CPU 21, GPU <span class=m>1091</span> <span class=o>(</span>MiB<span class=o>)</span>
<span class=o>[</span>04/15/2024-18:24:19<span class=o>]</span> <span class=o>[</span>I<span class=o>]</span> <span class=o>[</span>TRT<span class=o>]</span> <span class=o>[</span>MemUsageChange<span class=o>]</span> Init builder kernel library: CPU +1765, GPU +312, now: CPU 1922, GPU <span class=m>1403</span> <span class=o>(</span>MiB<span class=o>)</span>
<span class=o>[</span>04/15/2024-18:24:19<span class=o>]</span> <span class=o>[</span>I<span class=o>]</span> <span class=o>[</span>TRT<span class=o>]</span> ----------------------------------------------------------------
<span class=o>[</span>04/15/2024-18:24:19<span class=o>]</span> <span class=o>[</span>I<span class=o>]</span> <span class=o>[</span>TRT<span class=o>]</span> Input filename:   ../data/mnist/mnist.onnx
<span class=o>[</span>04/15/2024-18:24:19<span class=o>]</span> <span class=o>[</span>I<span class=o>]</span> <span class=o>[</span>TRT<span class=o>]</span> ONNX IR version:  0.0.3
<span class=o>[</span>04/15/2024-18:24:19<span class=o>]</span> <span class=o>[</span>I<span class=o>]</span> <span class=o>[</span>TRT<span class=o>]</span> Opset version:    <span class=m>8</span>
<span class=o>[</span>04/15/2024-18:24:19<span class=o>]</span> <span class=o>[</span>I<span class=o>]</span> <span class=o>[</span>TRT<span class=o>]</span> Producer name:    CNTK
<span class=o>[</span>04/15/2024-18:24:19<span class=o>]</span> <span class=o>[</span>I<span class=o>]</span> <span class=o>[</span>TRT<span class=o>]</span> Producer version: 2.5.1
<span class=o>[</span>04/15/2024-18:24:19<span class=o>]</span> <span class=o>[</span>I<span class=o>]</span> <span class=o>[</span>TRT<span class=o>]</span> Domain:           ai.cntk
<span class=o>[</span>04/15/2024-18:24:19<span class=o>]</span> <span class=o>[</span>I<span class=o>]</span> <span class=o>[</span>TRT<span class=o>]</span> Model version:    <span class=m>1</span>
<span class=o>[</span>04/15/2024-18:24:19<span class=o>]</span> <span class=o>[</span>I<span class=o>]</span> <span class=o>[</span>TRT<span class=o>]</span> Doc string:
<span class=o>[</span>04/15/2024-18:24:19<span class=o>]</span> <span class=o>[</span>I<span class=o>]</span> <span class=o>[</span>TRT<span class=o>]</span> ----------------------------------------------------------------
<span class=o>[</span>04/15/2024-18:24:19<span class=o>]</span> <span class=o>[</span>I<span class=o>]</span> <span class=o>[</span>TRT<span class=o>]</span> Local timing cache in use. Profiling results in this builder pass will not be stored.
<span class=o>[</span>04/15/2024-18:24:21<span class=o>]</span> <span class=o>[</span>I<span class=o>]</span> <span class=o>[</span>TRT<span class=o>]</span> Detected <span class=m>1</span> inputs and <span class=m>1</span> output network tensors.
<span class=o>[</span>04/15/2024-18:24:21<span class=o>]</span> <span class=o>[</span>I<span class=o>]</span> <span class=o>[</span>TRT<span class=o>]</span> Total Host Persistent Memory: <span class=m>26272</span>
<span class=o>[</span>04/15/2024-18:24:21<span class=o>]</span> <span class=o>[</span>I<span class=o>]</span> <span class=o>[</span>TRT<span class=o>]</span> Total Device Persistent Memory: <span class=m>0</span>
<span class=o>[</span>04/15/2024-18:24:21<span class=o>]</span> <span class=o>[</span>I<span class=o>]</span> <span class=o>[</span>TRT<span class=o>]</span> Total Scratch Memory: <span class=m>0</span>
<span class=o>[</span>04/15/2024-18:24:21<span class=o>]</span> <span class=o>[</span>I<span class=o>]</span> <span class=o>[</span>TRT<span class=o>]</span> <span class=o>[</span>BlockAssignment<span class=o>]</span> Started assigning block shifts. This will take <span class=m>6</span> steps to complete.
<span class=o>[</span>04/15/2024-18:24:21<span class=o>]</span> <span class=o>[</span>I<span class=o>]</span> <span class=o>[</span>TRT<span class=o>]</span> <span class=o>[</span>BlockAssignment<span class=o>]</span> Algorithm ShiftNTopDown took 0.02021ms to assign <span class=m>3</span> blocks to <span class=m>6</span> nodes requiring <span class=m>32256</span> bytes.
<span class=o>[</span>04/15/2024-18:24:21<span class=o>]</span> <span class=o>[</span>I<span class=o>]</span> <span class=o>[</span>TRT<span class=o>]</span> Total Activation Memory: <span class=m>31744</span>
<span class=o>[</span>04/15/2024-18:24:21<span class=o>]</span> <span class=o>[</span>I<span class=o>]</span> <span class=o>[</span>TRT<span class=o>]</span> Total Weights Memory: <span class=m>26152</span>
<span class=o>[</span>04/15/2024-18:24:21<span class=o>]</span> <span class=o>[</span>I<span class=o>]</span> <span class=o>[</span>TRT<span class=o>]</span> Engine generation completed in 1.46118 seconds.
<span class=o>[</span>04/15/2024-18:24:21<span class=o>]</span> <span class=o>[</span>I<span class=o>]</span> <span class=o>[</span>TRT<span class=o>]</span> <span class=o>[</span>MemUsageStats<span class=o>]</span> Peak memory usage of TRT CPU/GPU memory allocators: CPU <span class=m>0</span> MiB, GPU <span class=m>5</span> MiB
<span class=o>[</span>04/15/2024-18:24:21<span class=o>]</span> <span class=o>[</span>I<span class=o>]</span> <span class=o>[</span>TRT<span class=o>]</span> <span class=o>[</span>MemUsageStats<span class=o>]</span> Peak memory usage during Engine building and serialization: CPU: <span class=m>3052</span> MiB
<span class=o>[</span>04/15/2024-18:24:21<span class=o>]</span> <span class=o>[</span>I<span class=o>]</span> <span class=o>[</span>TRT<span class=o>]</span> Loaded engine size: <span class=m>0</span> MiB
<span class=o>[</span>04/15/2024-18:24:21<span class=o>]</span> <span class=o>[</span>I<span class=o>]</span> <span class=o>[</span>TRT<span class=o>]</span> <span class=o>[</span>MemUsageChange<span class=o>]</span> TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU <span class=m>0</span> <span class=o>(</span>MiB<span class=o>)</span>
<span class=o>[</span>04/15/2024-18:24:21<span class=o>]</span> <span class=o>[</span>I<span class=o>]</span> Input:
<span class=o>[</span>04/15/2024-18:24:21<span class=o>]</span> <span class=o>[</span>I<span class=o>]</span> @@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@%<span class=o>=</span><span class=c1>#@@@@@%=%@@@@@@@@@@</span>
@@@@@@@           %@@@@@@@@@
@@@@@@@           %@@@@@@@@@
@@@@@@@#:-#-.     %@@@@@@@@@
@@@@@@@@@@@@#    <span class=c1>#@@@@@@@@@@</span>
@@@@@@@@@@@@@    <span class=c1>#@@@@@@@@@@</span>
@@@@@@@@@@@@@:  :@@@@@@@@@@@
@@@@@@@@@%+<span class=o>==</span>   *%%%%%%%%%@@
@@@@@@@@%                 -@
@@@@@@@@@#+.          .:-%@@
@@@@@@@@@@@*     :-###@@@@@@
@@@@@@@@@@@*   -%@@@@@@@@@@@
@@@@@@@@@@@*   *@@@@@@@@@@@@
@@@@@@@@@@@*   @@@@@@@@@@@@@
@@@@@@@@@@@*   <span class=c1>#@@@@@@@@@@@@</span>
@@@@@@@@@@@*   *@@@@@@@@@@@@
@@@@@@@@@@@*   *@@@@@@@@@@@@
@@@@@@@@@@@*   @@@@@@@@@@@@@
@@@@@@@@@@@*   @@@@@@@@@@@@@
@@@@@@@@@@@@+<span class=o>=</span><span class=c1>#@@@@@@@@@@@@@</span>
@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@

<span class=o>[</span>04/15/2024-18:24:21<span class=o>]</span> <span class=o>[</span>I<span class=o>]</span> Output:
<span class=o>[</span>04/15/2024-18:24:21<span class=o>]</span> <span class=o>[</span>I<span class=o>]</span>  Prob <span class=m>0</span>  0.0000 Class 0:
<span class=o>[</span>04/15/2024-18:24:21<span class=o>]</span> <span class=o>[</span>I<span class=o>]</span>  Prob <span class=m>1</span>  0.0000 Class 1:
<span class=o>[</span>04/15/2024-18:24:21<span class=o>]</span> <span class=o>[</span>I<span class=o>]</span>  Prob <span class=m>2</span>  0.0000 Class 2:
<span class=o>[</span>04/15/2024-18:24:21<span class=o>]</span> <span class=o>[</span>I<span class=o>]</span>  Prob <span class=m>3</span>  0.0000 Class 3:
<span class=o>[</span>04/15/2024-18:24:21<span class=o>]</span> <span class=o>[</span>I<span class=o>]</span>  Prob <span class=m>4</span>  0.0000 Class 4:
<span class=o>[</span>04/15/2024-18:24:21<span class=o>]</span> <span class=o>[</span>I<span class=o>]</span>  Prob <span class=m>5</span>  0.0000 Class 5:
<span class=o>[</span>04/15/2024-18:24:21<span class=o>]</span> <span class=o>[</span>I<span class=o>]</span>  Prob <span class=m>6</span>  0.0000 Class 6:
<span class=o>[</span>04/15/2024-18:24:21<span class=o>]</span> <span class=o>[</span>I<span class=o>]</span>  Prob <span class=m>7</span>  1.0000 Class 7: **********
<span class=o>[</span>04/15/2024-18:24:21<span class=o>]</span> <span class=o>[</span>I<span class=o>]</span>  Prob <span class=m>8</span>  0.0000 Class 8:
<span class=o>[</span>04/15/2024-18:24:21<span class=o>]</span> <span class=o>[</span>I<span class=o>]</span>  Prob <span class=m>9</span>  0.0000 Class 9:
<span class=o>[</span>04/15/2024-18:24:21<span class=o>]</span> <span class=o>[</span>I<span class=o>]</span>
<span class=o>&amp;&amp;&amp;&amp;</span> PASSED TensorRT.sample_onnx_mnist <span class=o>[</span>TensorRT v100000<span class=o>]</span> <span class=c1># ./sample_onnx_mnist</span>
</code></pre></div></details>
<p>If you are using <strong>TensorRT Python API</strong>, make sure <strong>CUDA-Python</strong> is installed in your system or your virtual environment.</p>
<div class="highlight command"><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>mamba create -n tensorrt_env pip wheel
mamba activate tensorrt_env
</code></pre></div><ul>
<li>Installing from <strong>PyPI</strong>, <strong>TensorRT</strong> runtime wheels</li>
</ul>
<div class="highlight command"><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>pip install cuda-python
pip install --pre --upgrade tensorrt
pip install --pre --upgrade tensorrt_lean tensorrt_dispatch
</code></pre></div><p>To verify the installation is working, use python code.</p>
<div class="highlight command"><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=kn>import</span> <span class=nn>tensorrt</span>
<span class=nb>print</span><span class=p>(</span><span class=n>tensorrt</span><span class=o>.</span><span class=n>__version__</span><span class=p>)</span>
<span class=k>assert</span><span class=p>(</span><span class=n>tensorrt</span><span class=o>.</span><span class=n>Builder</span><span class=p>(</span><span class=n>tensorrt</span><span class=o>.</span><span class=n>Logger</span><span class=p>()))</span>

<span class=kn>import</span> <span class=nn>tensorrt_lean</span> <span class=k>as</span> <span class=nn>trtl</span>
<span class=nb>print</span><span class=p>(</span><span class=n>trtl</span><span class=o>.</span><span class=n>__version__</span><span class=p>)</span>
<span class=k>assert</span> <span class=n>trtl</span><span class=o>.</span><span class=n>Runtime</span><span class=p>(</span><span class=n>trt</span><span class=o>.</span><span class=n>Logger</span><span class=p>())</span>

<span class=kn>import</span> <span class=nn>tensorrt_dispatch</span> <span class=k>as</span> <span class=nn>trtd</span>
<span class=nb>print</span><span class=p>(</span><span class=n>trtd</span><span class=o>.</span><span class=n>__version__</span><span class=p>)</span>
<span class=k>assert</span> <span class=n>trtd</span><span class=o>.</span><span class=n>Runtime</span><span class=p>(</span><span class=n>trt</span><span class=o>.</span><span class=n>Logger</span><span class=p>())</span>
</code></pre></div><p>The <code>tensorrt/samples</code> directory also provides the samples that utilize with the <strong>TensorRT</strong>, you can follow instructions from <code>README.md</code> to run samples.</p>
<h2 id=3-set-up-python-environment-using-mambaforge>3. Set up Python environment using Mambaforge<a hidden class=anchor aria-hidden=true href=#3-set-up-python-environment-using-mambaforge>Â¶</a></h2>
<p>Python environment allows to manage separatelly different installations of Python and modules. It is useful when you have many projects running different version of Python and modules. It also help to manage installed modules for publish or reproduce.</p>
<p>There are different ways to create a python virtual environment, including built-in <code>venv</code>, <code>Conda</code> and <code>Anaconda</code>.</p>
<p><code>Conda</code> is a packaging tool and installer that aims to handle library dependencies outside of the Python packages as well as the Python packages themselves. For non preinstalled package manager, <code>Miniconda</code>, an installation of Conda, will be a good option.</p>
<p><code>Anacond</code>a is an installation of Conda that comes pre-loaded with a bunch of packages for scientific computing, i.e., <code>numpy</code>, <code>matplotlib</code>, <code>scipy</code>, etc. It also comes with IDE, Jupyter notebooks out of the box. This is helpful for beginers, but doesn&rsquo;t give much control.</p>
<p><code>Mamba</code> is a package manager which can be used with Python. Unlike <code>Conda</code>, it uses the C/C++ implementation to speed up the package installation. Read more about <code>mamba</code> in <a href=https://focalplane.biologists.com/2022/12/08/managing-scientific-python-environments-using-conda-mamba-and-friends/>here</a>. To install <code>mamba</code>, access <a href=https://github.com/conda-forge/miniforge>its repo</a> and pick the Mabaforge installer for your operating system.</p>
<p>Remember to run <code>conda init</code> at the end of your installation in your bash to activate the <code>mamba</code> command.</p>
<p><img loading=lazy src=/posts/setitng-up-nvidia-cuda-on-wsl2/mambaforge-install.png type alt="Mambaforge install"></p>
<h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>Â¶</a></h2>
<p>By following these steps and installing the required software, you will have an CUDA-ready environment in WSL for further machine learnin/deep learning applications. This environment will provide the necessary tools and libraries for GPU-accelerated computing and Python package management.</p>
<h2 id=reference>Reference<a hidden class=anchor aria-hidden=true href=#reference>Â¶</a></h2>
<ul>
<li><a href=https://medium.com/geekculture/install-cuda-and-cudnn-on-windows-linux-52d1501a8805>Install CUDA and CUDNN on Windows & Linux</a>.</li>
<li><a href=https://docs.nvidia.com/cuda/wsl-user-guide/index.html>CUDA on WSL User Guide</a>.</li>
<li><a href=https://visualstudio.microsoft.com/free-developer-offers/>Machine learning environment build: WLS2+Ubuntu+CUDA+cuDNN</a>.</li>
<li><a href=https://biapol.github.io/blog/mara_lampert/getting_started_with_mambaforge_and_python/readme.html>Getting started with Mambaforge and Python</a>.</li>
<li><a href=https://ross-dobson.github.io/posts/2021/01/setting-up-python-virtual-environments-with-mambaforge/>Tutorial: Setting up Python enviroments with Mambaforge</a>.</li>
</ul>
</div>
<footer class=post-footer>
<nav class=paginav>
<a class=prev href=http://visionbike.github.io/posts/build-opencv-cuda-on-windows/>
<span class=title>
<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-left" style="user-select:text"><line x1="19" y1="12" x2="5" y2="12" style="user-select:text"/><polyline points="12 19 5 12 12 5" style="user-select:text"/></svg>&nbsp;Prev Page</span>
<br>
<span>Build OpenCV with CUDA on Windows 11</span>
</a>
<a class=next href=http://visionbike.github.io/posts/setting-up-nvidia-cuda-on-windows-11/>
<span class=title>Next Page&nbsp;<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-right" style="user-select:text"><line x1="5" y1="12" x2="19" y2="12" style="user-select:text"/><polyline points="12 5 19 12 12 19" style="user-select:text"/></svg>
</span>
<br>
<span>Setting Up NVIDIA CUDA on Windows 11</span>
</a>
</nav>
</footer>
<div class=comments-separator></div>
</article>
</main>
<footer class=footer>
<span>&copy; 2024 <a href=http://visionbike.github.io/>Visionbike - Personal Blog of CV | DSP | ML notes</a></span><span style=display:inline-block;margin-left:1em>
<a href=https://creativecommons.org/licenses/by-sa/4.0/>CC BY-SA</a>
</span>
<span style=display:inline-block;margin-left:1em>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
    <a href=https://github.com/reorx/hugo-PaperModX/ rel=noopener target=_blank>PaperModX</a>
</span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script>(function(){const b='1'=='1';if(b)return;let a=document.getElementById("theme-toggle");a.removeEventListener('click',toggleThemeListener),a.addEventListener('click',toggleThemeListener)})()</script>
<script>(function(){let a=document.getElementById('menu');a&&(a.scrollLeft=localStorage.getItem("menu-scroll-position"),a.onscroll=function(){localStorage.setItem("menu-scroll-position",a.scrollLeft)});const b=''=='1',c='1'=='1';if(window.matchMedia('(prefers-reduced-motion: reduce)').matches||b||c)return;document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})})()</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>if(window.scrollListeners)for(const a of scrollListeners)window.removeEventListener('scroll',a);window.scrollListeners=[]</script>
<script src=/js/medium-zoom.min.js data-no-instant></script>
<script>document.querySelectorAll('pre > code').forEach(b=>{const c=b.parentNode.parentNode,a=document.createElement('button');a.classList.add('copy-code'),a.innerText='copy';function d(){a.innerText='copied!',setTimeout(()=>{a.innerText='copy'},2e3)}a.addEventListener('click',e=>{if('clipboard'in navigator){navigator.clipboard.writeText(b.textContent),d();return}const a=document.createRange();a.selectNodeContents(b);const c=window.getSelection();c.removeAllRanges(),c.addRange(a);try{document.execCommand('copy'),d()}catch(a){}c.removeRange(a)}),c.classList.contains("highlight")?c.appendChild(a):c.parentNode.firstChild==c||(b.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?b.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(a):b.parentNode.appendChild(a))})</script>
<script>(function(){const h='1'=='1';if(!h)return;if(!document.querySelector('.toc')){console.log('no toc found, ignore toc scroll');return}const i=window.scrollListeners,c=document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id]'),d='active';let a=c[0];e(a).classList.add(d);const g=()=>{const b=[];for(const a of c)if(j(a)<5)b.push(a);else break;b.length>0?newActiveHeading=b[b.length-1]:newActiveHeading=c[0],a!=newActiveHeading&&(e(a).classList.remove(d),a=newActiveHeading,e(a).classList.add(d))};let b=null;const f=()=>{b!==null&&clearTimeout(b),b=setTimeout(g,50)};window.addEventListener('scroll',f,!1),i.push(f);function e(a){const b=encodeURI(a.getAttribute('id')).toLowerCase();return document.querySelector(`.toc ul li a[href="#${b}"]`)}function j(a){if(!a.getClientRects().length)return 0;let b=a.getBoundingClientRect();return b.top}})()</script>
<script>mediumZoom('.entry-cover img'),mediumZoom('.post-content img:not([no-zoom])')</script>
<script src=/js/instantclick.min.js data-no-instant></script>
<script data-no-instant>InstantClick.init()</script>
</body>
</html>