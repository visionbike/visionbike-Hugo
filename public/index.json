[{"content":"Since pre-built OpenCV binaries do not include CUDA modules, this post is a tutorial for building OpenCV with CUDA on Windows 11. The obvious dvantage of OpenCV CUDA is boosting performance of most functions, you can find evidence here.\n1. Prerequisite There are a couples of softwares or libraries having been downloaded and installed before getting started:\n  Install the Visual Studio Community 2022 and select Desktop development with C++ workload.\n  Download the sources for OpenCV from GitHub by cloning the repositories (opencv and opencv_contrib).\n  After downloading, you can indicate the OpenCV\u0026rsquo;s version you want to. For the current version, you can run the following command in Command Prompt at OpenCV\u0026rsquo;s repositories:\ngit checkout tags/4.8.0  Install the latest stable version (not release candidate -rc) of CMake.\n  Install the latest version of NVIDIA CUDA Toolkit and add PATH. You can follow this tutorial.\n  The latest CUDA Toolkit version\nAt the time writting this post, the latest NVIDIA CUDA Toolkit version 12.2 still makes somes error when building with OpenCV 4.8.0, like error C2666: 'operator !=': overloaded functions have similar conversions. Therefore, if meeting such problem, try installing older version.\n  Register an account and download the latest verson of NVIDIA DNN CUDA backend for the version of CUDA. Extract the downloaded .zip file and copy bin, include and lib directories to your CUDA installation directory, i.e., C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\vxx.x.\n  Register an account and download the latest version of NVIDIA VIdeo Codec SDK. Extract the downloaded .zip file and copy the contents inside Interface and Lib to include and lib directories inside the CUDA installation directory.\n  Optional - Download and install the latest version of Gstreamer.\n  Download and install the lastest version of mambaforge to call OpenCV CUDA routines from python.\n  2. Create virtual environtment from Mambaforge To bind OpenCV for python3 without any conflit package installation, you should create an new virtual environment for the installation.\nmamba create -n opencv-cuda python=3.10 Activate the created environment and install numpy package.\nmamba activate opencv-cuda mamaba install numpy 3. Build OpenCV with CMake Preparation Create a build folder with your OpenCV extracted folders.\nEdit the prioeiry of Python3 installation in OpenCVDetectPython.cmake file inside opencv-x.x.x/cmake folder.\nBuild GUI Build Configuration Open Cmake GUI and provide the paths to the OpenCV and target build folders.\nHit Configure and select x64 for the Optional platform for generator, then hit finish to start the configuration.\nOnce the configuration is done, edit the following parameters:\n   Parameter Value     CMAKE_INSTALL_PREFIX path of opencv installation   ENABLE_FAST_MATH âœ…   WITH_CUDA âœ…   BUILD_opencv_world âœ…   BUILD_opencv_python3 âœ…   OPENCV_DNN_CUDA âœ…   OPENCV_EXTRA_MODULES_PATH path of modules directory in opencv_contrib-x.x.x   OPENCV_PYTHON3_VERSION âœ…   PYTHON3_EXECUTABLE path of python3 executable in virtual env, i.e., C:/Users/ntthi/mambaforge/envs/opencv-cuda/python.exe   PYTHON3_INCLUDE_DIR path of include folder in the virtual env, i.e., C:/Users/ntthi/mambaforge/envs/opencv-cuda/include   PYTHON3_LIBRARY path of .lib file in the virtual env, i.e., C:/Users/ntthi/mambaforge/envs/opencv-cuda/libs/python310.lib   PYTHON3_NUMPY_INCLUDE_DIRS path of numpy in the virtual env, i.e., C:/Users/ntthi/mambaforge/envs/opencv-cuda/Lib/site-pakages/numpy/core/include   PYTHON3_PACKAGES_PATH path of site-packages in the virtual env, i.e., C:/Users/ntthi/mambaforge/envs/opencv-cuda/Lib/site-pakages    Note that the path separator hase to be \u0026ldquo;/\u0026rdquo; , not \u0026ldquo;\u0026quot;.\nHit Configure again again and check edit more parameters:\n   Parameter Value     CUDA_FAST_MATH âœ…   CUDA_ARCH_BIN version of computing capability, i.e., 8.6   WITH_CUBLAS âœ…   WITH_CUDNN âœ…   WITH_CUFFT âœ…    The CUDA_ARCH_BIN corresponding to your GPU is the value found in the left column of the GPU support table. For instance, \u0026ldquo;8.6\u0026rdquo; fir the RTX 3070 Ti.\nIf you do not want to create shared lib and make sure the opencv python libraries is installed, edit the following parameters:\n   Parameter Value     BUILD_SHARED_LIBS ðŸ”³   OPENCV_FORCE_PYTHON_LIBS âœ…    Hit Configure at the last time and then hit Generate.\nBuild the project with Visual Studio Open project OpenCV.sln created in the build folder. Go to Tools \u0026gt; Options\u0026hellip;, then uncheck the last parameter in Projects and Solutions \u0026gt; Web Projects.\n\nThis setting may help to prevent the ImportError: DLL load failed while importing cv2: The specified module could not be found. error.\nTo build the OpenCV project, change Debug mode to Release. In the solution explorer expand CMakeTargets, right-click ALL_BUILD and select Build. This will take about an hour.\nThen repeat the step for INSTALL (below ALL_BUILD). Check for error in the two building steps. If everything is fine, you are done.\nCheck Installation and Troubleshooting To verify the Python installation, activate the virtual environment for OpenCV install and try this code:\nimport cv2 print(cv2.__version__) print(cv2.cuda.getCudaEnabledDeviceCount()) If it works, congratulations you are good to go!\nIf you meets the problem ImportError: DLL load failed while importing cv2: The specified module could not be found., it may lack the library\u0026rsquo;s binaries. One solution is to edit config.py in C:/Users/ntthi/mambaforge/envs/opencv-cuda/Lib/site-packages/cv2.\nimport os BINARIES_PATHS = [ os.path.join(\u0026#39;C:/opencv-cuda-4.8.0\u0026#39;, \u0026#39;x64/vc17/bin\u0026#39;), os.path.join(os.getenv(\u0026#39;CUDA_PATH\u0026#39;, \u0026#39;C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.1\u0026#39;), \u0026#39;bin\u0026#39;), os.path.join(\u0026#39;C:/gstreamer/1.0/msvc_x86_64\u0026#39;, \u0026#39;bin\u0026#39;), ] + BINARIES_PATHS These binary paths are from installed OpenCV, CUDA and Gstreamer (if installed).\nFor other bugs and problems, I refer you to the chrismeunier and Bowley\u0026rsquo;s troubleshooting tutorial.\nReference  Build OpenCV (including Python) with CUDA on Windows. OpenCV CUDA installation.  ","permalink":"http://visionbike.github.io/posts/build-opencv-cuda-on-windows/","summary":"Since pre-built OpenCV binaries do not include CUDA modules, this post is a tutorial for building OpenCV with CUDA on Windows 11. The obvious dvantage of OpenCV CUDA is boosting performance of most functions, you can find evidence here.\n1. Prerequisite There are a couples of softwares or libraries having been downloaded and installed before getting started:\n  Install the Visual Studio Community 2022 and select Desktop development with C++ workload.","title":"Build OpenCV with CUDA on Windows 11"},{"content":"Windows Subsystem for Linux (WSL) is a compatibility layer provided by Microsoft that allows you to run a Linux environment directly on WIndows. It will bring Linux applications, utilities and Bash command-line tool directly on Windows without the overhead of traditional virtual machine or dualboot setup.\n1. Prequisites The Windows must be upgraded to Windows 10 version 2004 and higher or Windows 11 to install WSL by a single command line. Open Command Prompt in Administrator mode, then run the following command:\nwsl --install --no-distribution The command will setup WSL version 2 by default without any Linux distribution. Reboot the machine to apply changes.\nOpen a Command Prompt window and install latest version Ubuntu distro:\nwsl --install -d Ubuntu The installation takes a few minutes and you need to create user name and password.\nDisplay available WSL Linux distros If you want to install another Linux distro, you can run following command to display a list of available WSL Linux distros:\nwsl --list --online  For the first start, we should to install several Linux updates. The process will depend on the speed of the internet, so be patient if it slow!\nsudo apt update -y \u0026amp;\u0026amp; sudo apt upgrade -y Issue: /usr/lib/wsl/lib/libcuda.so.1 is not a symbolic link If your machine has integrated NVIDIA GPU, you may meet the /usr/lib/wsl/lib/libcuda.so.1 is not a symbolic link issue when updating the system.\nIn the WSL Ubuntu side, run following commands:\ncd /usr/lib/wsl/lib sudo rm libcuda.so sudo rm libcuda.so.1 sudo ln -s libcuda.so.1.1 libcuda.so sudo ln -s libcuda.so.1.1 libcuda.so.1 Now reinstall libc-bin package.\nsudo apt reinstall libc-bin In the Windows side, run following comands in Administrator mode:\ncd C:\\Windows\\System32\\lxss\\lib del libcuda.so del libcuda.so.1 mklink libcuda.so libcuda.so.1.1 mklink libcuda.so.1 libcuda.so.1.1 Reboot the machine and run the sudo ldconfig command in WSL Ubuntu side to verify the problem is solved.\n 2. Installing Mamba Mamba is a command-line interfacer (CLI) to manage conda\u0026rsquo;s environemts. For mamba configuration, please refer to conda documentation.\nFor the fresh installation, you can install Miniforge distribution \u0026gt;= Miniforge3-22.3.1.0. Miniforge comes with the popular conda-forge channel preconfigured, but you can modify the configuration to use any channel you like.\nFor Unix-like platforms (MAC OS and Linux), download the installer using curl or wget or your favorite program and run the script\ncurl -L -O \u0026#34;https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\u0026#34; bash Miniforge3-$(uname)-$(uname -m).sh After successful installation, you can use use the mamba commands as described in this user guide.\nWorking with Conda Environment\nThe command using mamba is similar to the conda command.\n Create new environment  mamba create -n \u0026amp;lt;ENV_NAME\u0026amp;gt; python=\u0026amp;lt;VERSION\u0026amp;gt;  Activate an environment  mamba activate \u0026amp;lt;ENV_NAME\u0026amp;gt;  Deactivate environment  mamba deactivate  Delete an environment  mamba env remove -n \u0026amp;lt;ENV_NAME\u0026amp;gt;  Show all created environments  mamba env list Remember to activate an environment first, do not install any packages in your base environment!\n Install python packages  mamba install \u0026amp;lt;PKG_NAME\u0026amp;gt;[=VERSION] [-c \u0026amp;lt;CHANNEL_NAME\u0026amp;gt;] When installing a package, you can optionally indicate specific additional channel that the packages are posted by community. The conda-forge is one of most common additional channels.\n Delete packages  mamba remove \u0026amp;lt;PKG_NAME\u0026amp;gt;  Show all installed packages in the virtual environment  mamba list [-n \u0026amp;lt;ENV_NAME\u0026amp;gt;]  2. Installing NVIDIA CUDA Toolkit for WSL 2 Note that you only need to install NVIDIA Driver for Windows. Do not install any Linux Driver in WSL.\nThe latest NVIDIA Windows GPU Driver will fully support WSL 2. With CUDA support in the driver, existing applications compiled on a Linux system for the same target GPU can run unmodified within the WSL environment. Once NVIDIA Windows GPU Driver is installed in the system, the CUDA driver will be stubbed inside the WSL 2 as libcuda.so. Therefore, you only use a separate CUDA Toolkit for WSL 2 which does not contain the NVIDIA Linux GPU Driver.\nTo get started with running CUDA on WSL 2, you need to instal NVIDIA Driver on Windows 11 with a compatible GeForce or NVIDIA RTX/Quadro card from here. Once a Windows NVIDIA GPU driver is installed on the system, CUDA becomes available within WSL 2. The CUDA driver installed on Windows host will be stubbed inside the WSL 2 as libcuda.so.\nHowever, the NVIDIA CUDA Toolkit for WSL need to be install for further use.\nFirst, remove the old GPG key:\nsudo apt-key del 7fa2af80 Install the NVIDIA CUDA Toolkit for WSL-Ubuntu by following the step in this link\nDownload CUDA Toolkit for WSL 2.\nSome actions must be taken after the installation before the CUDA Toolkit and Driver can be used. The PATH and LD_LIBRARY_PATH variables need to be updated.\nOpen ~/.bashrc by running nano ~/.bashrc, add two lines at the end of the file:\nexport PATH=/usr/local/cuda-12.4/bin${PATH:+:${PATH}} export LD_LIBRARY_PATH=/usr/local/cuda-12.4/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} Then you run source ~/.bashrc for activation changes without reboot.\nVerify CUDA Toolkit Installation You can verify the installation by running the following command:\nnvcc --version If the installation was successful, you should see the CUDA version information displayed.\nnvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2024 NVIDIA Corporation Built on Thu_Mar_28_02:18:24_PDT_2024 Cuda compilation tools, release 12.4, V12.4.131 Build cuda_12.4.r12.4/compiler.34097967_0 It is important to verify that the NVIDIA CUDA Toolkit can find and communicate correctly with CUDA-compatible hardware. To do this, you need to compile and run some sample programs.\nCUDA samples are located in https://github.com/nvidia/cuda-samples. To use the samples, clone the project, build the samples in cuda-samples directory using make command and run them following the instruction on the Github page.\nTo verify a correct configuration of the hardware and software, it is highly recommended that you build and run the deviceQuery sample program.\n./deviceQuery If CUDA is installed and configured correctly, the output should look similar as below:\n./deviceQuery Starting... CUDA Device Query (Runtime API) version (CUDART static linking) Detected 1 CUDA Capable device(s) Device 0: \u0026#34;NVIDIA GeForce RTX 3070 Ti Laptop GPU\u0026#34; CUDA Driver Version / Runtime Version 12.4 / 12.4 CUDA Capability Major/Minor version number: 8.6 Total amount of global memory: 8192 MBytes (8589410304 bytes) (046) Multiprocessors, (128) CUDA Cores/MP: 5888 CUDA Cores GPU Max Clock rate: 1410 MHz (1.41 GHz) Memory Clock rate: 7001 Mhz Memory Bus Width: 256-bit L2 Cache Size: 4194304 bytes Maximum Texture Dimension Size (x,y,z) 1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384) Maximum Layered 1D Texture Size, (num) layers 1D=(32768), 2048 layers Maximum Layered 2D Texture Size, (num) layers 2D=(32768, 32768), 2048 layers Total amount of constant memory: 65536 bytes Total amount of shared memory per block: 49152 bytes Total shared memory per multiprocessor: 102400 bytes Total number of registers available per block: 65536 Warp size: 32 Maximum number of threads per multiprocessor: 1536 Maximum number of threads per block: 1024 Max dimension size of a thread block (x,y,z): (1024, 1024, 64) Max dimension size of a grid size (x,y,z): (2147483647, 65535, 65535) Maximum memory pitch: 2147483647 bytes Texture alignment: 512 bytes Concurrent copy and kernel execution: Yes with 1 copy engine(s) Run time limit on kernels: Yes Integrated GPU sharing Host Memory: No Support host page-locked memory mapping: Yes Alignment requirement for Surfaces: Yes Device has ECC support: Disabled Device supports Unified Addressing (UVA): Yes Device supports Managed Memory: Yes Device supports Compute Preemption: Yes Supports Cooperative Kernel Launch: Yes Supports MultiDevice Co-op Kernel Launch: No Device PCI Domain ID / Bus ID / location ID: 0 / 1 / 0 Compute Mode: \u0026lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) \u0026gt; deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 12.4, CUDA Runtime Version = 12.4, NumDevs = 1 Result = PASS By running the bandwidthTest program, you can ensure that the system and CUDA-capable device are able to communicate correctly.\n./bandwidthText The output shoud be here.\n[CUDA Bandwidth Test] - Starting... Running on... Device 0: NVIDIA GeForce RTX 3070 Ti Laptop GPU Quick Mode Host to Device Bandwidth, 1 Device(s) PINNED Memory Transfers Transfer Size (Bytes) Bandwidth(GB/s) 32000000 24.5 Device to Host Bandwidth, 1 Device(s) PINNED Memory Transfers Transfer Size (Bytes) Bandwidth(GB/s) 32000000 26.3 Device to Device Bandwidth, 1 Device(s) PINNED Memory Transfers Transfer Size (Bytes) Bandwidth(GB/s) 32000000 323.6 Result = PASS NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled. For CUDA graphic programs, WSL 2 currently does not support GL, Vulcan. Hence, you cannot run CUDA graphic programs.\n 2. Installing NVIDIA CuDNN for WSL 2 Since cuDNN version 9 can exist independently with previous version cuDNN, you can refer here and follow the steps for your target platform.\nVerify cuDNN Installation To verify cuDNN is install and is running properly, you can download and compile cuDNN samples. These samples can be downloaded by running:\nsudo apt install -y libcudnn9-samples The samples is located in /usr/src/cudnn_samples_v9 directory, copy the directory to $HOME and set the permission.\nsudo cp -r cudnn_samples_v9/ $HOME sudo chmod -R a+rwx $HOME/cudnn_samples_v9 For the mnistCUDNN sample, install FreeImage package and compile the sample.\nsudo apt install -y libfreeimage3 libfreeimage-dev cd $HOME/cudnn_samples_v9/mnistCUDNN make clean \u0026amp;\u0026amp; make ./mnistCUDNN The result should be as:\nExecuting: mnistCUDNN cudnnGetVersion() : 90100 , CUDNN_VERSION from cudnn.h : 90100 (9.1.0) Host compiler version : GCC 11.4.0 There are 1 CUDA capable devices on your machine : device 0 : sms 46 Capabilities 8.6, SmClock 1410.0 Mhz, MemSize (Mb) 8191, MemClock 7001.0 Mhz, Ecc=0, boardGroupID=0 Using device 0 Testing single precision Loading binary file data/conv1.bin Loading binary file data/conv1.bias.bin Loading binary file data/conv2.bin Loading binary file data/conv2.bias.bin Loading binary file data/ip1.bin Loading binary file data/ip1.bias.bin Loading binary file data/ip2.bin Loading binary file data/ip2.bias.bin Loading image data/one_28x28.pgm Performing forward propagation ... Testing cudnnGetConvolutionForwardAlgorithm_v7 ... ^^^^ CUDNN_STATUS_SUCCESS for Algo 1: -1.000000 time requiring 0 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 0: -1.000000 time requiring 0 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 2: -1.000000 time requiring 0 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 5: -1.000000 time requiring 178432 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 4: -1.000000 time requiring 184784 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 7: -1.000000 time requiring 2057744 memory ^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 6: -1.000000 time requiring 0 memory ^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 3: -1.000000 time requiring 0 memory Testing cudnnFindConvolutionForwardAlgorithm ... ^^^^ CUDNN_STATUS_SUCCESS for Algo 0: 0.009216 time requiring 0 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 1: 0.009216 time requiring 0 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 5: 0.096256 time requiring 178432 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 7: 0.115712 time requiring 2057744 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 4: 0.168960 time requiring 184784 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 2: 0.546016 time requiring 0 memory ^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 6: -1.000000 time requiring 0 memory ^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 3: -1.000000 time requiring 0 memory Testing cudnnGetConvolutionForwardAlgorithm_v7 ... ^^^^ CUDNN_STATUS_SUCCESS for Algo 1: -1.000000 time requiring 0 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 0: -1.000000 time requiring 0 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 2: -1.000000 time requiring 0 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 4: -1.000000 time requiring 2450080 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 5: -1.000000 time requiring 4656640 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 7: -1.000000 time requiring 1433120 memory ^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 6: -1.000000 time requiring 0 memory ^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 3: -1.000000 time requiring 0 memory Testing cudnnFindConvolutionForwardAlgorithm ... ^^^^ CUDNN_STATUS_SUCCESS for Algo 0: 0.035840 time requiring 0 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 1: 0.058368 time requiring 0 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 2: 0.066560 time requiring 0 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 7: 0.093184 time requiring 1433120 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 5: 0.109600 time requiring 4656640 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 4: 0.192512 time requiring 2450080 memory ^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 6: -1.000000 time requiring 0 memory ^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 3: -1.000000 time requiring 0 memory Resulting weights from Softmax: 0.0000000 0.9999399 0.0000000 0.0000000 0.0000561 0.0000000 0.0000012 0.0000017 0.0000010 0.0000000 Loading image data/three_28x28.pgm Performing forward propagation ... Testing cudnnGetConvolutionForwardAlgorithm_v7 ... ^^^^ CUDNN_STATUS_SUCCESS for Algo 1: -1.000000 time requiring 0 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 0: -1.000000 time requiring 0 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 2: -1.000000 time requiring 0 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 5: -1.000000 time requiring 178432 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 4: -1.000000 time requiring 184784 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 7: -1.000000 time requiring 2057744 memory ^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 6: -1.000000 time requiring 0 memory ^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 3: -1.000000 time requiring 0 memory Testing cudnnFindConvolutionForwardAlgorithm ... ^^^^ CUDNN_STATUS_SUCCESS for Algo 1: 0.008192 time requiring 0 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 2: 0.009216 time requiring 0 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 0: 0.026624 time requiring 0 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 7: 0.061248 time requiring 2057744 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 4: 0.073728 time requiring 184784 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 5: 0.106496 time requiring 178432 memory ^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 6: -1.000000 time requiring 0 memory ^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 3: -1.000000 time requiring 0 memory Testing cudnnGetConvolutionForwardAlgorithm_v7 ... ^^^^ CUDNN_STATUS_SUCCESS for Algo 1: -1.000000 time requiring 0 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 0: -1.000000 time requiring 0 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 2: -1.000000 time requiring 0 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 4: -1.000000 time requiring 2450080 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 5: -1.000000 time requiring 4656640 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 7: -1.000000 time requiring 1433120 memory ^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 6: -1.000000 time requiring 0 memory ^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 3: -1.000000 time requiring 0 memory Testing cudnnFindConvolutionForwardAlgorithm ... ^^^^ CUDNN_STATUS_SUCCESS for Algo 0: 0.031520 time requiring 0 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 1: 0.038912 time requiring 0 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 2: 0.066400 time requiring 0 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 7: 0.091136 time requiring 1433120 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 5: 0.129024 time requiring 4656640 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 4: 0.145408 time requiring 2450080 memory ^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 6: -1.000000 time requiring 0 memory ^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 3: -1.000000 time requiring 0 memory Resulting weights from Softmax: 0.0000000 0.0000000 0.0000000 0.9999288 0.0000000 0.0000711 0.0000000 0.0000000 0.0000000 0.0000000 Loading image data/five_28x28.pgm Performing forward propagation ... Resulting weights from Softmax: 0.0000000 0.0000008 0.0000000 0.0000002 0.0000000 0.9999820 0.0000154 0.0000000 0.0000012 0.0000006 Result of classification: 1 3 5 Test passed! Testing half precision (math in single precision) Loading binary file data/conv1.bin Loading binary file data/conv1.bias.bin Loading binary file data/conv2.bin Loading binary file data/conv2.bias.bin Loading binary file data/ip1.bin Loading binary file data/ip1.bias.bin Loading binary file data/ip2.bin Loading binary file data/ip2.bias.bin Loading image data/one_28x28.pgm Performing forward propagation ... Testing cudnnGetConvolutionForwardAlgorithm_v7 ... ^^^^ CUDNN_STATUS_SUCCESS for Algo 1: -1.000000 time requiring 0 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 0: -1.000000 time requiring 0 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 2: -1.000000 time requiring 0 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 5: -1.000000 time requiring 178432 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 4: -1.000000 time requiring 184784 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 7: -1.000000 time requiring 2057744 memory ^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 6: -1.000000 time requiring 0 memory ^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 3: -1.000000 time requiring 0 memory Testing cudnnFindConvolutionForwardAlgorithm ... ^^^^ CUDNN_STATUS_SUCCESS for Algo 2: 0.009216 time requiring 0 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 0: 0.010240 time requiring 0 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 1: 0.017408 time requiring 0 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 7: 0.068512 time requiring 2057744 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 5: 0.070784 time requiring 178432 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 4: 0.103424 time requiring 184784 memory ^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 6: -1.000000 time requiring 0 memory ^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 3: -1.000000 time requiring 0 memory Testing cudnnGetConvolutionForwardAlgorithm_v7 ... ^^^^ CUDNN_STATUS_SUCCESS for Algo 7: -1.000000 time requiring 1433120 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 1: -1.000000 time requiring 1536 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 0: -1.000000 time requiring 0 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 2: -1.000000 time requiring 64000 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 5: -1.000000 time requiring 4656640 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 4: -1.000000 time requiring 2450080 memory ^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 6: -1.000000 time requiring 0 memory ^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 3: -1.000000 time requiring 0 memory Testing cudnnFindConvolutionForwardAlgorithm ... ^^^^ CUDNN_STATUS_SUCCESS for Algo 0: 0.079872 time requiring 0 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 1: 0.079872 time requiring 1536 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 2: 0.101376 time requiring 64000 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 5: 0.101440 time requiring 4656640 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 4: 0.105472 time requiring 2450080 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 7: 0.126976 time requiring 1433120 memory ^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 6: -1.000000 time requiring 0 memory ^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 3: -1.000000 time requiring 0 memory Resulting weights from Softmax: 0.0000001 1.0000000 0.0000001 0.0000000 0.0000563 0.0000001 0.0000012 0.0000017 0.0000010 0.0000001 Loading image data/three_28x28.pgm Performing forward propagation ... Testing cudnnGetConvolutionForwardAlgorithm_v7 ... ^^^^ CUDNN_STATUS_SUCCESS for Algo 1: -1.000000 time requiring 0 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 0: -1.000000 time requiring 0 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 2: -1.000000 time requiring 0 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 5: -1.000000 time requiring 178432 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 4: -1.000000 time requiring 184784 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 7: -1.000000 time requiring 2057744 memory ^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 6: -1.000000 time requiring 0 memory ^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 3: -1.000000 time requiring 0 memory Testing cudnnFindConvolutionForwardAlgorithm ... ^^^^ CUDNN_STATUS_SUCCESS for Algo 2: 0.010240 time requiring 0 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 0: 0.023552 time requiring 0 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 1: 0.064480 time requiring 0 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 7: 0.087008 time requiring 2057744 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 4: 0.103424 time requiring 184784 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 5: 0.138144 time requiring 178432 memory ^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 6: -1.000000 time requiring 0 memory ^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 3: -1.000000 time requiring 0 memory Testing cudnnGetConvolutionForwardAlgorithm_v7 ... ^^^^ CUDNN_STATUS_SUCCESS for Algo 7: -1.000000 time requiring 1433120 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 1: -1.000000 time requiring 1536 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 0: -1.000000 time requiring 0 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 2: -1.000000 time requiring 64000 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 5: -1.000000 time requiring 4656640 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 4: -1.000000 time requiring 2450080 memory ^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 6: -1.000000 time requiring 0 memory ^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 3: -1.000000 time requiring 0 memory Testing cudnnFindConvolutionForwardAlgorithm ... ^^^^ CUDNN_STATUS_SUCCESS for Algo 0: 0.083904 time requiring 0 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 7: 0.090112 time requiring 1433120 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 5: 0.094976 time requiring 4656640 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 4: 0.095232 time requiring 2450080 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 2: 0.111616 time requiring 64000 memory ^^^^ CUDNN_STATUS_SUCCESS for Algo 1: 0.186368 time requiring 1536 memory ^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 6: -1.000000 time requiring 0 memory ^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 3: -1.000000 time requiring 0 memory Resulting weights from Softmax: 0.0000000 0.0000000 0.0000000 1.0000000 0.0000000 0.0000714 0.0000000 0.0000000 0.0000000 0.0000000 Loading image data/five_28x28.pgm Performing forward propagation ... Resulting weights from Softmax: 0.0000000 0.0000008 0.0000000 0.0000002 0.0000000 1.0000000 0.0000154 0.0000000 0.0000012 0.0000006 Result of classification: 1 3 5 Test passed!  3. Installing NVIDIA TensorRT (Optional) NVIDIA TensorRT is a C++ library that facilitates high-performance inference NVIDIA graphic processing units (GPUs). TensorRT takes a trained network, which consists of a network definition and a set of trained parameters, and produces a highly optimized runtime engine that performs inference for that network.\nTensorRT provides APIs via C++ and Python that help to express deep learning model via the Network Definition API or load a pre-defined model via the ONNX parser that allow TensorRT to optimize and run them on the NVIDIA GPU. TensorRT also include optional high speed mixed precision capabilities with difference NVIDIA architectures.\nYou can download TensorRT local repo that matches the Ubuntu version and CPU architecture of your machine from here.\nInstall the Debian local package.\nos=\u0026#34;ubuntuxx04\u0026#34; tag=\u0026#34;10.x.x-cuda-x.x\u0026#34; sudo dpkg -i nv-tensorrt-local-repo-${os}-${tag}_1.0-1_amd64.deb sudo cp /var/nv-tensorrt-local-repo-${os}-${tag}/*-keyring.gpg /usr/share/keyrings/ sudo apt-get update Replace ubuntuxx04, 10.x.x and cuda-x.x with your specific OS version, TensorRT version, and CUDA version. The full TensorRT C++ and Python runtimes can be installed by:\nsudo apt install -y tensorrt Verify TensorRT Installation To verify the installation, run the command:\ndpkg-query -W tensorrt The ouput should be as following:\ntensorrt 10.0.0.6-1+cuda12.4 The samples is located in /usr/src/tensorrt directory, copy the directory to $HOME and set the permission.\nsudo cp -r tensorrt/ $HOME sudo chmod -R a+rwx $HOME/tensorrt For the sampleOnnxMNIST sample, compile the sample and run the program.\ncd $HOME/tensorrt/samples/sampleOnnxMNIST make clean \u0026amp;\u0026amp; make cd ../../bin ./sampleOnnxMNIST The output should be as following:\n\u0026amp;\u0026amp;\u0026amp;\u0026amp; RUNNING TensorRT.sample_onnx_mnist [TensorRT v100000] # ./sample_onnx_mnist [04/15/2024-18:24:17] [I] Building and running a GPU inference engine for Onnx MNIST [04/15/2024-18:24:17] [I] [TRT] [MemUsageChange] Init CUDA: CPU +19, GPU +0, now: CPU 21, GPU 1091 (MiB) [04/15/2024-18:24:19] [I] [TRT] [MemUsageChange] Init builder kernel library: CPU +1765, GPU +312, now: CPU 1922, GPU 1403 (MiB) [04/15/2024-18:24:19] [I] [TRT] ---------------------------------------------------------------- [04/15/2024-18:24:19] [I] [TRT] Input filename: ../data/mnist/mnist.onnx [04/15/2024-18:24:19] [I] [TRT] ONNX IR version: 0.0.3 [04/15/2024-18:24:19] [I] [TRT] Opset version: 8 [04/15/2024-18:24:19] [I] [TRT] Producer name: CNTK [04/15/2024-18:24:19] [I] [TRT] Producer version: 2.5.1 [04/15/2024-18:24:19] [I] [TRT] Domain: ai.cntk [04/15/2024-18:24:19] [I] [TRT] Model version: 1 [04/15/2024-18:24:19] [I] [TRT] Doc string: [04/15/2024-18:24:19] [I] [TRT] ---------------------------------------------------------------- [04/15/2024-18:24:19] [I] [TRT] Local timing cache in use. Profiling results in this builder pass will not be stored. [04/15/2024-18:24:21] [I] [TRT] Detected 1 inputs and 1 output network tensors. [04/15/2024-18:24:21] [I] [TRT] Total Host Persistent Memory: 26272 [04/15/2024-18:24:21] [I] [TRT] Total Device Persistent Memory: 0 [04/15/2024-18:24:21] [I] [TRT] Total Scratch Memory: 0 [04/15/2024-18:24:21] [I] [TRT] [BlockAssignment] Started assigning block shifts. This will take 6 steps to complete. [04/15/2024-18:24:21] [I] [TRT] [BlockAssignment] Algorithm ShiftNTopDown took 0.02021ms to assign 3 blocks to 6 nodes requiring 32256 bytes. [04/15/2024-18:24:21] [I] [TRT] Total Activation Memory: 31744 [04/15/2024-18:24:21] [I] [TRT] Total Weights Memory: 26152 [04/15/2024-18:24:21] [I] [TRT] Engine generation completed in 1.46118 seconds. [04/15/2024-18:24:21] [I] [TRT] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 5 MiB [04/15/2024-18:24:21] [I] [TRT] [MemUsageStats] Peak memory usage during Engine building and serialization: CPU: 3052 MiB [04/15/2024-18:24:21] [I] [TRT] Loaded engine size: 0 MiB [04/15/2024-18:24:21] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 0 (MiB) [04/15/2024-18:24:21] [I] Input: [04/15/2024-18:24:21] [I] @@@@@@@@@@@@@@@@@@@@@@@@@@@@ @@@@@@@@@@@@@@@@@@@@@@@@@@@@ @@@@@@@@@@@@@@@@@@@@@@@@@@@@ @@@@@@@@@@@@@@@@@@@@@@@@@@@@ @@@@@@@@@@@@@@@@@@@@@@@@@@@@ @@@@@@@@@@@@@@@@@@@@@@@@@@@@ @@@@@@@%=#@@@@@%=%@@@@@@@@@@ @@@@@@@ %@@@@@@@@@ @@@@@@@ %@@@@@@@@@ @@@@@@@#:-#-. %@@@@@@@@@ @@@@@@@@@@@@# #@@@@@@@@@@ @@@@@@@@@@@@@ #@@@@@@@@@@ @@@@@@@@@@@@@: :@@@@@@@@@@@ @@@@@@@@@%+== *%%%%%%%%%@@ @@@@@@@@% -@ @@@@@@@@@#+. .:-%@@ @@@@@@@@@@@* :-###@@@@@@ @@@@@@@@@@@* -%@@@@@@@@@@@ @@@@@@@@@@@* *@@@@@@@@@@@@ @@@@@@@@@@@* @@@@@@@@@@@@@ @@@@@@@@@@@* #@@@@@@@@@@@@ @@@@@@@@@@@* *@@@@@@@@@@@@ @@@@@@@@@@@* *@@@@@@@@@@@@ @@@@@@@@@@@* @@@@@@@@@@@@@ @@@@@@@@@@@* @@@@@@@@@@@@@ @@@@@@@@@@@@+=#@@@@@@@@@@@@@ @@@@@@@@@@@@@@@@@@@@@@@@@@@@ @@@@@@@@@@@@@@@@@@@@@@@@@@@@ [04/15/2024-18:24:21] [I] Output: [04/15/2024-18:24:21] [I] Prob 0 0.0000 Class 0: [04/15/2024-18:24:21] [I] Prob 1 0.0000 Class 1: [04/15/2024-18:24:21] [I] Prob 2 0.0000 Class 2: [04/15/2024-18:24:21] [I] Prob 3 0.0000 Class 3: [04/15/2024-18:24:21] [I] Prob 4 0.0000 Class 4: [04/15/2024-18:24:21] [I] Prob 5 0.0000 Class 5: [04/15/2024-18:24:21] [I] Prob 6 0.0000 Class 6: [04/15/2024-18:24:21] [I] Prob 7 1.0000 Class 7: ********** [04/15/2024-18:24:21] [I] Prob 8 0.0000 Class 8: [04/15/2024-18:24:21] [I] Prob 9 0.0000 Class 9: [04/15/2024-18:24:21] [I] \u0026amp;\u0026amp;\u0026amp;\u0026amp; PASSED TensorRT.sample_onnx_mnist [TensorRT v100000] # ./sample_onnx_mnist  If you are using TensorRT Python API, make sure CUDA-Python is installed in your system or your virtual environment.\nmamba create -n tensorrt_env pip wheel mamba activate tensorrt_env  Installing from PyPI, TensorRT runtime wheels  pip install cuda-python pip install --pre --upgrade tensorrt pip install --pre --upgrade tensorrt_lean tensorrt_dispatch To verify the installation is working, use python code.\nimport tensorrt print(tensorrt.__version__) assert(tensorrt.Builder(tensorrt.Logger())) import tensorrt_lean as trtl print(trtl.__version__) assert trtl.Runtime(trt.Logger()) import tensorrt_dispatch as trtd print(trtd.__version__) assert trtd.Runtime(trt.Logger()) The tensorrt/samples directory also provides the samples that utilize with the TensorRT, you can follow instructions from README.md to run samples.\n3. Set up Python environment using Mambaforge Python environment allows to manage separatelly different installations of Python and modules. It is useful when you have many projects running different version of Python and modules. It also help to manage installed modules for publish or reproduce.\nThere are different ways to create a python virtual environment, including built-in venv, Conda and Anaconda.\nConda is a packaging tool and installer that aims to handle library dependencies outside of the Python packages as well as the Python packages themselves. For non preinstalled package manager, Miniconda, an installation of Conda, will be a good option.\nAnaconda is an installation of Conda that comes pre-loaded with a bunch of packages for scientific computing, i.e., numpy, matplotlib, scipy, etc. It also comes with IDE, Jupyter notebooks out of the box. This is helpful for beginers, but doesn\u0026rsquo;t give much control.\nMamba is a package manager which can be used with Python. Unlike Conda, it uses the C/C++ implementation to speed up the package installation. Read more about mamba in here. To install mamba, access its repo and pick the Mabaforge installer for your operating system.\nRemember to run conda init at the end of your installation in your bash to activate the mamba command.\nConclusion By following these steps and installing the required software, you will have an CUDA-ready environment in WSL for further machine learnin/deep learning applications. This environment will provide the necessary tools and libraries for GPU-accelerated computing and Python package management.\nReference  Install CUDA and CUDNN on Windows \u0026amp; Linux. CUDA on WSL User Guide. Machine learning environment build: WLS2+Ubuntu+CUDA+cuDNN. Getting started with Mambaforge and Python. Tutorial: Setting up Python enviroments with Mambaforge.  ","permalink":"http://visionbike.github.io/posts/setitng-up-nvidia-cuda-on-wsl2/","summary":"Windows Subsystem for Linux (WSL) is a compatibility layer provided by Microsoft that allows you to run a Linux environment directly on WIndows. It will bring Linux applications, utilities and Bash command-line tool directly on Windows without the overhead of traditional virtual machine or dualboot setup.\n1. Prequisites The Windows must be upgraded to Windows 10 version 2004 and higher or Windows 11 to install WSL by a single command line.","title":"Setting Up NVIDIA CUDA on WSL 2"},{"content":"In this post, I will cover how to setup NVIDIA CUDA on Windows 11. To ensure a smooth setup process, it is crucial to follow the following steps:\n NVIDIA Drivers. Microsoft Visual Studio 2022 Community. NVIDIA CUDA Toolkit. NVIDIA cuDNN. TensorRT (optional). Miniforge (optional).  1. System Requirements To use CUDA, make sure your machine has a CUDA-capable GPU inside and the Microsoft Windows 11 should be updated 21H2 version.\nYou can verify if your machine has CUDA-supported GPU through Display Adapters section in the Windows Device Manager. Here you will find the vendor name and model of your GPU.\nYou also need to install Microsft Visual Studio 2022 Community as the native compilter for x86_64 application. The download link os the latest version is here and install the Desktop development with C++ workload.\nIf there is any NVIDIA CUDA Toolkit installed before, you need to uninstall before proceeding further, following these steps:\n Open the Settings \u0026gt; Apps \u0026gt; Installed Apps. Scroll down and find NVIDIA CUDA applications.  Click to \u0026ldquo;\u0026hellip;\u0026rdquo; button on the right and uninstall all NVIDIA GPU drivers and any associated software.  Then you need to install NVIDIA driver to communicate your computer with NVIDIA devices. You can find the suitable driver from this website.\nAfter the installation is complete, reboot your system.\n2. Installing Mamba Mamba is a command-line interfacer (CLI) to manage conda\u0026rsquo;s environemts. For mamba configuration, please refer to conda documentation.\nFor the fresh installation, you can install Miniforge distribution \u0026gt;= Miniforge3-22.3.1.0. Miniforge comes with the popular conda-forge channel preconfigured, but you can modify the configuration to use any channel you like.\nInstallation\nFollow the instaltion prompts, taking note of options to Create start menu shortcuts and Add Miniforge3 to my PATH environment variable.\n After successful installation, you can use use the mamba commands as described in this user guide.\nWorking with Conda Environment\nThe command using mamba is similar to the conda command.\n Create new environment  mamba create -n \u0026amp;lt;ENV_NAME\u0026amp;gt; python=\u0026amp;lt;VERSION\u0026amp;gt;  Activate an environment  mamba activate \u0026amp;lt;ENV_NAME\u0026amp;gt;  Deactivate environment  mamba deactivate  Delete an environment  mamba env remove -n \u0026amp;lt;ENV_NAME\u0026amp;gt;  Show all created environments  mamba env list Remember to activate an environment first, do not install any packages in your base environment!\n Install python packages  mamba install \u0026amp;lt;PKG_NAME\u0026amp;gt;[=VERSION] [-c \u0026amp;lt;CHANNEL_NAME\u0026amp;gt;] When installing a package, you can optionally indicate specific additional channel that the packages are posted by community. The conda-forge is one of most common additional channels.\n Delete packages  mamba remove \u0026amp;lt;PKG_NAME\u0026amp;gt;  Show all installed packages in the virtual environment  mamba list [-n \u0026amp;lt;ENV_NAME\u0026amp;gt;]  Post-installation\nAfter installation, you make sure that the Anaconda is not the default configured channel, seeing this.\nDO NOT install anything into the base environment as this might break your installation. See here for details.\nThe most convenient way to use the Mamba will be via the Miniforge Prompt installed in the Start Menu.\n 3. Installing NVIDIA CUDA Toolkit You can visit the NVIDIA Developer website for CUDA Toolkit TO get the latest version of NVIDIA CUDA Toolkit. For previous versions, you can check from the Archive of Previous CUDA Releases page.\nOn the dowload page, you choose the appropriate version based on your system.\nThen, you locate the downloaded installer file and double-click on it to start the installation process. Follow the on-screen instructions provided by the installer.\nOnce the installation is completed, you check environment variables CUDA_PATH and PATH to ensure that your system recognizes NVIDIA CUDA Toolkit.\nVerify CUDA Toolkit Installation You can verify the installation by running the following command in command prompt.\nnvcc --version If the installation was successful, you should see the CUDA version information displayed.\nnvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2024 NVIDIA Corporation Built on Tue_Feb_27_16:28:36_Pacific_Standard_Time_2024 Cuda compilation tools, release 12.4, V12.4.99 Build cuda_12.4.r12.4/compiler.33961263_0 It is important to verify that the NVIDIA CUDA Toolkit can find and communicate correctly with CUDA-compatible hardware. To do this, you need to compile and run some sample programs.\nCUDA samples are located in https://github.com/nvidia/cuda-samples. To use the samples, clone the project, build the samples in cuda-samples directory using MVSC 2022 compiler and run them following the instruction on the Github page.\nTo verify a correct configuration of the hardware and software, it is highly recommended that you build and run the deviceQuery sample program.\ndeviceQuery.exe If CUDA is installed and configured correctly, the output should look similar as below:\ndeviceQuery.exe Starting... CUDA Device Query (Runtime API) version (CUDART static linking) Detected 1 CUDA Capable device(s) Device 0: \u0026#34;NVIDIA GeForce RTX 3070 Ti Laptop GPU\u0026#34; CUDA Driver Version / Runtime Version 12.4 / 12.4 CUDA Capability Major/Minor version number: 8.6 Total amount of global memory: 8192 MBytes (8589410304 bytes) (046) Multiprocessors, (128) CUDA Cores/MP: 5888 CUDA Cores GPU Max Clock rate: 1410 MHz (1.41 GHz) Memory Clock rate: 7001 Mhz Memory Bus Width: 256-bit L2 Cache Size: 4194304 bytes Maximum Texture Dimension Size (x,y,z) 1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384) Maximum Layered 1D Texture Size, (num) layers 1D=(32768), 2048 layers Maximum Layered 2D Texture Size, (num) layers 2D=(32768, 32768), 2048 layers Total amount of constant memory: 65536 bytes Total amount of shared memory per block: 49152 bytes Total shared memory per multiprocessor: 102400 bytes Total number of registers available per block: 65536 Warp size: 32 Maximum number of threads per multiprocessor: 1536 Maximum number of threads per block: 1024 Max dimension size of a thread block (x,y,z): (1024, 1024, 64) Max dimension size of a grid size (x,y,z): (2147483647, 65535, 65535) Maximum memory pitch: 2147483647 bytes Texture alignment: 512 bytes Concurrent copy and kernel execution: Yes with 1 copy engine(s) Run time limit on kernels: Yes Integrated GPU sharing Host Memory: No Support host page-locked memory mapping: Yes Alignment requirement for Surfaces: Yes Device has ECC support: Disabled CUDA Device Driver Mode (TCC or WDDM): WDDM (Windows Display Driver Model) Device supports Unified Addressing (UVA): Yes Device supports Managed Memory: Yes Device supports Compute Preemption: Yes Supports Cooperative Kernel Launch: Yes Supports MultiDevice Co-op Kernel Launch: No Device PCI Domain ID / Bus ID / location ID: 0 / 1 / 0 Compute Mode: \u0026lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) \u0026gt; deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 12.4, CUDA Runtime Version = 12.4, NumDevs = 1 Result = PASS By running the bandwidthTest program, you can ensure that the system and CUDA-capable device are able to communicate correctly.\nbandwidthText.exe The output shoud be here.\n[CUDA Bandwidth Test] - Starting... Running on... Device 0: NVIDIA GeForce RTX 3070 Ti Laptop GPU Quick Mode Host to Device Bandwidth, 1 Device(s) PINNED Memory Transfers Transfer Size (Bytes) Bandwidth(GB/s) 32000000 11.3 Device to Host Bandwidth, 1 Device(s) PINNED Memory Transfers Transfer Size (Bytes) Bandwidth(GB/s) 32000000 13.9 Device to Device Bandwidth, 1 Device(s) PINNED Memory Transfers Transfer Size (Bytes) Bandwidth(GB/s) 32000000 361.1 Result = PASS NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled. To see a graphical representation, you can run the particles sample program.\nparticles.exe  The installed NVIDIA CUDA Toolkit provides the necessary libraries, compilers, and tools for developing and running CUDA-accelerated applications and machine learning models.\n4. Installing NVIDIA cuDNN cuDNN (CUDA Deep Neural Network Library) is a GPU-accelerated library specifically designed and colaborated with NVIDA CUDA Toolkit to accelerate deep neural network computations. By utilizing cuDNN, deep learning frameworks can leverage the parallel processing capabilities of NVIDIA GPUs, leading to significant speed improvements in training and inference of deep neural networks.\nYou can visit NVIDIA Developer website for cuDNN for the latest version. You will need to register or log in to your NVIDIA Developer account in order to access the cuDNN download files. If you don\u0026rsquo;t have an account, you can create one for free.\nOnce you are logged in, choose the appropriate cuDNN version based on the NVIDIA CUDA Toolkit version and operating system. There are two main installation options:\n Graphical installation (executable): the graphical installer bundles the available per-CUDA cuDNN verions in one package. Tarball installation (zip): per-CUDA cuDNN versions are provided as saperate tarballs (zip). These .zip archives do not replace the graphical installer and are not meant for general consumption, as they are not installers. These zip archives can be found at this.  Select one of two options for installing cuDNN. In this post, I will install cuDNN via the Tarball installation option.\nOnce download the zip archive, you unzip the cuDNN package.\nCopy the following files from the unzipped package into the NVIDIA cuDNN directory created by yourself.\n Copy bin\\cudnn*.h to C:Program Files\\NVIDIA\\CUDNN\\vx.y\\bin. Copy include\\cudnn*.h to C:\\Program Files\\NVIDIA\\CUDNN\\vx.y\\include. Copy lib\\x64\\cudnn*.lib to C:\\Program Files\\NVIDIA\\CUDNN\\vx.y\\lib.  You must replace x.y with your specific cuDNN version.\nSet the environment variable to point to where cuDNN is located and add bin directory path to PATH environment variable.\nFor upgrading cuDNN, the remove the path to the directory containing cuDNN from PATH environment variable.\nVerify cuDNN Installation The cuDNN samples can be found here and download and extract the tar.xz archive.\nSince this is cross-platform LINUX samples, you need to install CMAKE and use it to compile the cuDNN samples.\nInside the cuda_sample_vx directory (x as the cuDNN version), make the comment to line 21 # add_subdirectory(mnistCUDNN).\nRun the following command in the command prompt.\nmkdir build \u0026amp;\u0026amp; cd build cmake -G \u0026#34;Visual Studio 17 2022\u0026#34; -D cuDNN_INCLUDE_DIR=\u0026#34;C:\\Program Files\\NVIDIA\\CUDNN\\v9.0\\include\u0026#34; -D cuDNN_LIBRARY_DIR=\u0026#34;C:\\Program Files\\NVIDIA\\CUDNN\\v9.0\\lib\u0026#34; .. cmake --build . --config Release By running the conv_sample program, you can ensure the cuDNN work in your system.\n.\\conv_sample\\Release\\conv_sample.exe The result should be as following:\nExecuting: conv_sample.exe Using format CUDNN_TENSOR_NCHW (for INT8x4 and INT8x32 tests use CUDNN_TENSOR_NCHW_VECT_C) Testing single precision ====USER DIMENSIONS==== input dims are 1, 32, 4, 4 filter dims are 32, 32, 1, 1 output dims are 1, 32, 4, 4 ====PADDING DIMENSIONS==== padded input dims are 1, 32, 4, 4 padded filter dims are 32, 32, 1, 1 padded output dims are 1, 32, 4, 4 Testing conv ^^^^ CUDA : elapsed = 0.0127218 sec, Test PASSED Testing half precision (math in single precision) ====USER DIMENSIONS==== input dims are 1, 32, 4, 4 filter dims are 32, 32, 1, 1 output dims are 1, 32, 4, 4 ====PADDING DIMENSIONS==== padded input dims are 1, 32, 4, 4 padded filter dims are 32, 32, 1, 1 padded output dims are 1, 32, 4, 4 Testing conv ^^^^ CUDA : elapsed = 0.0029165 sec, Test PASSED  For Visual Studio project, add cuDNN by following steps:\n Right-click on the project name in Solution Explorer and choose **Properties. Click VC++ Directories and append C:\\Program Files\\NVIDIA\\CUDNN\\v9.x\\include to the Include Direcotries field. Click Linker \u0026gt; General and append C:\\Program Files\\NVIDIA\\CUDNN\\v9.x\\lib to the Additional Library Directories field. Click Linker \u0026gt; Input and append cudnn.h to the Additional Dependencies field and click OK.  5. Installing NVIDIA TensorRT (Optional) NVIDIA TensorRT is a C++ library that facilitates high-performance inference NVIDIA graphic processing units (GPUs). TensorRT takes a trained network, which consists of a network definition and a set of trained parameters, and produces a highly optimized runtime engine that performs inference for that network.\nTensorRT provides APIs via C++ and Python that help to express deep learning model via the Network Definition API or load a pre-defined model via the ONNX parser that allow TensorRT to optimize and run them on the NVIDIA GPU.\nTensorRT also include optional high speed mixed precision capabilities with difference NVIDIA architectures.\nYou can download the TensorRT at here. For Windows architecture, there is only zip archive installation.\nUnzip the zip archive and copy files in lib, include direcotries to C:\\Program Files\\NVIDIA\\TensorRT\\v10.0 directory created by yourself. Then, you add lib directory path to PATH environment variable.\nVerify TensorRT Installation Inside the zip archive also include the sample programs. To verify the installation is working, you should open a Visual Studio file from one of the samples, such as sampleOnnxMNIST.In the project, ensure that following is presented in the Visual Studio Solution project properties:\n Add C:\\Program Files\\NVIDIA\\TensorRT\\v10.0\\lib to PATH and VC++ Directories \u0026gt; Executable Directories. Add C:\\Program Files\\NVIDIA\\TensorRT\\v10.0\\include to C/C++ \u0026gt; General \u0026gt; Additional Directories. Add nvinfer.lib and .lib files that that the projects requires to Linker \u0026gt; Input \u0026gt; Additional Dependencies.  Compile the source code and run the example.\n.\\bin\\sample_onnx_mnist.exe The output should be as following:\n\u0026amp;\u0026amp;\u0026amp;\u0026amp; RUNNING TensorRT.sample_onnx_mnist [TensorRT v100000] # .\\sample_onnx_mnist.exe [04/08/2024-18:12:10] [I] Building and running a GPU inference engine for Onnx MNIST [04/08/2024-18:12:10] [I] [TRT] [MemUsageChange] Init CUDA: CPU +109, GPU +0, now: CPU 18227, GPU 1091 (MiB) [04/08/2024-18:12:18] [I] [TRT] [MemUsageChange] Init builder kernel library: CPU +2597, GPU +310, now: CPU 21109, GPU 1401 (MiB) [04/08/2024-18:12:18] [I] [TRT] ---------------------------------------------------------------- [04/08/2024-18:12:18] [I] [TRT] Input filename: ../data/mnist/mnist.onnx [04/08/2024-18:12:18] [I] [TRT] ONNX IR version: 0.0.3 [04/08/2024-18:12:18] [I] [TRT] Opset version: 8 [04/08/2024-18:12:18] [I] [TRT] Producer name: CNTK [04/08/2024-18:12:18] [I] [TRT] Producer version: 2.5.1 [04/08/2024-18:12:18] [I] [TRT] Domain: ai.cntk [04/08/2024-18:12:18] [I] [TRT] Model version: 1 [04/08/2024-18:12:18] [I] [TRT] Doc string: [04/08/2024-18:12:18] [I] [TRT] ---------------------------------------------------------------- [04/08/2024-18:12:18] [I] [TRT] Local timing cache in use. Profiling results in this builder pass will not be stored. [04/08/2024-18:12:20] [I] [TRT] Detected 1 inputs and 1 output network tensors. [04/08/2024-18:12:20] [I] [TRT] Total Host Persistent Memory: 26400 [04/08/2024-18:12:20] [I] [TRT] Total Device Persistent Memory: 0 [04/08/2024-18:12:20] [I] [TRT] Total Scratch Memory: 0 [04/08/2024-18:12:20] [I] [TRT] [BlockAssignment] Started assigning block shifts. This will take 6 steps to complete. [04/08/2024-18:12:20] [I] [TRT] [BlockAssignment] Algorithm ShiftNTopDown took 0.8572ms to assign 3 blocks to 6 nodes requiring 32256 bytes. [04/08/2024-18:12:20] [I] [TRT] Total Activation Memory: 31744 [04/08/2024-18:12:20] [I] [TRT] Total Weights Memory: 26152 [04/08/2024-18:12:20] [I] [TRT] Engine generation completed in 1.68333 seconds. [04/08/2024-18:12:20] [I] [TRT] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 5 MiB [04/08/2024-18:12:20] [I] [TRT] [MemUsageStats] Peak memory usage during Engine building and serialization: CPU: 3036 MiB [04/08/2024-18:12:20] [I] [TRT] Loaded engine size: 0 MiB [04/08/2024-18:12:21] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 0 (MiB) [04/08/2024-18:12:21] [I] Input: [04/08/2024-18:12:21] [I] @@@@@@@@@@@@@@@@@@@@@@@@@@@@ @@@@@@@@@@@@@@@@@@@@@@@@@@@@ @@@@@@@@@@@@@@@@@@@@@@@@@@@@ @@@@@@@@@@@@@@@@@@@@@@@@@@@@ @@@@@@@@@@@@@@@@@@@@@@@@@@@@ @@@@@@@@@@= ++++#++=*@@@@@ @@@@@@@@#. *@@@@@ @@@@@@@@= *@@@@@ @@@@@@@@. .. ...****%@@@@@ @@@@@@@@: .%@@#@@@@@@@@@@@@@ @@@@@@@% -@@@@@@@@@@@@@@@@@ @@@@@@@% -@@*@@@*@@@@@@@@@@ @@@@@@@# :#- ::. ::=@@@@@@@ @@@@@@@- -@@@@@@ @@@@@@%. *@@@@@ @@@@@@# :==*+== *@@@@@ @@@@@@%---%%@@@@@@@. *@@@@@ @@@@@@@@@@@@@@@@@@@+ *@@@@@ @@@@@@@@@@@@@@@@@@@= *@@@@@ @@@@@@@@@@@@@@@@@@* *@@@@@ @@@@@%+%@@@@@@@@%. .%@@@@@ @@@@@* .******= -@@@@@@@ @@@@@* .#@@@@@@@ @@@@@* =%@@@@@@@@ @@@@@@%#+++= =@@@@@@@@@@ @@@@@@@@@@@@@@@@@@@@@@@@@@@@ @@@@@@@@@@@@@@@@@@@@@@@@@@@@ @@@@@@@@@@@@@@@@@@@@@@@@@@@@ [04/08/2024-18:12:21] [I] Output: [04/08/2024-18:12:21] [I] Prob 0 0.0000 Class 0: [04/08/2024-18:12:21] [I] Prob 1 0.0000 Class 1: [04/08/2024-18:12:21] [I] Prob 2 0.0000 Class 2: [04/08/2024-18:12:21] [I] Prob 3 0.0000 Class 3: [04/08/2024-18:12:21] [I] Prob 4 0.0000 Class 4: [04/08/2024-18:12:21] [I] Prob 5 1.0000 Class 5: ********** [04/08/2024-18:12:21] [I] Prob 6 0.0000 Class 6: [04/08/2024-18:12:21] [I] Prob 7 0.0000 Class 7: [04/08/2024-18:12:21] [I] Prob 8 0.0000 Class 8: [04/08/2024-18:12:21] [I] Prob 9 0.0000 Class 9: [04/08/2024-18:12:21] [I] \u0026amp;\u0026amp;\u0026amp;\u0026amp; PASSED TensorRT.sample_onnx_mnist [TensorRT v100000] # .\\sample_onnx_mnist.exe  If you are using TensorRT Python API, make sure CUDA-Python is installed in your system or your virtual environment.\nmamba create -n tensorrt_env pip wheel mamba activate tensorrt_env  Installing from PyPI, TensorRT runtime wheels  pip install cuda-python pip install --pre --upgrade tensorrt pip install --pre --upgrade tensorrt_lean tensorrt_dispatch To verify the installation is working, use python code.\nimport tensorrt print(tensorrt.__version__) assert(tensorrt.Builder(tensorrt.Logger())) import tensorrt_lean as trtl print(trtl.__version__) assert trtl.Runtime(trt.Logger()) import tensorrt_dispatch as trtd print(trtd.__version__) assert trtd.Runtime(trt.Logger()) The tensorrt/samples directory also provides the samples that utilize with the TensorRT, you can follow instructions from README.md to run samples.\nConclusion By following these steps and installing the required software, you will have an CUDA-ready environment in Windows 11 system for further machine learnin/deep learning applications. This environment will provide the necessary tools and libraries for GPU-accelerated computing and Python package management.\nReference  CUDA Installation Guide for Microsoft Windows. Install CUDA and CUDNN on Windows \u0026amp; Linux. Installing Latest TensorFlow version with CUDA, cudNN and GPU support on Windows 11 PC.  ","permalink":"http://visionbike.github.io/posts/setting-up-nvidia-cuda-on-windows-11/","summary":"In this post, I will cover how to setup NVIDIA CUDA on Windows 11. To ensure a smooth setup process, it is crucial to follow the following steps:\n NVIDIA Drivers. Microsoft Visual Studio 2022 Community. NVIDIA CUDA Toolkit. NVIDIA cuDNN. TensorRT (optional). Miniforge (optional).  1. System Requirements To use CUDA, make sure your machine has a CUDA-capable GPU inside and the Microsoft Windows 11 should be updated 21H2 version.","title":"Setting Up NVIDIA CUDA on Windows 11"},{"content":"Creating a personal blog with technical content is a excellent way to enhance the writting skill, keep memorial notes and share personal experience with others. Ideally, these goals need to be achieved when creating and mantaining a blog:\n Low-cost - Free or as close to free as posisble. Productive - Easy to write and maintain. Cloud Native - Utilizes public cloud services for hosting, allowing for infinite scaling.  After researching, I found that using Markdown, Hugo and GitHub Pages is indeed a powerful combination for creating and maintaining a cost-effective, productive, and cloud-native blog.\n Markdown is markup language that is extremely easy to read, write natively and can be converted into HTML. Hugo is a static site generator written in the Go language that allows for content written in Markdown to be rendered into HTML webpages. GitHub Pages is the GitHub service that hosts web contentstored in a GitHub repository.  In this post, I will show how to create an simple personal blog for FREE using above technologies. The blog was developed in Window Subsystem for Linux (WSL2).\nHere\u0026rsquo;s an outline of the steps you can follow to create the personal blog using these technologies:\n1. Setting up GitHub Account If you don\u0026rsquo;t have one already, creating a GitHub account. GitHub Pages allows you to host your blog for free using a GitHub repository.\n2. Installing Hugo Before starting, make sure git is installed in the local machine.\nsudo apt install -y git Configure Git with your username and email address.\ngit config --global user.name \u0026#34;Your Name\u0026#34; git config --global user.email \u0026#34;your.email@example.com\u0026#34; To verify that Git has been installed successfully, you can check the version using git --version. This command will display the installed version of Git.\nFor Ubuntu user, you can install Hugo on your host by this command:\nsudo apt install -y hugo Run the folloing command for verification:\nhugo version The Hugo version should be shown if the installation is successfull.\nhugo v0.92.2+extended linux/amd64 BuildDate=2023-01-31T11:11:57Z VendorInfo=ubuntu:0.92.2-1ubuntu0.1 3. Creating a new Hugo site You can run hugo new site command to create a new Hugo site:\nhugo new site \u0026amp;lt;USERNAME\u0026amp;gt;-Hugo -f yml This command will set up the basic directory structure and configuration file in *.yml format for your blog.\nWhat is `\u0026lt;USERNAME\u0026gt;`?\nFor convenient management and organization, you should name the your blog project as above format with \u0026lt;USERNAME\u0026gt; as the your GitHub\u0026rsquo;s username, i.e., Visionbike-Hugo. It\u0026rsquo;s helpful to keep track your project and ensuring clarity when managing multiple repositories.\n The site will be associated with a GitHub repository where you can store the source code of your blog. Hence, you need to initialize git in the local project for further use.\ncd \u0026amp;lt;USERNAME\u0026amp;gt;-Hugo git init You also need to create a new repository on GitHub storage for your blog\u0026rsquo;s source code.\nCreating Github Repository Without README File\nBy creating a repository without a README file, you can avoid accidental history conflicts when pushing your local project to a fresh repository. You can always add a README file later if needed.\n Now, you link the local project to the GitHub repository by the git remote command:\ngit remote add origin https://github.com/\u0026amp;lt;USERNAME\u0026amp;gt;/\u0026amp;lt;USERNAME\u0026amp;gt;.github.io.git git banrch -M master By completing these steps, you have linked your local Hugo site to the GitHub repository. Now you can continue working on your site locally, commit any changes, and push them to the remote repository when ready.\n4. Installing Hugo Theme Installing a Hugo theme is a fantastic way to personalize your blog and enhance its visual appeal. You can access free Hugo themes via this website.\nFor my blog, I chose the PaperModX theme because of fonding its style and awesome features. I added its source code by the git submodule command.\ngit submodule add --depth 1 https://github.com/reorx/hugo-PaperModX themes/PaperModX The command will add the PaperModX theme repository as a submodule in the themes/PaperModX directory of your Hugo site.\nUpdating submodules\nIf you have already added the submodule before, you can run the following command to reclone it.\ngit submodule update --init --recursive For updating the theme, run this command.\ngit submodule update --remote --merge  5. Modify Hugo Configuration Once you have added the theme, you can configure it in your Hugo site\u0026rsquo;s configuration file (config.yml). Refer to the theme\u0026rsquo;s documentation for specific instructions on customization and configuration options. You will most likely want to modify the following fields:\n  baseURL: This should be set into the URL GitHub Pages for hosting your blog. If the GitHub repository is named \u0026lt;USERNAME\u0026gt;.github.io, then the value of baseURL will be https://\u0026lt;USERNAME\u0026gt;.github.io/. If the GitHub repository has any other name, then the value will be https://\u0026lt;USERNAME\u0026gt;.github.io/\u0026lt;REPOSITORY_NAME\u0026gt;/. For instance, my GitHub username is visionbike, then:\n If the GitHub repository is named visionbike.github.io, then the baseURL will be https://visionbike.github.io/. If the GitHub repository is named visionbike-hugo, then the baseURL will be https://visionbike.github.io/visionbike-hugo/.    title: This will be the title of your blog site as it appears at the top of a visitorâ€™s web browser when your site is open. It will also appear underneath your avatar, if one is present.\n  theme: The name of the theme Hugo should use to render your site. In my example, this will be set to PaperModX, since that is the name of the theme I am using.\n  Example contents of the config.yml file can be found below.\nconfig.yml # base URLbaseURL:\u0026#34;http://visionbike.github.io/\u0026#34;# site titletitle:\u0026#34;Visionbike - Personal Blog of CV | DSP | ML notes\u0026#34;# paginatepaginate:5# theme configtheme:\u0026#34;PaperModX\u0026#34;themesdir:\u0026#34;themes\u0026#34;# global configenableInlineShortcodes:trueenableRobotsTXT:truebuildDrafts:falsebuildFuture:falsebuildExpired:falseenableEmoji:true# css minify for speeding up siteminify:disableXML:trueminifyOutput:true# site param configparams:# environmentenv:\u0026#34;production\u0026#34;description:\u0026#34;Visionbike - Personal Blog of CV | DSP | ML notes\u0026#34;# color scheme: auto, dark, lightdefaultTheme:\u0026#34;dark\u0026#34;disableThemeToggle:true# header logologo:text:\u0026#34;Visionbike\u0026#34;icon:\u0026#34;images/apple-touch-icon.png\u0026#34;iconHeight:35iconWidth:35# profile-modeprofileMode:enabled:truetitle:\u0026#34;Phuc Thanh-Thien Nguyen\u0026#34;subtitle:\u0026#34;AI Researcher - Personal Blog of CV | DSP | ML notes\u0026#34;imageUrl:\u0026#34;images/avatar-real.png\u0026#34;imageTitle:\u0026#34;avatar-real\u0026#34;imageWidth:180imageHeight:180buttons:- name:Archivesurl:/archives/- name:Tagsurl:/tags/# home-info-modehomeInfoParams:title:\u0026#34;Phuc Thanh-Thien Nguyen\u0026#34;content:\u0026#34;AI Researcher - Personal Blog of CV | DSP | ML notes\u0026#34;# home social iconssocialIcons:- name:githuburl:\u0026#34;https://github.com/visionbike/\u0026#34;- name:linkedInurl:\u0026#34;https://linkedin.com/in/nttphuc/\u0026#34;# post configauthor:\u0026#34;Visionbike\u0026#34;showCodeCopyButtons:truedisplayFullLangName:trueshowReadingTime:trueshowWordCount:trueshowPostNavLinks:trueshowBreadCrumbs:trueenableImageZoom:true# assets imagesassets:# disableFingerprinting: truefavicon:\u0026#34;images/favicon.ico\u0026#34;favicon16x16:\u0026#34;images/favicon-16x16.png\u0026#34;favicon32x32:\u0026#34;images/favicon-32x32.png\u0026#34;appleTouchIcon:\u0026#34;images/apple-touch-icon.png\u0026#34;safarPinnedTab:\u0026#34;images/safari-pinned-tab.svg\u0026#34;# search pagefuseOpts:isCaseSensitive:falseshouldSort:truelocation:0distance:1000threshold:0.4minMatchCharLength:0keys:[\u0026#34;title\u0026#34;,\u0026#34;permalink\u0026#34;,\u0026#34;summary\u0026#34;,\u0026#34;content\u0026#34;]# laguage configlanguages:en:weight:1# language codelanguageCode:\u0026#34;en\u0026#34;# determines default content language: \u0026#34;en\u0026#34;, \u0026#34;zh-cn\u0026#34;, \u0026#34;fr\u0026#34;, \u0026#34;vi\u0026#34;, ...defaultContentLanguage:\u0026#34;en\u0026#34;# menu languagemenu:main:- name:\u0026#34;Posts\u0026#34;weight:1url:\u0026#34;/posts/\u0026#34;- name:\u0026#34;Tags\u0026#34;url:\u0026#34;/tags/\u0026#34;weight:3- name:\u0026#34;Archive\u0026#34;url:\u0026#34;/archives/\u0026#34;weight:4- name:\u0026#34;Publish\u0026#34;url:\u0026#34;/publish/\u0026#34;weight:5- name:\u0026#34;About\u0026#34;weight:10url:\u0026#34;/about/\u0026#34;- name:\u0026#34;Search\u0026#34;weight:100url:\u0026#34;/search/\u0026#34;# for search pageoutputs:home:- HTML- RSS- JSON# syntax highlightpygmentsUseClasses:truepygmentsCodeFences:truemarkup:goldmark:renderer:unsafe:truehighlight:lineNos:falsecodeFences:truenoClasses:false After modifying the configuration file accordingly, you can commit and push changes from your local repository to GitHub.\ngit add config.yml git commit -m \u0026#34;modify configuration file\u0026#34; git push -u origin master 6. Creating New Hugo Post You are almost done finishing your personal blog!\nTo create the first post, you execute the hugo new command in the terminal.\nhugo new posts/first-post/index.md The command will create a new folder named first-post with new Markdown file index.md, inside the content/posts directory. Creating a new directory for each single post helps you manage your resource better when images, media sources can be store directly in this directory. The Markdown file will contain the template for your first blog post and you can start writing your content using Markdown syntax.\nThe contents of the first-post.md file will look like as:\nfirst-post.md\n--- title: \u0026#34;First Post\u0026#34; date: 2023-07-04T01:53:57+08:00 draft: true ---  You need to add content to the Markdown file and update the metadata header. In the metadata header, you will find information such as the post\u0026rsquo;s title, publishing date and draft status. Change the value of the draft field from true to false to indicate that the post is ready to be published on your blog site. Your can also add other features supported by the installed theme for your post, i.e., comments, share buttons, navigation, etc.\nThe, you add the desired content to the body of the post. For instance, I added the line \u0026ldquo;This is my first post! Hello world!\u0026quot; at the bottom of the file. Feel free to customize the content to reflect your own thoughts and ideas in Markdown syntax.\nAfter modifying the first blog post, you can use git commit to commit and push the changes from your local repository to GitHub.\ngit add content/posts/first-post.md git commit -m \u0026#34;add the first post\u0026#34; git push -u origin master 7. Testing the Hugo Configuration Before hosting your blog to GitHub pages, make ensure Hugo can parse the configuration file and build our new blog post successfully.\nIn the local machine, you can run hugo server command to serve your site locally.\nhugo server --disableFastRender It will watch for any changes in your files and automatically rebuild your site whenever there are updates. Hugo will provide a local development server address, i.e., http://localhost:1313, where you can access your site locally.\nTo parse the configuration and build your site, you simply run hugo command.\nhugo If Hugo encounters any errors, they will be reported here. If the site is successfully built, then you will see output similar to the following.\nStart building sites â€¦ hugo v0.92.2+extended linux/amd64 BuildDate=2023-01-31T11:11:57Z VendorInfo=ubuntu:0.92.2-1ubuntu0.1 | EN -------------------+----- Pages | 13 Paginator pages | 0 Non-page files | 0 Static files | 18 Processed images | 0 Aliases | 0 Sitemaps | 1 Cleaned | 0 Total in 38 ms 8. Setting Up GitHub Actions Workflow Lastly, the GitHub Actions workflow has to be prepared for automatically building and deploying your blog to GitHub Pages. This workflow is defined by a YAML file in the .github/workflows directory structure at the root of the project.\nFirst, create the workflows folder.\nmkdir -p .github/workflows The command will create the .github/workflows directory if it doesn\u0026rsquo;t already exist. The -p option ensures that the parent directories are created if needed.\nThen, we create a new file within the created folder directory, named deploy_gh_pages.yaml with the following contents.\ndeploy_gh_pages.yaml\n------name:Deploy Hugo site via GitHub Pageson:push:branches:- master# Set a branch to deploypull_request:jobs:deploy:runs-on:ubuntu-22.04permissions:contents:writeconcurrency:group:${{ github.workflow }}-${{ github.ref }}steps:- uses:actions/checkout@v3with:submodules:true# Fetch Hugo themes (true OR recursive)fetch-depth:0# Fetch all history for .GitInfo and .Lastmod- name:Setup Hugouses:peaceiris/actions-hugo@v2with:hugo-version:\u0026#34;latest\u0026#34;extended:true- name:Buildrun:hugo --minify- name:Deployuses:peaceiris/actions-gh-pages@v3# If you\u0026#39;re changing the branch from main,# also change the `master` in `refs/heads/master`# below accordingly.if:github.ref == \u0026#39;refs/heads/master\u0026#39;with:github_token:${{ secrets.GITHUB_TOKEN }}publish_dir:./public The YAML file sets up the deployment process using Hugo and GitHub Pages. The workflow is triggered on a push to the master branch, and it uses the specified actions to build and deploy your blog. You can find more in this.\nFinally, we use git commands to commit and push the changes from your local repository to GitHub.\ngit add .github/workflows/deploy_gh_pages.yaml git commit -m \u0026#34;Add GitHub Actions workflow\u0026#34; git push -u origin master 9. Configuring GitHub Pages The GitHub Pages will allow GitHub to build automatically and serve our website whenever changes are made to the underlying repository.\nFirst, we create new branch named gh-page. This branch will be used by GitHub Pages to build and serve your website. You can create the branch using the branch creation feature in your GitHub repository.\nThen, go to the Settings tab near the top of your repository.\nIn the left hand pane, locate and click on the Pages category.\nBy default, GitHub Pages will be disabled for your repository. To enable it, we need to select a branch for GitHub Pages to build and serve our website from. Under the Source section in the middle pane, you will see a dropdown menu labeled None. Click on the dropdown menu and select the gh-pages branch. This tells GitHub Pages to build and serve your website from the gh-pages branch.\nAfter selecting the deployment branch, you will see a notification indicating where your site will be published. It will provide you with a URL where your website can be accessed.\nWait for a few minutes to allow GitHub Pages to build and deploy your website. When the deployment completes, you can click on the URL provided in the notification to view your website. It may take some time for the changes to propagate and for your website to become accessible.\nConclusion Congratulations on setting up your blog using Hugo, Markdown, and GitHub Pages! This free and accessible solution enables you to create and share your technical knowledge with a wide audience. Happy blogging!\nReference   How to Create a Simple, Free Blog with Hugo and GitHub Pages.\n  https://github.com/reorx/hugo-PaperModX.\n  https://gohugo.io/.\n  https://github.com/olOwOlo/hugo-theme-even/.\n  ","permalink":"http://visionbike.github.io/posts/setting-up-personal-blog-with-hugo-and-gh-pages/","summary":"Creating a personal blog with technical content is a excellent way to enhance the writting skill, keep memorial notes and share personal experience with others. Ideally, these goals need to be achieved when creating and mantaining a blog:\n Low-cost - Free or as close to free as posisble. Productive - Easy to write and maintain. Cloud Native - Utilizes public cloud services for hosting, allowing for infinite scaling.  After researching, I found that using Markdown, Hugo and GitHub Pages is indeed a powerful combination for creating and maintaining a cost-effective, productive, and cloud-native blog.","title":"Setting Up a Personal Blog With Hugo and GitHub Pages"},{"content":"I am Phuc Thanh-Thien Nguyen, current working as Ph.D Candidate at Autonomous and Soft Robotics (ASR) Laboratory, National Taiwan University, Taiwan. I focus on computer vision, digital signal processing, machine learning and deep learning in various applications.\nWelcome to my blog! In this blog, I will post small yet interesting notes of knowledge as well as state-of-the-art AI innovation, that I have found during my work as a researcher.\nI have done research on convolutional neural networks for computer-aided diagnosis and biosignal processing for various applications. In my current work, I am applying machine learning (supervised \u0026amp; unsupervised) and statistics to business problems. My go-to programming language is Python with the common data science and machine learning stack.\nCheck out my CV and connect with me on Linkedinâ€‹ if you want to get in touch.\nDisclaimer While I make every effort to ensure the information on this website is accurate and correct, I make no representations about the suitability of this content for any purposes.\nAbout Me! 1. Experience Research Assistant Applied Computing and Multimedia Lab, National Yang-Ming Chiao-Tung University, Taiwan, Sept 2020 - Jan 2021\n Research suppervised and unsuppervised-based approaches for digital image enhancement.  Software Engineer Lac Viet Computing Corporation, Vietnam, Sept 2017 - Aug 2018\n  Develop deep learning core of Question Answering System that using Restful API, and noSQL database (MongoDB) for Vietnamese Law.\n  Develop a prototype version for a facial recognition-based timekeeping service.\n  2. Education Background National Taiwan University of Science and Technology Ph.D. Candidate, Electrical Engineering, 2021 - now\n Study vision-based road anomaly detection, sEMG signal processing, and sEMG-based hand gesture recognition.  National Taiwan University of Science and Technology M.Sc., Computer Science and Information Engineering, 2018 - 2020\n GPA: 4.11 / 4.3 Thesis: A Study on Optimal Chroma Subsampling and Upsampling Combination Methods for JPEG Image Reconstrsuction. Study small object detection and distance estimation for the ADAS system, chroma subsampling and upsampling in JPEG compression.  VNU-HCM University of Science B.Sc., Computer Science, 2013 - 2017\n GPA: 3.36 / 4.0 Thesis: Spatial-Temporal Shape and Motion Feature for Dynamic Hand Gesture Recognition in Depth Video.  ","permalink":"http://visionbike.github.io/about/","summary":"I am Phuc Thanh-Thien Nguyen, current working as Ph.D Candidate at Autonomous and Soft Robotics (ASR) Laboratory, National Taiwan University, Taiwan. I focus on computer vision, digital signal processing, machine learning and deep learning in various applications.\nWelcome to my blog! In this blog, I will post small yet interesting notes of knowledge as well as state-of-the-art AI innovation, that I have found during my work as a researcher.\nI have done research on convolutional neural networks for computer-aided diagnosis and biosignal processing for various applications.","title":"About"},{"content":"C.-H. Kuo, P. T.-T. Nguyen, and S.-L. Wu. Teaching Artificial Intelligence in Mechanical Engineering to Cultivate Cyber-physical System Talents. In Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 2 (ITiCSE 2023). https://doi.org/10.1145/3587103.3594193\nI.W.D. Pranata, P.T.-T. Nguyen, K.-H. Su, Y.-C.Kuo, C.-H Kuo. Knee Angle Generation with Walking Speed Adaptation Ability for a Powered Transfemoral Prosthetic Leg Prototype. Inventions 2023, 8, 67. https://doi.org/10.3390/inventions8030067\nM.-H. Hsu; P.T.-T. Nguyen; D.-D. Nguyen; C.-H. Kuo. Image Servo Tracking of a Flexible Manipulator Prototype with Connected Continuum Kinematic Modules. Actuators 2022, 11, 360. DOI: https://doi.org/10.3390/act11120360\nT.-K. Nguyen, P.T.-T. Nguyen, D.-D.Nguyen, C.-H. Kuo. Effective Free-Driving Region Detection for Mobile Robots by Uncertainty Estimation Using RGB-D Data. Sensors 2022, 22, 4751. DOI: https://doi.org/10.3390/s22134751\nP.T.-T. Nguyen, S.-W. Yan, J.-F. Liao, C.-H. Kuo. Autonomous Mobile Robot Navigation in Sparse LiDAR Feature Environments. Appl. Sci. 2021, 11, 5963. DOI: https://doi.org/10.3390/app11135963\nA.-T. Nguyen, S.-H. Lu, P.T.-T. Nguyen. Validating and Forecasting Carbon Emissions in the Framework of the Environmental Kuznets Curve: The Case of Vietnam. Energies 2021, 14, 3144. DOI: https://doi.org/10.3390/en14113144\nVo Hoai Viet, Nguyen Thanh Thien Phuc, Pham Minh Hoang, Liu Kim Nghia, \u0026quot; Spatial-Temporal Shape and Motion Features for Dynamic Hand Gesture Recognition in Depth Video\u0026quot;, International Journal of Image, Graphics and Signal Processing(IJIGSP), Vol.10, No.9, pp. 17-26, 2018. DOI: http://doi.org/10.5815/ijigsp.2018.09.03\n","permalink":"http://visionbike.github.io/publish/","summary":"C.-H. Kuo, P. T.-T. Nguyen, and S.-L. Wu. Teaching Artificial Intelligence in Mechanical Engineering to Cultivate Cyber-physical System Talents. In Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 2 (ITiCSE 2023). https://doi.org/10.1145/3587103.3594193\nI.W.D. Pranata, P.T.-T. Nguyen, K.-H. Su, Y.-C.Kuo, C.-H Kuo. Knee Angle Generation with Walking Speed Adaptation Ability for a Powered Transfemoral Prosthetic Leg Prototype. Inventions 2023, 8, 67. https://doi.org/10.3390/inventions8030067\nM.-H. Hsu; P.T.-T. Nguyen; D.","title":"Publish"}]